DistributedRunner run(): command = deepspeed --num_nodes 1 --num_gpus 4 --no_local_rank run_clm_starcoder.py --model_name_or_path bigcode/starcoder --train_file /starcoder/finetune-data/roblox-train.json --validation_file /starcoder/finetune-data/roblox-validation.json --report_to tensorboard --bf16 True --output_dir ./checkpoints/starcoder-full-deepspeed-offload-4-hpu-8-2-adamw --num_train_epochs 10 --per_device_train_batch_size 8 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy no --learning_rate 1e-4 --logging_steps 1 --dataset_concatenation --do_train --use_habana --use_lazy_mode --throughput_warmup_steps 3 --cache_dir /starcoder/cache/ --use_hpu_graphs_for_inference --deepspeed deepspeed_config_s3_offload.json
[2023-08-21 16:36:02,938] [WARNING] [runner.py:185:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Namespace(autotuning='', elastic_training=False, exclude='', force_multi=False, hostfile='/job/hostfile', include='', launcher='pdsh', launcher_args='', master_addr='', master_port=29500, max_elastic_nodes=-1, min_elastic_nodes=-1, module=False, no_local_rank=True, no_python=False, no_ssh_check=False, num_gpus=4, num_nodes=1, save_pid=False, use_hpu=True, user_args=['--model_name_or_path', 'bigcode/starcoder', '--train_file', '/starcoder/finetune-data/roblox-train.json', '--validation_file', '/starcoder/finetune-data/roblox-validation.json', '--report_to', 'tensorboard', '--bf16', 'True', '--output_dir', './checkpoints/starcoder-full-deepspeed-offload-4-hpu-8-2-adamw', '--num_train_epochs', '10', '--per_device_train_batch_size', '8', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'no', '--learning_rate', '1e-4', '--logging_steps', '1', '--dataset_concatenation', '--do_train', '--use_habana', '--use_lazy_mode', '--throughput_warmup_steps', '3', '--cache_dir', '/starcoder/cache/', '--use_hpu_graphs_for_inference', '--deepspeed', 'deepspeed_config_s3_offload.json'], user_script='run_clm_starcoder.py')
[2023-08-21 16:36:04,572] [INFO] [runner.py:543:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --no_local_rank run_clm_starcoder.py --model_name_or_path bigcode/starcoder --train_file /starcoder/finetune-data/roblox-train.json --validation_file /starcoder/finetune-data/roblox-validation.json --report_to tensorboard --bf16 True --output_dir ./checkpoints/starcoder-full-deepspeed-offload-4-hpu-8-2-adamw --num_train_epochs 10 --per_device_train_batch_size 8 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy no --learning_rate 1e-4 --logging_steps 1 --dataset_concatenation --do_train --use_habana --use_lazy_mode --throughput_warmup_steps 3 --cache_dir /starcoder/cache/ --use_hpu_graphs_for_inference --deepspeed deepspeed_config_s3_offload.json
[2023-08-21 16:36:06,434] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2023-08-21 16:36:06,434] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2023-08-21 16:36:06,434] [INFO] [launch.py:164:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2023-08-21 16:36:06,434] [INFO] [launch.py:165:main] dist_world_size=4
[2023-08-21 16:36:11,873] [INFO] [comm.py:762:init_distributed] Initializing TorchBackend in DeepSpeed with backend hccl
08/21/2023 16:36:12 - WARNING - __main__ -   Process rank: 0, device: hpu
distributed training: True, 16-bits training: True
08/21/2023 16:36:12 - WARNING - __main__ -   Process rank: 3, device: hpu
distributed training: True, 16-bits training: True
08/21/2023 16:36:12 - INFO - __main__ -   Training/evaluation parameters GaudiTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-06,
adjust_throughput=False,
auto_find_batch_size=False,
bf16=True,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=230,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=deepspeed_config_s3_offload.json,
disable_tqdm=False,
distribution_strategy=ddp,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fsdp_config=None,
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gaudi_config_name=None,
gradient_accumulation_steps=2,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=hpu_amp,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./checkpoints/starcoder-full-deepspeed-offload-4-hpu-8-2-adamw/runs/Aug21_16-36-11_sysid674631,
logging_first_step=False,
logging_nan_inf_filter=False,
logging_steps=1,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
no_cuda=False,
non_blocking_data_copy=False,
num_train_epochs=10.0,
optim=adamw_torch,
optim_args=None,
output_dir=./checkpoints/starcoder-full-deepspeed-offload-4-hpu-8-2-adamw,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
pipelining_fwd_bwd=False,
prediction_loss_only=False,
profiling_steps=0,
profiling_warmup_steps=0,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=./checkpoints/starcoder-full-deepspeed-offload-4-hpu-8-2-adamw,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=no,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
throughput_warmup_steps=3,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
use_habana=True,
use_hpu_graphs=False,
use_hpu_graphs_for_inference=True,
use_hpu_graphs_for_training=False,
use_ipex=False,
use_lazy_mode=True,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
08/21/2023 16:36:12 - WARNING - __main__ -   Process rank: 2, device: hpu
distributed training: True, 16-bits training: True
08/21/2023 16:36:12 - WARNING - __main__ -   Process rank: 1, device: hpu
distributed training: True, 16-bits training: True
[INFO|configuration_utils.py:668] 2023-08-21 16:36:12,536 >> loading configuration file config.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/config.json
[INFO|configuration_utils.py:720] 2023-08-21 16:36:12,540 >> Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoder",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 6144,
  "n_head": 48,
  "n_inner": 24576,
  "n_layer": 40,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

[INFO|tokenization_utils_base.py:1809] 2023-08-21 16:36:12,655 >> loading file vocab.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/vocab.json
[INFO|tokenization_utils_base.py:1809] 2023-08-21 16:36:12,656 >> loading file merges.txt from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/merges.txt
[INFO|tokenization_utils_base.py:1809] 2023-08-21 16:36:12,656 >> loading file tokenizer.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/tokenizer.json
[INFO|tokenization_utils_base.py:1809] 2023-08-21 16:36:12,656 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2023-08-21 16:36:12,656 >> loading file special_tokens_map.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/special_tokens_map.json
[INFO|tokenization_utils_base.py:1809] 2023-08-21 16:36:12,656 >> loading file tokenizer_config.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/tokenizer_config.json
/usr/local/lib/python3.8/dist-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
[INFO|modeling_utils.py:2534] 2023-08-21 16:36:13,192 >> loading weights file pytorch_model.bin from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/pytorch_model.bin.index.json
[INFO|modeling_utils.py:1176] 2023-08-21 16:36:13,193 >> Instantiating GaudiGPTBigCodeForCausalLM model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2623] 2023-08-21 16:36:13,193 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:575] 2023-08-21 16:36:13,198 >> Generate config GaudiGenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "eos_token_id": 0,
  "ignore_eos": null,
  "static_shapes": null,
  "transformers_version": "4.28.1"
}

=============================HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_HPU_LAZY_EAGER_OPTIM_CACHE = 1
 PT_HPU_ENABLE_COMPILE_THREAD = 0
 PT_HPU_ENABLE_EXECUTION_THREAD = 1
 PT_HPU_ENABLE_LAZY_EAGER_EXECUTION_THREAD = 1
 PT_ENABLE_INTER_HOST_CACHING = 0
 PT_ENABLE_INFERENCE_MODE = 1
 PT_ENABLE_HABANA_CACHING = 1
 PT_HPU_MAX_RECIPE_SUBMISSION_LIMIT = 0
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_MAX_COMPOUND_OP_SIZE_SS = 10
 PT_HPU_ENABLE_STAGE_SUBMISSION = 1
 PT_HPU_STAGE_SUBMISSION_MODE = 2
 PT_HPU_PGM_ENABLE_CACHE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 0
 PT_HCCL_SLICE_SIZE_MB = 16
 PT_HCCL_MEMORY_ALLOWANCE_MB = 0
 PT_HPU_INITIAL_WORKSPACE_SIZE = 0
 PT_HABANA_POOL_SIZE = 24
 PT_HPU_POOL_STRATEGY = 5
 PT_HPU_POOL_LOG_FRAGMENTATION_INFO = 0
 PT_ENABLE_MEMORY_DEFRAGMENTATION = 1
 PT_ENABLE_DEFRAGMENTATION_INFO = 0
 PT_HPU_ENABLE_SYNAPSE_OUTPUT_PERMUTE = 1
 PT_HPU_ENABLE_VALID_DATA_RANGE_CHECK = 1
 PT_HPU_FORCE_USE_DEFAULT_STREAM = 0
 PT_RECIPE_CACHE_PATH = 
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_DYNAMIC_MIN_POLICY_ORDER = 4,5,3,1
 PT_HPU_DYNAMIC_MAX_POLICY_ORDER = 2,4,5,3,1
 PT_HPU_LAZY_ACC_PAR_MODE = 0
 PT_HPU_CLUSTERED_PROGRAM = 0
 PT_HPU_CLUSTERED_PROGRAM_ENFORCE = 0
 PT_HPU_CLUSTERED_PROGRAM_SPLIT_STR = default
 PT_HPU_CLUSTERED_PROGRAM_SCHED_STR = default
=============================SYSTEM CONFIGURATION ========================================= 
Num CPU Cores = 160
CPU RAM = 1056389528 KB 
============================================================================================ 
[2023-08-21 16:36:25,911] [INFO] [partition_parameters.py:481:__exit__] finished initializing model with 15.82B parameters
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
Loading checkpoint shards:  14%|█▍        | 1/7 [00:07<00:42,  7.15s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:07<00:42,  7.15s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:07<00:42,  7.13s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:07<00:43,  7.17s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:13<00:33,  6.72s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:13<00:33,  6.72s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:13<00:33,  6.74s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:13<00:33,  6.77s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:20<00:26,  6.60s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:20<00:26,  6.62s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:20<00:26,  6.61s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:20<00:26,  6.64s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:24<00:17,  5.75s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:24<00:17,  5.75s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:24<00:17,  5.75s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:24<00:17,  5.76s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:31<00:12,  6.35s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:31<00:12,  6.35s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:31<00:12,  6.36s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:31<00:12,  6.37s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:37<00:06,  6.27s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:38<00:06,  6.28s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:38<00:06,  6.28s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:38<00:06,  6.28s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:41<00:00,  5.34s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:41<00:00,  5.92s/it]
[INFO|modeling_utils.py:3190] 2023-08-21 16:37:07,447 >> All model checkpoint weights were used when initializing GaudiGPTBigCodeForCausalLM.

[INFO|modeling_utils.py:3198] 2023-08-21 16:37:07,447 >> All the weights of GaudiGPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoder.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GaudiGPTBigCodeForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 7/7 [00:41<00:00,  5.34s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:41<00:00,  5.92s/it]
Loading checkpoint shards: 100%|██████████| 7/7 [00:41<00:00,  5.35s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:41<00:00,  5.93s/it]
Loading checkpoint shards: 100%|██████████| 7/7 [00:41<00:00,  5.35s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:41<00:00,  5.93s/it]
[INFO|configuration_utils.py:537] 2023-08-21 16:37:07,610 >> loading configuration file generation_config.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/generation_config.json
[INFO|configuration_utils.py:575] 2023-08-21 16:37:07,611 >> Generate config GaudiGenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "eos_token_id": 0,
  "ignore_eos": null,
  "static_shapes": null,
  "transformers_version": "4.28.1"
}

08/21/2023 16:37:08 - INFO - __main__ -   Using data collator of type DataCollatorForSeq2Seq
Number of trainable paraemters: 0
[INFO|trainer.py:621] 2023-08-21 16:37:08,230 >> Using hpu_amp half precision backend
[2023-08-21 16:37:08,282] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.7.7+hpu.synapse.v1.10.0, git-hash=4bc77a6, git-branch=1.10.0
Number of trainable paraemters: 0
Number of trainable paraemters: 0
Number of trainable paraemters: 0
[2023-08-21 16:37:08,503] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py38_cpu/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
ninja: no work to do.
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...Loading extension module cpu_adam...

Time to load cpu_adam op: 4.191485643386841 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2023-08-21 16:37:16,125] [INFO] [logging.py:68:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2023-08-21 16:37:16,200] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-08-21 16:37:16,200] [INFO] [utils.py:58:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-08-21 16:37:16,200] [INFO] [logging.py:68:log_dist] [Rank 0] Creating bf16 ZeRO stage 3 optimizer
[2023-08-21 16:37:16,200] [INFO] [logging.py:68:log_dist] [Rank 0] Creating bf16 ZeRO stage 3 optimizer
[2023-08-21 16:37:16,304] [INFO] [utils.py:930:see_memory_usage] Stage 3 initialize beginning
[2023-08-21 16:37:16,315] [WARNING] [utils.py:49:not_implemented] max_memory_reserved is not implemented on HPU
[2023-08-21 16:37:16,315] [INFO] [utils.py:931:see_memory_usage] MA 0.07 GB         Max_MA 1.75 GB         CA 0.07 GB         Max_CA 0 GB 
[2023-08-21 16:37:16,315] [INFO] [utils.py:939:see_memory_usage] CPU Virtual Memory:  used = 122.57 GB, percent = 12.2%
[2023-08-21 16:37:16,319] [INFO] [stage3.py:116:__init__] Reduce bucket size 500,000,000
[2023-08-21 16:37:16,319] [INFO] [stage3.py:117:__init__] Prefetch bucket size 50,000,000
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py38_cpu/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Emitting ninja build file /root/.cache/torch_extensions/py38_cpu/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.8962602615356445 seconds
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.765706539154053 seconds
[2023-08-21 16:37:17,331] [INFO] [utils.py:930:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2023-08-21 16:37:17,341] [WARNING] [utils.py:49:not_implemented] max_memory_reserved is not implemented on HPU
[2023-08-21 16:37:17,341] [INFO] [utils.py:931:see_memory_usage] MA 0.07 GB         Max_MA 0.0 GB         CA 0.07 GB         Max_CA 0 GB 
[2023-08-21 16:37:17,341] [INFO] [utils.py:939:see_memory_usage] CPU Virtual Memory:  used = 122.55 GB, percent = 12.2%
Parameter Offload: Total persistent parameters: 2725888 in 322 params
Loading extension module cpu_adam...
Time to load cpu_adam op: 5.405457019805908 seconds
Loading extension module cpu_adam...
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Time to load cpu_adam op: 4.874428033828735 seconds
[2023-08-21 16:37:17,451] [INFO] [utils.py:930:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
[2023-08-21 16:37:17,461] [WARNING] [utils.py:49:not_implemented] max_memory_reserved is not implemented on HPU
[2023-08-21 16:37:17,462] [INFO] [utils.py:931:see_memory_usage] MA 0.07 GB         Max_MA 0.0 GB         CA 0.07 GB         Max_CA 0 GB 
[2023-08-21 16:37:17,462] [INFO] [utils.py:939:see_memory_usage] CPU Virtual Memory:  used = 122.56 GB, percent = 12.2%
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py38_cpu/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 1.1729309558868408 seconds
Loading extension module utils...
Time to load utils op: 1.1051747798919678 seconds
Loading extension module utils...
Time to load utils op: 1.205392599105835 seconds
tcmalloc: large alloc 7758733312 bytes == 0x28aed2000 @  0x7f0110d80680 0x7f0110da1824 0x7f0110da1b8a 0x7f00518b6adc 0x7f0051890ef7 0x7f0052bb42f0 0x7f0052bade38 0x7f0052badeb8 0x7f0052badf38 0x7f00533130c3 0x7f0053f80290 0x7f0053f8030f 0x7f0053bb3e7b 0x7f0053f3ba93 0x7f0053bf8c65 0x7f005dc4033a 0x5f6939 0x5f7506 0x571019 0x5f6ce6 0x56b619 0x5697da 0x5f6ec3 0x56b619 0x5697da 0x5f6ec3 0x59c427 0x5f746f 0x571019 0x50b07e 0x570556
tcmalloc: large alloc 7758733312 bytes == 0x28bdfc000 @  0x7f205add4680 0x7f205adf5824 0x7f205adf5b8a 0x7f1f9b912adc 0x7f1f9b8ecef7 0x7f1f9cc102f0 0x7f1f9cc09e38 0x7f1f9cc09eb8 0x7f1f9cc09f38 0x7f1f9d36f0c3 0x7f1f9dfdc290 0x7f1f9dfdc30f 0x7f1f9dc0fe7b 0x7f1f9df97a93 0x7f1f9dc54c65 0x7f1fa7cab33a 0x5f6939 0x5f7506 0x571019 0x5f6ce6 0x56b619 0x5697da 0x5f6ec3 0x56b619 0x5697da 0x5f6ec3 0x59c427 0x5f746f 0x571019 0x50b07e 0x570556
tcmalloc: large alloc 7758733312 bytes == 0x28ad9c000 @  0x7f30d2a3b680 0x7f30d2a5c824 0x7f30d2a5cb8a 0x7f3013574adc 0x7f301354eef7 0x7f30148722f0 0x7f301486be38 0x7f301486beb8 0x7f301486bf38 0x7f3014fd10c3 0x7f3015c3e290 0x7f3015c3e30f 0x7f3015871e7b 0x7f3015bf9a93 0x7f30158b6c65 0x7f301f90d33a 0x5f6939 0x5f7506 0x571019 0x5f6ce6 0x56b619 0x5697da 0x5f6ec3 0x56b619 0x5697da 0x5f6ec3 0x59c427 0x5f746f 0x571019 0x50b07e 0x570556
tcmalloc: large alloc 7758733312 bytes == 0x28b1ba000 @  0x7fd8c5780680 0x7fd8c57a1824 0x7fd8c57a1b8a 0x7fd88a209adc 0x7fd88a1e3ef7 0x7fd8071dc2f0 0x7fd8071d5e38 0x7fd8071d5eb8 0x7fd8071d5f38 0x7fd80793b0c3 0x7fd8085a8290 0x7fd8085a830f 0x7fd8081dbe7b 0x7fd808563a93 0x7fd808220c65 0x7fd81226833a 0x5f6939 0x5f7506 0x571019 0x5f6ce6 0x56b619 0x5697da 0x5f6ec3 0x56b619 0x5697da 0x5f6ec3 0x59c427 0x5f746f 0x571019 0x50b07e 0x570556
[2023-08-21 16:37:33,953] [INFO] [stage3.py:378:_setup_for_real_optimizer] optimizer state initialized
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0007302761077880859 seconds
Time to load utils op: 0.0006392002105712891 seconds
Time to load utils op: 0.0006685256958007812 seconds
You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
[2023-08-21 16:37:37,554] [INFO] [utils.py:930:see_memory_usage] After initializing ZeRO optimizer
[2023-08-21 16:37:37,561] [WARNING] [utils.py:49:not_implemented] max_memory_reserved is not implemented on HPU
[2023-08-21 16:37:37,561] [INFO] [utils.py:931:see_memory_usage] MA 0.07 GB         Max_MA 1.78 GB         CA 0.07 GB         Max_CA 0 GB 
[2023-08-21 16:37:37,561] [INFO] [utils.py:939:see_memory_usage] CPU Virtual Memory:  used = 317.13 GB, percent = 31.5%
[2023-08-21 16:37:37,563] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2023-08-21 16:37:37,563] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed using client callable to create LR scheduler
[2023-08-21 16:37:37,563] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7efe83e38fd0>
[2023-08-21 16:37:37,563] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-08-21 16:37:37,564] [INFO] [config.py:1028:print] DeepSpeedEngine configuration:
[2023-08-21 16:37:37,564] [INFO] [config.py:1032:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-21 16:37:37,564] [INFO] [config.py:1032:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-21 16:37:37,564] [INFO] [config.py:1032:print]   amp_enabled .................. False
[2023-08-21 16:37:37,564] [INFO] [config.py:1032:print]   amp_params ................... False
[2023-08-21 16:37:37,564] [INFO] [config.py:1032:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-21 16:37:37,564] [INFO] [config.py:1032:print]   bfloat16_enabled ............. True
[2023-08-21 16:37:37,564] [INFO] [config.py:1032:print]   checkpoint_parallel_write_pipeline  False
[2023-08-21 16:37:37,564] [INFO] [config.py:1032:print]   checkpoint_tag_validation_enabled  True
[2023-08-21 16:37:37,564] [INFO] [config.py:1032:print]   checkpoint_tag_validation_fail  False
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7efea9c91310>
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   communication_data_type ...... None
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   curriculum_enabled ........... False
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   curriculum_params ............ False
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   dataloader_drop_last ......... False
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   disable_allgather ............ False
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   dump_state ................... False
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   dynamic_loss_scale_args ...... None
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   eigenvalue_enabled ........... False
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   eigenvalue_layer_num ......... 0
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   eigenvalue_max_iter .......... 100
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   eigenvalue_stability ......... 1e-06
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   eigenvalue_tol ............... 0.01
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   eigenvalue_verbose ........... False
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   elasticity_enabled ........... False
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   fp16_auto_cast ............... None
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   fp16_enabled ................. False
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   fp16_master_weights_and_gradients  False
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   global_rank .................. 0
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   grad_accum_dtype ............. None
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   gradient_accumulation_steps .. 2
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   gradient_clipping ............ 1
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   gradient_predivide_factor .... 1.0
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   initial_dynamic_scale ........ 1
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   load_universal_checkpoint .... False
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   loss_scale ................... 1.0
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   memory_breakdown ............. False
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7efea9c914c0>
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   optimizer_legacy_fusion ...... False
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   optimizer_name ............... adamw
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   optimizer_params ............. {'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-06, 'weight_decay': 0.0}
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   pld_enabled .................. False
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   pld_params ................... False
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   prescale_gradients ........... False
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   scheduler_name ............... None
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   scheduler_params ............. None
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   sparse_attention ............. None
[2023-08-21 16:37:37,565] [INFO] [config.py:1032:print]   sparse_gradients_enabled ..... False
[2023-08-21 16:37:37,566] [INFO] [config.py:1032:print]   steps_per_print .............. 64
[2023-08-21 16:37:37,566] [INFO] [config.py:1032:print]   train_batch_size ............. 64
[2023-08-21 16:37:37,566] [INFO] [config.py:1032:print]   train_micro_batch_size_per_gpu  8
[2023-08-21 16:37:37,566] [INFO] [config.py:1032:print]   use_node_local_storage ....... False
[2023-08-21 16:37:37,566] [INFO] [config.py:1032:print]   wall_clock_breakdown ......... False
[2023-08-21 16:37:37,566] [INFO] [config.py:1032:print]   world_size ................... 4
[2023-08-21 16:37:37,566] [INFO] [config.py:1032:print]   zero_allow_comm_data_type_fp32  False
[2023-08-21 16:37:37,566] [INFO] [config.py:1032:print]   zero_allow_untested_optimizer  False
[2023-08-21 16:37:37,566] [INFO] [config.py:1032:print]   zero_config .................. stage=3 contiguous_gradients=False reduce_scatter=False reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False max_group_size=4000000000 load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2023-08-21 16:37:37,566] [INFO] [config.py:1032:print]   zero_enabled ................. True
[2023-08-21 16:37:37,566] [INFO] [config.py:1032:print]   zero_optimization_stage ...... 3
[2023-08-21 16:37:37,566] [INFO] [config.py:1017:print_user_config]   json = {
    "steps_per_print": 64, 
    "train_batch_size": 64, 
    "train_micro_batch_size_per_gpu": 8, 
    "gradient_accumulation_steps": 2, 
    "bf16": {
        "enabled": true
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0001, 
            "betas": [0.9, 0.999], 
            "eps": 1e-06, 
            "weight_decay": 0.0
        }
    }, 
    "gradient_clipping": 1, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "cpu"
        }, 
        "offload_param": {
            "device": "cpu"
        }, 
        "overlap_comm": false, 
        "reduce_scatter": false, 
        "contiguous_gradients": false
    }
}
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00036263465881347656 seconds
[INFO|trainer.py:770] 2023-08-21 16:37:37,566 >> ***** Running training *****
[INFO|trainer.py:771] 2023-08-21 16:37:37,566 >>   Num examples = 489
[INFO|trainer.py:772] 2023-08-21 16:37:37,566 >>   Num Epochs = 10
[INFO|trainer.py:773] 2023-08-21 16:37:37,566 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:774] 2023-08-21 16:37:37,567 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:775] 2023-08-21 16:37:37,567 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:776] 2023-08-21 16:37:37,567 >>   Total optimization steps = 80
[INFO|trainer.py:777] 2023-08-21 16:37:37,570 >>   Number of trainable parameters = 15,517,456,384
  0%|          | 0/80 [00:00<?, ?it/s][WARNING|logging.py:280] 2023-08-21 16:37:37,599 >> You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:3030: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:3030: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:3030: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:3030: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
  1%|▏         | 1/80 [01:56<2:32:47, 116.04s/it]                                                 {'loss': 1.4219, 'learning_rate': 9.875000000000002e-05, 'epoch': 0.12, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.29, 'total_memory_available (GB)': 93.74}
  1%|▏         | 1/80 [01:56<2:32:47, 116.04s/it]  2%|▎         | 2/80 [02:54<1:46:35, 82.00s/it]                                                 {'loss': 1.417, 'learning_rate': 9.75e-05, 'epoch': 0.25, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 81.11, 'total_memory_available (GB)': 93.74}
  2%|▎         | 2/80 [02:54<1:46:35, 82.00s/it]  4%|▍         | 3/80 [03:23<1:14:26, 58.01s/it]                                                {'loss': 1.2402, 'learning_rate': 9.625000000000001e-05, 'epoch': 0.38, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 81.11, 'total_memory_available (GB)': 93.74}
  4%|▍         | 3/80 [03:23<1:14:26, 58.01s/it]  5%|▌         | 4/80 [03:54<59:50, 47.24s/it]                                                {'loss': 1.1318, 'learning_rate': 9.5e-05, 'epoch': 0.5, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 81.11, 'total_memory_available (GB)': 93.74}
  5%|▌         | 4/80 [03:54<59:50, 47.24s/it]  6%|▋         | 5/80 [04:24<51:25, 41.14s/it]                                              {'loss': 1.0215, 'learning_rate': 9.375e-05, 'epoch': 0.62, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 81.11, 'total_memory_available (GB)': 93.74}
  6%|▋         | 5/80 [04:24<51:25, 41.14s/it]  8%|▊         | 6/80 [04:56<46:39, 37.83s/it]                                              {'loss': 1.0024, 'learning_rate': 9.250000000000001e-05, 'epoch': 0.75, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 81.11, 'total_memory_available (GB)': 93.74}
  8%|▊         | 6/80 [04:56<46:39, 37.83s/it]  9%|▉         | 7/80 [05:25<42:51, 35.23s/it]                                              {'loss': 0.981, 'learning_rate': 9.125e-05, 'epoch': 0.88, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 81.11, 'total_memory_available (GB)': 93.74}
  9%|▉         | 7/80 [05:26<42:51, 35.23s/it] 10%|█         | 8/80 [05:56<40:24, 33.68s/it]                                              {'loss': 0.9194, 'learning_rate': 9e-05, 'epoch': 1.0, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 81.11, 'total_memory_available (GB)': 93.74}
 10%|█         | 8/80 [05:56<40:24, 33.68s/it] 11%|█▏        | 9/80 [06:27<38:44, 32.75s/it]                                              {'loss': 0.7363, 'learning_rate': 8.875e-05, 'epoch': 1.12, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 81.11, 'total_memory_available (GB)': 93.74}
 11%|█▏        | 9/80 [06:27<38:44, 32.75s/it] 12%|█▎        | 10/80 [06:58<37:44, 32.34s/it]                                               {'loss': 0.6968, 'learning_rate': 8.75e-05, 'epoch': 1.25, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 12%|█▎        | 10/80 [06:58<37:44, 32.34s/it] 14%|█▍        | 11/80 [07:28<36:22, 31.64s/it]                                               {'loss': 0.644, 'learning_rate': 8.625000000000001e-05, 'epoch': 1.38, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 14%|█▍        | 11/80 [07:28<36:22, 31.64s/it] 15%|█▌        | 12/80 [07:57<35:00, 30.90s/it]                                               {'loss': 0.6426, 'learning_rate': 8.5e-05, 'epoch': 1.5, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 15%|█▌        | 12/80 [07:57<35:00, 30.90s/it] 16%|█▋        | 13/80 [08:28<34:27, 30.86s/it]                                               {'loss': 0.6558, 'learning_rate': 8.375e-05, 'epoch': 1.62, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 16%|█▋        | 13/80 [08:28<34:27, 30.86s/it] 18%|█▊        | 14/80 [08:59<34:09, 31.05s/it]                                               {'loss': 0.6484, 'learning_rate': 8.25e-05, 'epoch': 1.75, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 18%|█▊        | 14/80 [09:00<34:09, 31.05s/it] 19%|█▉        | 15/80 [09:29<33:11, 30.64s/it]                                               {'loss': 0.6831, 'learning_rate': 8.125000000000001e-05, 'epoch': 1.88, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 19%|█▉        | 15/80 [09:29<33:11, 30.64s/it] 20%|██        | 16/80 [09:59<32:18, 30.29s/it]                                               {'loss': 0.5029, 'learning_rate': 8e-05, 'epoch': 2.0, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 20%|██        | 16/80 [09:59<32:18, 30.29s/it] 21%|██▏       | 17/80 [10:29<31:45, 30.24s/it]                                               {'loss': 0.4172, 'learning_rate': 7.875e-05, 'epoch': 2.12, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 21%|██▏       | 17/80 [10:29<31:45, 30.24s/it] 22%|██▎       | 18/80 [10:59<31:09, 30.15s/it]                                               {'loss': 0.3708, 'learning_rate': 7.75e-05, 'epoch': 2.25, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 22%|██▎       | 18/80 [10:59<31:09, 30.15s/it] 24%|██▍       | 19/80 [11:29<30:34, 30.07s/it]                                               {'loss': 0.3804, 'learning_rate': 7.625e-05, 'epoch': 2.38, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 24%|██▍       | 19/80 [11:29<30:34, 30.07s/it] 25%|██▌       | 20/80 [11:59<30:08, 30.14s/it]                                               {'loss': 0.374, 'learning_rate': 7.500000000000001e-05, 'epoch': 2.5, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 25%|██▌       | 20/80 [11:59<30:08, 30.14s/it] 26%|██▋       | 21/80 [12:29<29:35, 30.09s/it]                                               {'loss': 0.3547, 'learning_rate': 7.375e-05, 'epoch': 2.62, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 26%|██▋       | 21/80 [12:29<29:35, 30.09s/it] 28%|██▊       | 22/80 [12:58<28:47, 29.78s/it]                                               {'loss': 0.2927, 'learning_rate': 7.25e-05, 'epoch': 2.75, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 28%|██▊       | 22/80 [12:58<28:47, 29.78s/it] 29%|██▉       | 23/80 [13:28<28:28, 29.97s/it]                                               {'loss': 0.3372, 'learning_rate': 7.125000000000001e-05, 'epoch': 2.88, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 29%|██▉       | 23/80 [13:28<28:28, 29.97s/it] 30%|███       | 24/80 [13:58<27:55, 29.92s/it]                                               {'loss': 0.2926, 'learning_rate': 7e-05, 'epoch': 3.0, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 30%|███       | 24/80 [13:58<27:55, 29.92s/it] 31%|███▏      | 25/80 [14:27<27:12, 29.69s/it]                                               {'loss': 0.1923, 'learning_rate': 6.875e-05, 'epoch': 3.12, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 31%|███▏      | 25/80 [14:27<27:12, 29.69s/it] 32%|███▎      | 26/80 [14:58<27:06, 30.11s/it]                                               {'loss': 0.1913, 'learning_rate': 6.750000000000001e-05, 'epoch': 3.25, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 32%|███▎      | 26/80 [14:58<27:06, 30.11s/it] 34%|███▍      | 27/80 [15:28<26:25, 29.91s/it]                                               {'loss': 0.1758, 'learning_rate': 6.625e-05, 'epoch': 3.38, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 34%|███▍      | 27/80 [15:28<26:25, 29.91s/it] 35%|███▌      | 28/80 [15:57<25:46, 29.74s/it]                                               {'loss': 0.1664, 'learning_rate': 6.500000000000001e-05, 'epoch': 3.5, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 35%|███▌      | 28/80 [15:57<25:46, 29.74s/it] 36%|███▋      | 29/80 [16:26<25:09, 29.60s/it]                                               {'loss': 0.1814, 'learning_rate': 6.375e-05, 'epoch': 3.62, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 36%|███▋      | 29/80 [16:26<25:09, 29.60s/it] 38%|███▊      | 30/80 [16:56<24:42, 29.65s/it]                                               {'loss': 0.1748, 'learning_rate': 6.25e-05, 'epoch': 3.75, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 38%|███▊      | 30/80 [16:56<24:42, 29.65s/it] 39%|███▉      | 31/80 [17:26<24:13, 29.66s/it]                                               {'loss': 0.1777, 'learning_rate': 6.125000000000001e-05, 'epoch': 3.88, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 39%|███▉      | 31/80 [17:26<24:13, 29.66s/it][2023-08-21 16:55:33,296] [INFO] [timer.py:241:stop] 0/64, RunningAvgSamplesPerSec=2.076493490100155, CurrSamplesPerSec=1.4611306433806124, MemAllocated=2.3GB, MaxMemAllocated=83.76GB
 40%|████      | 32/80 [17:55<23:38, 29.55s/it]                                               {'loss': 0.1418, 'learning_rate': 6e-05, 'epoch': 4.0, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 40%|████      | 32/80 [17:55<23:38, 29.55s/it] 41%|████▏     | 33/80 [18:24<22:54, 29.25s/it]                                               {'loss': 0.1176, 'learning_rate': 5.8750000000000005e-05, 'epoch': 4.12, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 41%|████▏     | 33/80 [18:24<22:54, 29.25s/it] 42%|████▎     | 34/80 [18:53<22:27, 29.29s/it]                                               {'loss': 0.1152, 'learning_rate': 5.7499999999999995e-05, 'epoch': 4.25, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 42%|████▎     | 34/80 [18:53<22:27, 29.29s/it] 44%|████▍     | 35/80 [19:21<21:40, 28.91s/it]                                               {'loss': 0.1115, 'learning_rate': 5.6250000000000005e-05, 'epoch': 4.38, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 44%|████▍     | 35/80 [19:21<21:40, 28.91s/it] 45%|████▌     | 36/80 [19:51<21:29, 29.31s/it]                                               {'loss': 0.1271, 'learning_rate': 5.500000000000001e-05, 'epoch': 4.5, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 45%|████▌     | 36/80 [19:51<21:29, 29.31s/it] 46%|████▋     | 37/80 [20:19<20:43, 28.91s/it]                                               {'loss': 0.1193, 'learning_rate': 5.375e-05, 'epoch': 4.62, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 46%|████▋     | 37/80 [20:19<20:43, 28.91s/it] 48%|████▊     | 38/80 [20:49<20:21, 29.09s/it]                                               {'loss': 0.1224, 'learning_rate': 5.25e-05, 'epoch': 4.75, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 48%|████▊     | 38/80 [20:49<20:21, 29.09s/it] 49%|████▉     | 39/80 [21:21<20:25, 29.90s/it]                                               {'loss': 0.1094, 'learning_rate': 5.125e-05, 'epoch': 4.88, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 49%|████▉     | 39/80 [21:21<20:25, 29.90s/it] 50%|█████     | 40/80 [21:50<19:54, 29.85s/it]                                               {'loss': 0.0941, 'learning_rate': 5e-05, 'epoch': 5.0, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 50%|█████     | 40/80 [21:50<19:54, 29.85s/it] 51%|█████▏    | 41/80 [22:19<19:09, 29.48s/it]                                               {'loss': 0.0873, 'learning_rate': 4.875e-05, 'epoch': 5.12, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 51%|█████▏    | 41/80 [22:19<19:09, 29.48s/it] 52%|█████▎    | 42/80 [22:49<18:41, 29.50s/it]                                               {'loss': 0.0812, 'learning_rate': 4.75e-05, 'epoch': 5.25, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 52%|█████▎    | 42/80 [22:49<18:41, 29.50s/it] 54%|█████▍    | 43/80 [23:18<18:06, 29.37s/it]                                               {'loss': 0.0769, 'learning_rate': 4.6250000000000006e-05, 'epoch': 5.38, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 54%|█████▍    | 43/80 [23:18<18:06, 29.37s/it] 55%|█████▌    | 44/80 [23:47<17:38, 29.39s/it]                                               {'loss': 0.0814, 'learning_rate': 4.5e-05, 'epoch': 5.5, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 55%|█████▌    | 44/80 [23:47<17:38, 29.39s/it] 56%|█████▋    | 45/80 [24:17<17:12, 29.51s/it]                                               {'loss': 0.0694, 'learning_rate': 4.375e-05, 'epoch': 5.62, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 56%|█████▋    | 45/80 [24:17<17:12, 29.51s/it] 57%|█████▊    | 46/80 [24:46<16:37, 29.35s/it]                                               {'loss': 0.0773, 'learning_rate': 4.25e-05, 'epoch': 5.75, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 57%|█████▊    | 46/80 [24:46<16:37, 29.35s/it] 59%|█████▉    | 47/80 [25:15<16:07, 29.33s/it]                                               {'loss': 0.0689, 'learning_rate': 4.125e-05, 'epoch': 5.88, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 59%|█████▉    | 47/80 [25:15<16:07, 29.33s/it] 60%|██████    | 48/80 [25:45<15:41, 29.42s/it]                                               {'loss': 0.0663, 'learning_rate': 4e-05, 'epoch': 6.0, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 60%|██████    | 48/80 [25:45<15:41, 29.42s/it] 61%|██████▏   | 49/80 [26:14<15:07, 29.27s/it]                                               {'loss': 0.4746, 'learning_rate': 3.875e-05, 'epoch': 6.12, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 61%|██████▏   | 49/80 [26:14<15:07, 29.27s/it] 62%|██████▎   | 50/80 [26:44<14:44, 29.49s/it]                                               {'loss': 0.0487, 'learning_rate': 3.7500000000000003e-05, 'epoch': 6.25, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 62%|██████▎   | 50/80 [26:44<14:44, 29.49s/it] 64%|██████▍   | 51/80 [27:12<14:05, 29.17s/it]                                               {'loss': 0.0506, 'learning_rate': 3.625e-05, 'epoch': 6.38, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 64%|██████▍   | 51/80 [27:12<14:05, 29.17s/it] 65%|██████▌   | 52/80 [27:42<13:39, 29.27s/it]                                               {'loss': 0.045, 'learning_rate': 3.5e-05, 'epoch': 6.5, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 65%|██████▌   | 52/80 [27:42<13:39, 29.27s/it] 66%|██████▋   | 53/80 [28:11<13:13, 29.41s/it]                                               {'loss': 0.0463, 'learning_rate': 3.375000000000001e-05, 'epoch': 6.62, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 66%|██████▋   | 53/80 [28:11<13:13, 29.41s/it] 68%|██████▊   | 54/80 [28:41<12:46, 29.49s/it]                                               {'loss': 0.0444, 'learning_rate': 3.2500000000000004e-05, 'epoch': 6.75, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 68%|██████▊   | 54/80 [28:41<12:46, 29.49s/it] 69%|██████▉   | 55/80 [29:10<12:14, 29.39s/it]                                               {'loss': 0.0418, 'learning_rate': 3.125e-05, 'epoch': 6.88, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 69%|██████▉   | 55/80 [29:10<12:14, 29.39s/it] 70%|███████   | 56/80 [29:40<11:46, 29.42s/it]                                               {'loss': 0.0469, 'learning_rate': 3e-05, 'epoch': 7.0, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 70%|███████   | 56/80 [29:40<11:46, 29.42s/it] 71%|███████▏  | 57/80 [30:11<11:30, 30.03s/it]                                               {'loss': 0.0299, 'learning_rate': 2.8749999999999997e-05, 'epoch': 7.12, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 71%|███████▏  | 57/80 [30:11<11:30, 30.03s/it] 72%|███████▎  | 58/80 [30:41<10:56, 29.84s/it]                                               {'loss': 0.0334, 'learning_rate': 2.7500000000000004e-05, 'epoch': 7.25, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 72%|███████▎  | 58/80 [30:41<10:56, 29.84s/it] 74%|███████▍  | 59/80 [31:10<10:23, 29.68s/it]                                               {'loss': 0.0288, 'learning_rate': 2.625e-05, 'epoch': 7.38, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 74%|███████▍  | 59/80 [31:10<10:23, 29.68s/it] 75%|███████▌  | 60/80 [31:39<09:48, 29.45s/it]                                               {'loss': 0.0285, 'learning_rate': 2.5e-05, 'epoch': 7.5, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 75%|███████▌  | 60/80 [31:39<09:48, 29.45s/it] 76%|███████▋  | 61/80 [32:08<09:19, 29.45s/it]                                               {'loss': 0.0281, 'learning_rate': 2.375e-05, 'epoch': 7.62, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 76%|███████▋  | 61/80 [32:08<09:19, 29.45s/it] 78%|███████▊  | 62/80 [32:37<08:49, 29.41s/it]                                               {'loss': 0.0277, 'learning_rate': 2.25e-05, 'epoch': 7.75, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 78%|███████▊  | 62/80 [32:38<08:49, 29.41s/it] 79%|███████▉  | 63/80 [33:07<08:20, 29.41s/it]                                               {'loss': 0.0267, 'learning_rate': 2.125e-05, 'epoch': 7.88, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 79%|███████▉  | 63/80 [33:07<08:20, 29.41s/it][2023-08-21 17:11:14,587] [INFO] [logging.py:68:log_dist] [Rank 0] step=64, skipped=0, lr=[2e-05], mom=[[0.9, 0.999]]
[2023-08-21 17:11:14,594] [INFO] [timer.py:241:stop] 0/128, RunningAvgSamplesPerSec=2.1303835964046383, CurrSamplesPerSec=1.4179961142318636, MemAllocated=2.3GB, MaxMemAllocated=83.76GB
 80%|████████  | 64/80 [33:37<07:51, 29.47s/it]                                               {'loss': 0.0259, 'learning_rate': 2e-05, 'epoch': 8.0, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 80%|████████  | 64/80 [33:37<07:51, 29.47s/it] 81%|████████▏ | 65/80 [34:06<07:22, 29.47s/it]                                               {'loss': 0.0208, 'learning_rate': 1.8750000000000002e-05, 'epoch': 8.12, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 81%|████████▏ | 65/80 [34:06<07:22, 29.47s/it] 82%|████████▎ | 66/80 [34:35<06:52, 29.48s/it]                                               {'loss': 0.0191, 'learning_rate': 1.75e-05, 'epoch': 8.25, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 82%|████████▎ | 66/80 [34:35<06:52, 29.48s/it] 84%|████████▍ | 67/80 [35:06<06:25, 29.67s/it]                                               {'loss': 0.0179, 'learning_rate': 1.6250000000000002e-05, 'epoch': 8.38, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 84%|████████▍ | 67/80 [35:06<06:25, 29.67s/it] 85%|████████▌ | 68/80 [35:35<05:54, 29.55s/it]                                               {'loss': 0.2514, 'learning_rate': 1.5e-05, 'epoch': 8.5, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 85%|████████▌ | 68/80 [35:35<05:54, 29.55s/it] 86%|████████▋ | 69/80 [36:05<05:25, 29.63s/it]                                               {'loss': 0.0183, 'learning_rate': 1.3750000000000002e-05, 'epoch': 8.62, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 86%|████████▋ | 69/80 [36:05<05:25, 29.63s/it] 88%|████████▊ | 70/80 [36:34<04:54, 29.42s/it]                                               {'loss': 0.0198, 'learning_rate': 1.25e-05, 'epoch': 8.75, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 88%|████████▊ | 70/80 [36:34<04:54, 29.42s/it] 89%|████████▉ | 71/80 [37:03<04:24, 29.40s/it]                                               {'loss': 0.0189, 'learning_rate': 1.125e-05, 'epoch': 8.88, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 89%|████████▉ | 71/80 [37:03<04:24, 29.40s/it] 90%|█████████ | 72/80 [37:32<03:54, 29.33s/it]                                               {'loss': 0.0182, 'learning_rate': 1e-05, 'epoch': 9.0, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 90%|█████████ | 72/80 [37:32<03:54, 29.33s/it] 91%|█████████▏| 73/80 [38:01<03:23, 29.07s/it]                                               {'loss': 0.0162, 'learning_rate': 8.75e-06, 'epoch': 9.12, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 91%|█████████▏| 73/80 [38:01<03:23, 29.07s/it] 92%|█████████▎| 74/80 [38:30<02:55, 29.20s/it]                                               {'loss': 0.0163, 'learning_rate': 7.5e-06, 'epoch': 9.25, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 92%|█████████▎| 74/80 [38:30<02:55, 29.20s/it] 94%|█████████▍| 75/80 [38:59<02:25, 29.03s/it]                                               {'loss': 0.0158, 'learning_rate': 6.25e-06, 'epoch': 9.38, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 94%|█████████▍| 75/80 [38:59<02:25, 29.03s/it] 95%|█████████▌| 76/80 [39:29<01:57, 29.30s/it]                                               {'loss': 0.015, 'learning_rate': 5e-06, 'epoch': 9.5, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 95%|█████████▌| 76/80 [39:29<01:57, 29.30s/it] 96%|█████████▋| 77/80 [39:57<01:27, 29.12s/it]                                               {'loss': 0.017, 'learning_rate': 3.75e-06, 'epoch': 9.62, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 96%|█████████▋| 77/80 [39:57<01:27, 29.12s/it] 98%|█████████▊| 78/80 [40:26<00:58, 29.01s/it]                                               {'loss': 0.0171, 'learning_rate': 2.5e-06, 'epoch': 9.75, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 98%|█████████▊| 78/80 [40:26<00:58, 29.01s/it] 99%|█████████▉| 79/80 [41:01<00:30, 30.69s/it]                                               {'loss': 0.0162, 'learning_rate': 1.25e-06, 'epoch': 9.88, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
 99%|█████████▉| 79/80 [41:01<00:30, 30.69s/it]100%|██████████| 80/80 [41:29<00:00, 30.11s/it]                                               {'loss': 0.0176, 'learning_rate': 0.0, 'epoch': 10.0, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
100%|██████████| 80/80 [41:30<00:00, 30.11s/it][INFO|trainer.py:1041] 2023-08-21 17:19:07,600 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               {'train_runtime': 2490.0307, 'train_samples_per_second': 2.055, 'train_steps_per_second': 0.034, 'train_loss': 0.2704339027404785, 'epoch': 10.0, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 83.76, 'total_memory_available (GB)': 93.74}
100%|██████████| 80/80 [41:30<00:00, 30.11s/it]100%|██████████| 80/80 [41:30<00:00, 31.13s/it]
Traceback (most recent call last):
  File "run_clm_starcoder.py", line 568, in <module>
    main()
  File "run_clm_starcoder.py", line 564, in main
    model.save_pretrained(training_args.output_dir, state_dict=unwrapped_model.state_dict())
NameError: name 'unwrapped_model' is not defined
[2023-08-21 17:19:15,455] [INFO] [launch.py:354:main] Process 3523279 exits successfully.
[2023-08-21 17:19:16,459] [INFO] [launch.py:354:main] Process 3523277 exits successfully.
[2023-08-21 17:19:16,459] [INFO] [launch.py:322:sigkill_handler] Killing subprocess 3523276
[2023-08-21 17:19:16,461] [INFO] [launch.py:322:sigkill_handler] Killing subprocess 3523277
[2023-08-21 17:19:16,462] [INFO] [launch.py:322:sigkill_handler] Killing subprocess 3523278
[2023-08-21 17:19:16,471] [INFO] [launch.py:322:sigkill_handler] Killing subprocess 3523279
[2023-08-21 17:19:16,472] [ERROR] [launch.py:328:sigkill_handler] ['/usr/bin/python3', '-u', 'run_clm_starcoder.py', '--model_name_or_path', 'bigcode/starcoder', '--train_file', '/starcoder/finetune-data/roblox-train.json', '--validation_file', '/starcoder/finetune-data/roblox-validation.json', '--report_to', 'tensorboard', '--bf16', 'True', '--output_dir', './checkpoints/starcoder-full-deepspeed-offload-4-hpu-8-2-adamw', '--num_train_epochs', '10', '--per_device_train_batch_size', '8', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'no', '--learning_rate', '1e-4', '--logging_steps', '1', '--dataset_concatenation', '--do_train', '--use_habana', '--use_lazy_mode', '--throughput_warmup_steps', '3', '--cache_dir', '/starcoder/cache/', '--use_hpu_graphs_for_inference', '--deepspeed', 'deepspeed_config_s3_offload.json'] exits with return code = 1
[ERROR|distributed_runner.py:218] 2023-08-21 17:19:17,249 >> deepspeed --num_nodes 1 --num_gpus 4 --no_local_rank run_clm_starcoder.py --model_name_or_path bigcode/starcoder --train_file /starcoder/finetune-data/roblox-train.json --validation_file /starcoder/finetune-data/roblox-validation.json --report_to tensorboard --bf16 True --output_dir ./checkpoints/starcoder-full-deepspeed-offload-4-hpu-8-2-adamw --num_train_epochs 10 --per_device_train_batch_size 8 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy no --learning_rate 1e-4 --logging_steps 1 --dataset_concatenation --do_train --use_habana --use_lazy_mode --throughput_warmup_steps 3 --cache_dir /starcoder/cache/ --use_hpu_graphs_for_inference --deepspeed deepspeed_config_s3_offload.json  exited with status = 1
