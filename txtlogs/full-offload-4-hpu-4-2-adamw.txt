DistributedRunner run(): command = deepspeed --num_nodes 1 --num_gpus 4 --no_local_rank run_clm_starcoder.py --model_name_or_path bigcode/starcoder --train_file /starcoder/finetune-data/roblox-train.json --validation_file /starcoder/finetune-data/roblox-validation.json --report_to tensorboard --bf16 True --output_dir ./checkpoints/starcoder-full-deepspeed-offload-4-hpu-4-2-adamw --num_train_epochs 10 --per_device_train_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy no --learning_rate 1e-4 --logging_steps 1 --dataset_concatenation --do_train --use_habana --use_lazy_mode --throughput_warmup_steps 3 --cache_dir /starcoder/cache/ --use_hpu_graphs_for_inference --deepspeed deepspeed_config_s3_offload.json
[2023-08-21 14:20:50,960] [WARNING] [runner.py:185:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Namespace(autotuning='', elastic_training=False, exclude='', force_multi=False, hostfile='/job/hostfile', include='', launcher='pdsh', launcher_args='', master_addr='', master_port=29500, max_elastic_nodes=-1, min_elastic_nodes=-1, module=False, no_local_rank=True, no_python=False, no_ssh_check=False, num_gpus=4, num_nodes=1, save_pid=False, use_hpu=True, user_args=['--model_name_or_path', 'bigcode/starcoder', '--train_file', '/starcoder/finetune-data/roblox-train.json', '--validation_file', '/starcoder/finetune-data/roblox-validation.json', '--report_to', 'tensorboard', '--bf16', 'True', '--output_dir', './checkpoints/starcoder-full-deepspeed-offload-4-hpu-4-2-adamw', '--num_train_epochs', '10', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'no', '--learning_rate', '1e-4', '--logging_steps', '1', '--dataset_concatenation', '--do_train', '--use_habana', '--use_lazy_mode', '--throughput_warmup_steps', '3', '--cache_dir', '/starcoder/cache/', '--use_hpu_graphs_for_inference', '--deepspeed', 'deepspeed_config_s3_offload.json'], user_script='run_clm_starcoder.py')
[2023-08-21 14:20:52,640] [INFO] [runner.py:543:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --no_local_rank run_clm_starcoder.py --model_name_or_path bigcode/starcoder --train_file /starcoder/finetune-data/roblox-train.json --validation_file /starcoder/finetune-data/roblox-validation.json --report_to tensorboard --bf16 True --output_dir ./checkpoints/starcoder-full-deepspeed-offload-4-hpu-4-2-adamw --num_train_epochs 10 --per_device_train_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy no --learning_rate 1e-4 --logging_steps 1 --dataset_concatenation --do_train --use_habana --use_lazy_mode --throughput_warmup_steps 3 --cache_dir /starcoder/cache/ --use_hpu_graphs_for_inference --deepspeed deepspeed_config_s3_offload.json
[2023-08-21 14:20:54,533] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2023-08-21 14:20:54,533] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2023-08-21 14:20:54,534] [INFO] [launch.py:164:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2023-08-21 14:20:54,534] [INFO] [launch.py:165:main] dist_world_size=4
[2023-08-21 14:21:00,064] [INFO] [comm.py:762:init_distributed] Initializing TorchBackend in DeepSpeed with backend hccl
08/21/2023 14:21:00 - WARNING - __main__ -   Process rank: 0, device: hpu
distributed training: True, 16-bits training: True
08/21/2023 14:21:00 - WARNING - __main__ -   Process rank: 3, device: hpu
distributed training: True, 16-bits training: True
08/21/2023 14:21:00 - INFO - __main__ -   Training/evaluation parameters GaudiTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-06,
adjust_throughput=False,
auto_find_batch_size=False,
bf16=True,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=230,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=deepspeed_config_s3_offload.json,
disable_tqdm=False,
distribution_strategy=ddp,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fsdp_config=None,
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gaudi_config_name=None,
gradient_accumulation_steps=2,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=hpu_amp,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./checkpoints/starcoder-full-deepspeed-offload-4-hpu-4-2-adamw/runs/Aug21_14-20-59_sysid674631,
logging_first_step=False,
logging_nan_inf_filter=False,
logging_steps=1,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
no_cuda=False,
non_blocking_data_copy=False,
num_train_epochs=10.0,
optim=adamw_torch,
optim_args=None,
output_dir=./checkpoints/starcoder-full-deepspeed-offload-4-hpu-4-2-adamw,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
pipelining_fwd_bwd=False,
prediction_loss_only=False,
profiling_steps=0,
profiling_warmup_steps=0,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=./checkpoints/starcoder-full-deepspeed-offload-4-hpu-4-2-adamw,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=no,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
throughput_warmup_steps=3,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
use_habana=True,
use_hpu_graphs=False,
use_hpu_graphs_for_inference=True,
use_hpu_graphs_for_training=False,
use_ipex=False,
use_lazy_mode=True,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
08/21/2023 14:21:00 - WARNING - __main__ -   Process rank: 2, device: hpu
distributed training: True, 16-bits training: True
08/21/2023 14:21:00 - WARNING - __main__ -   Process rank: 1, device: hpu
distributed training: True, 16-bits training: True
[INFO|configuration_utils.py:668] 2023-08-21 14:21:00,870 >> loading configuration file config.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/config.json
[INFO|configuration_utils.py:720] 2023-08-21 14:21:00,873 >> Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoder",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 6144,
  "n_head": 48,
  "n_inner": 24576,
  "n_layer": 40,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

[INFO|tokenization_utils_base.py:1809] 2023-08-21 14:21:00,999 >> loading file vocab.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/vocab.json
[INFO|tokenization_utils_base.py:1809] 2023-08-21 14:21:00,999 >> loading file merges.txt from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/merges.txt
[INFO|tokenization_utils_base.py:1809] 2023-08-21 14:21:00,999 >> loading file tokenizer.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/tokenizer.json
[INFO|tokenization_utils_base.py:1809] 2023-08-21 14:21:00,999 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2023-08-21 14:21:00,999 >> loading file special_tokens_map.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/special_tokens_map.json
[INFO|tokenization_utils_base.py:1809] 2023-08-21 14:21:00,999 >> loading file tokenizer_config.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/tokenizer_config.json
/usr/local/lib/python3.8/dist-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
[INFO|modeling_utils.py:2534] 2023-08-21 14:21:01,584 >> loading weights file pytorch_model.bin from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/pytorch_model.bin.index.json
[INFO|modeling_utils.py:1176] 2023-08-21 14:21:01,586 >> Instantiating GaudiGPTBigCodeForCausalLM model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2623] 2023-08-21 14:21:01,586 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:575] 2023-08-21 14:21:01,592 >> Generate config GaudiGenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "eos_token_id": 0,
  "ignore_eos": null,
  "static_shapes": null,
  "transformers_version": "4.28.1"
}

=============================HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_HPU_LAZY_EAGER_OPTIM_CACHE = 1
 PT_HPU_ENABLE_COMPILE_THREAD = 0
 PT_HPU_ENABLE_EXECUTION_THREAD = 1
 PT_HPU_ENABLE_LAZY_EAGER_EXECUTION_THREAD = 1
 PT_ENABLE_INTER_HOST_CACHING = 0
 PT_ENABLE_INFERENCE_MODE = 1
 PT_ENABLE_HABANA_CACHING = 1
 PT_HPU_MAX_RECIPE_SUBMISSION_LIMIT = 0
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_MAX_COMPOUND_OP_SIZE_SS = 10
 PT_HPU_ENABLE_STAGE_SUBMISSION = 1
 PT_HPU_STAGE_SUBMISSION_MODE = 2
 PT_HPU_PGM_ENABLE_CACHE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 0
 PT_HCCL_SLICE_SIZE_MB = 16
 PT_HCCL_MEMORY_ALLOWANCE_MB = 0
 PT_HPU_INITIAL_WORKSPACE_SIZE = 0
 PT_HABANA_POOL_SIZE = 24
 PT_HPU_POOL_STRATEGY = 5
 PT_HPU_POOL_LOG_FRAGMENTATION_INFO = 0
 PT_ENABLE_MEMORY_DEFRAGMENTATION = 1
 PT_ENABLE_DEFRAGMENTATION_INFO = 0
 PT_HPU_ENABLE_SYNAPSE_OUTPUT_PERMUTE = 1
 PT_HPU_ENABLE_VALID_DATA_RANGE_CHECK = 1
 PT_HPU_FORCE_USE_DEFAULT_STREAM = 0
 PT_RECIPE_CACHE_PATH = 
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_DYNAMIC_MIN_POLICY_ORDER = 4,5,3,1
 PT_HPU_DYNAMIC_MAX_POLICY_ORDER = 2,4,5,3,1
 PT_HPU_LAZY_ACC_PAR_MODE = 0
 PT_HPU_CLUSTERED_PROGRAM = 0
 PT_HPU_CLUSTERED_PROGRAM_ENFORCE = 0
 PT_HPU_CLUSTERED_PROGRAM_SPLIT_STR = default
 PT_HPU_CLUSTERED_PROGRAM_SCHED_STR = default
=============================SYSTEM CONFIGURATION ========================================= 
Num CPU Cores = 160
CPU RAM = 1056389528 KB 
============================================================================================ 
[2023-08-21 14:21:14,437] [INFO] [partition_parameters.py:481:__exit__] finished initializing model with 15.82B parameters
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
Loading checkpoint shards:  14%|█▍        | 1/7 [00:07<00:44,  7.38s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:07<00:44,  7.38s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:07<00:44,  7.38s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:07<00:44,  7.44s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:11<00:28,  5.67s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:11<00:28,  5.66s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:11<00:28,  5.66s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:11<00:28,  5.66s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:16<00:20,  5.00s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:16<00:19,  5.00s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:16<00:19,  5.00s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:16<00:19,  5.00s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:20<00:14,  4.72s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:20<00:14,  4.71s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:20<00:14,  4.71s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:20<00:14,  4.71s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:24<00:09,  4.64s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:24<00:09,  4.64s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:24<00:09,  4.64s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:24<00:09,  4.64s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:29<00:04,  4.51s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:29<00:04,  4.52s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:29<00:04,  4.51s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:29<00:04,  4.51s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:31<00:00,  3.93s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:31<00:00,  4.55s/it]
[INFO|modeling_utils.py:3190] 2023-08-21 14:21:46,395 >> All model checkpoint weights were used when initializing GaudiGPTBigCodeForCausalLM.

[INFO|modeling_utils.py:3198] 2023-08-21 14:21:46,395 >> All the weights of GaudiGPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoder.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GaudiGPTBigCodeForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 7/7 [00:31<00:00,  3.93s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:31<00:00,  4.55s/it]
Loading checkpoint shards: 100%|██████████| 7/7 [00:31<00:00,  3.93s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:31<00:00,  4.55s/it]
Loading checkpoint shards: 100%|██████████| 7/7 [00:31<00:00,  3.93s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:31<00:00,  4.55s/it]
[INFO|configuration_utils.py:537] 2023-08-21 14:21:46,495 >> loading configuration file generation_config.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/generation_config.json
[INFO|configuration_utils.py:575] 2023-08-21 14:21:46,496 >> Generate config GaudiGenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "eos_token_id": 0,
  "ignore_eos": null,
  "static_shapes": null,
  "transformers_version": "4.28.1"
}

08/21/2023 14:21:47 - INFO - __main__ -   Using data collator of type DataCollatorForSeq2Seq
Number of trainable paraemters: 0
[INFO|trainer.py:621] 2023-08-21 14:21:47,125 >> Using hpu_amp half precision backend
[2023-08-21 14:21:47,177] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.7.7+hpu.synapse.v1.10.0, git-hash=4bc77a6, git-branch=1.10.0
Number of trainable paraemters: 0
Number of trainable paraemters: 0
Number of trainable paraemters: 0
[2023-08-21 14:21:47,406] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py38_cpu/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.1417765617370605 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.115252256393433 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2023-08-21 14:21:54,916] [INFO] [logging.py:68:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.6378252506256104 seconds
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.6470401287078857 seconds
[2023-08-21 14:21:54,991] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-08-21 14:21:54,991] [INFO] [utils.py:58:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-08-21 14:21:54,991] [INFO] [logging.py:68:log_dist] [Rank 0] Creating bf16 ZeRO stage 3 optimizer
[2023-08-21 14:21:54,992] [INFO] [logging.py:68:log_dist] [Rank 0] Creating bf16 ZeRO stage 3 optimizer
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
[2023-08-21 14:21:55,105] [INFO] [utils.py:930:see_memory_usage] Stage 3 initialize beginning
[2023-08-21 14:21:55,116] [WARNING] [utils.py:49:not_implemented] max_memory_reserved is not implemented on HPU
[2023-08-21 14:21:55,116] [INFO] [utils.py:931:see_memory_usage] MA 0.07 GB         Max_MA 1.75 GB         CA 0.07 GB         Max_CA 0 GB 
[2023-08-21 14:21:55,116] [INFO] [utils.py:939:see_memory_usage] CPU Virtual Memory:  used = 122.56 GB, percent = 12.2%
[2023-08-21 14:21:55,120] [INFO] [stage3.py:116:__init__] Reduce bucket size 500,000,000
[2023-08-21 14:21:55,120] [INFO] [stage3.py:117:__init__] Prefetch bucket size 50,000,000
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py38_cpu/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.8447511196136475 seconds
Loading extension module utils...
Time to load utils op: 0.8043322563171387 seconds
Loading extension module utils...
Time to load utils op: 0.7043845653533936 seconds
Loading extension module utils...
Time to load utils op: 0.8043718338012695 seconds
[2023-08-21 14:21:55,920] [INFO] [utils.py:930:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2023-08-21 14:21:55,930] [WARNING] [utils.py:49:not_implemented] max_memory_reserved is not implemented on HPU
[2023-08-21 14:21:55,930] [INFO] [utils.py:931:see_memory_usage] MA 0.07 GB         Max_MA 0.0 GB         CA 0.07 GB         Max_CA 0 GB 
[2023-08-21 14:21:55,930] [INFO] [utils.py:939:see_memory_usage] CPU Virtual Memory:  used = 122.52 GB, percent = 12.2%
Parameter Offload: Total persistent parameters: 2725888 in 322 params
[2023-08-21 14:21:56,038] [INFO] [utils.py:930:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2023-08-21 14:21:56,049] [WARNING] [utils.py:49:not_implemented] max_memory_reserved is not implemented on HPU
[2023-08-21 14:21:56,049] [INFO] [utils.py:931:see_memory_usage] MA 0.07 GB         Max_MA 0.0 GB         CA 0.07 GB         Max_CA 0 GB 
[2023-08-21 14:21:56,049] [INFO] [utils.py:939:see_memory_usage] CPU Virtual Memory:  used = 122.52 GB, percent = 12.2%
tcmalloc: large alloc 7758733312 bytes == 0x28bf96000 @  0x7fb5c8fb6680 0x7fb5c8fd7824 0x7fb5c8fd7b8a 0x7fb509aecadc 0x7fb509ac6ef7 0x7fb50adea2f0 0x7fb50ade3e38 0x7fb50ade3eb8 0x7fb50ade3f38 0x7fb50b5490c3 0x7fb50c1b6290 0x7fb50c1b630f 0x7fb50bde9e7b 0x7fb50c171a93 0x7fb50be2ec65 0x7fb515e7633a 0x5f6939 0x5f7506 0x571019 0x5f6ce6 0x56b619 0x5697da 0x5f6ec3 0x56b619 0x5697da 0x5f6ec3 0x59c427 0x5f746f 0x571019 0x50b07e 0x570556
tcmalloc: large alloc 7758733312 bytes == 0x28b40a000 @  0x7f555057e680 0x7f555059f824 0x7f555059fb8a 0x7f5522021adc 0x7f5521ffbef7 0x7f5491fda2f0 0x7f5491fd3e38 0x7f5491fd3eb8 0x7f5491fd3f38 0x7f54927390c3 0x7f54933a6290 0x7f54933a630f 0x7f5492fd9e7b 0x7f5493361a93 0x7f549301ec65 0x7f549d06633a 0x5f6939 0x5f7506 0x571019 0x5f6ce6 0x56b619 0x5697da 0x5f6ec3 0x56b619 0x5697da 0x5f6ec3 0x59c427 0x5f746f 0x571019 0x50b07e 0x570556
tcmalloc: large alloc 7758733312 bytes == 0x28b070000 @  0x7fe3f29f3680 0x7fe3f2a14824 0x7fe3f2a14b8a 0x7fe33352cadc 0x7fe333506ef7 0x7fe33482a2f0 0x7fe334823e38 0x7fe334823eb8 0x7fe334823f38 0x7fe334f890c3 0x7fe335bf6290 0x7fe335bf630f 0x7fe335829e7b 0x7fe335bb1a93 0x7fe33586ec65 0x7fe33f8c533a 0x5f6939 0x5f7506 0x571019 0x5f6ce6 0x56b619 0x5697da 0x5f6ec3 0x56b619 0x5697da 0x5f6ec3 0x59c427 0x5f746f 0x571019 0x50b07e 0x570556
tcmalloc: large alloc 7758733312 bytes == 0x28c410000 @  0x7f24975c6680 0x7f24975e7824 0x7f24975e7b8a 0x7f23d80fcadc 0x7f23d80d6ef7 0x7f23d93fa2f0 0x7f23d93f3e38 0x7f23d93f3eb8 0x7f23d93f3f38 0x7f23d9b590c3 0x7f23da7c6290 0x7f23da7c630f 0x7f23da3f9e7b 0x7f23da781a93 0x7f23da43ec65 0x7f23e448633a 0x5f6939 0x5f7506 0x571019 0x5f6ce6 0x56b619 0x5697da 0x5f6ec3 0x56b619 0x5697da 0x5f6ec3 0x59c427 0x5f746f 0x571019 0x50b07e 0x570556
[2023-08-21 14:22:10,574] [INFO] [stage3.py:378:_setup_for_real_optimizer] optimizer state initialized
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0004382133483886719 seconds
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0006127357482910156 seconds
Time to load utils op: 0.0006015300750732422 seconds
You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
[2023-08-21 14:22:14,034] [INFO] [utils.py:930:see_memory_usage] After initializing ZeRO optimizer
[2023-08-21 14:22:14,041] [WARNING] [utils.py:49:not_implemented] max_memory_reserved is not implemented on HPU
[2023-08-21 14:22:14,041] [INFO] [utils.py:931:see_memory_usage] MA 0.07 GB         Max_MA 1.78 GB         CA 0.07 GB         Max_CA 0 GB 
[2023-08-21 14:22:14,042] [INFO] [utils.py:939:see_memory_usage] CPU Virtual Memory:  used = 317.06 GB, percent = 31.5%
[2023-08-21 14:22:14,043] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2023-08-21 14:22:14,043] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed using client callable to create LR scheduler
[2023-08-21 14:22:14,043] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7fb33c037fa0>
[2023-08-21 14:22:14,043] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-08-21 14:22:14,044] [INFO] [config.py:1028:print] DeepSpeedEngine configuration:
[2023-08-21 14:22:14,044] [INFO] [config.py:1032:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-21 14:22:14,044] [INFO] [config.py:1032:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-21 14:22:14,044] [INFO] [config.py:1032:print]   amp_enabled .................. False
[2023-08-21 14:22:14,044] [INFO] [config.py:1032:print]   amp_params ................... False
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   bfloat16_enabled ............. True
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   checkpoint_parallel_write_pipeline  False
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   checkpoint_tag_validation_enabled  True
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   checkpoint_tag_validation_fail  False
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb33d5f5370>
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   communication_data_type ...... None
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   curriculum_enabled ........... False
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   curriculum_params ............ False
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   dataloader_drop_last ......... False
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   disable_allgather ............ False
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   dump_state ................... False
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   dynamic_loss_scale_args ...... None
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   eigenvalue_enabled ........... False
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   eigenvalue_layer_num ......... 0
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   eigenvalue_max_iter .......... 100
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   eigenvalue_stability ......... 1e-06
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   eigenvalue_tol ............... 0.01
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   eigenvalue_verbose ........... False
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   elasticity_enabled ........... False
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   fp16_auto_cast ............... None
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   fp16_enabled ................. False
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   fp16_master_weights_and_gradients  False
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   global_rank .................. 0
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   grad_accum_dtype ............. None
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   gradient_accumulation_steps .. 2
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   gradient_clipping ............ 1
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   gradient_predivide_factor .... 1.0
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   initial_dynamic_scale ........ 1
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   load_universal_checkpoint .... False
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   loss_scale ................... 1.0
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   memory_breakdown ............. False
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7fb33d5f5520>
[2023-08-21 14:22:14,045] [INFO] [config.py:1032:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-21 14:22:14,046] [INFO] [config.py:1032:print]   optimizer_legacy_fusion ...... False
[2023-08-21 14:22:14,046] [INFO] [config.py:1032:print]   optimizer_name ............... adamw
[2023-08-21 14:22:14,046] [INFO] [config.py:1032:print]   optimizer_params ............. {'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-06, 'weight_decay': 0.0}
[2023-08-21 14:22:14,046] [INFO] [config.py:1032:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-21 14:22:14,046] [INFO] [config.py:1032:print]   pld_enabled .................. False
[2023-08-21 14:22:14,046] [INFO] [config.py:1032:print]   pld_params ................... False
[2023-08-21 14:22:14,046] [INFO] [config.py:1032:print]   prescale_gradients ........... False
[2023-08-21 14:22:14,046] [INFO] [config.py:1032:print]   scheduler_name ............... None
[2023-08-21 14:22:14,046] [INFO] [config.py:1032:print]   scheduler_params ............. None
[2023-08-21 14:22:14,046] [INFO] [config.py:1032:print]   sparse_attention ............. None
[2023-08-21 14:22:14,046] [INFO] [config.py:1032:print]   sparse_gradients_enabled ..... False
[2023-08-21 14:22:14,046] [INFO] [config.py:1032:print]   steps_per_print .............. 64
[2023-08-21 14:22:14,046] [INFO] [config.py:1032:print]   train_batch_size ............. 32
[2023-08-21 14:22:14,046] [INFO] [config.py:1032:print]   train_micro_batch_size_per_gpu  4
[2023-08-21 14:22:14,046] [INFO] [config.py:1032:print]   use_node_local_storage ....... False
[2023-08-21 14:22:14,046] [INFO] [config.py:1032:print]   wall_clock_breakdown ......... False
[2023-08-21 14:22:14,046] [INFO] [config.py:1032:print]   world_size ................... 4
[2023-08-21 14:22:14,046] [INFO] [config.py:1032:print]   zero_allow_comm_data_type_fp32  False
[2023-08-21 14:22:14,046] [INFO] [config.py:1032:print]   zero_allow_untested_optimizer  False
[2023-08-21 14:22:14,046] [INFO] [config.py:1032:print]   zero_config .................. stage=3 contiguous_gradients=False reduce_scatter=False reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False max_group_size=4000000000 load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2023-08-21 14:22:14,046] [INFO] [config.py:1032:print]   zero_enabled ................. True
[2023-08-21 14:22:14,046] [INFO] [config.py:1032:print]   zero_optimization_stage ...... 3
[2023-08-21 14:22:14,046] [INFO] [config.py:1017:print_user_config]   json = {
    "steps_per_print": 64, 
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 4, 
    "gradient_accumulation_steps": 2, 
    "bf16": {
        "enabled": true
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0001, 
            "betas": [0.9, 0.999], 
            "eps": 1e-06, 
            "weight_decay": 0.0
        }
    }, 
    "gradient_clipping": 1, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "cpu"
        }, 
        "offload_param": {
            "device": "cpu"
        }, 
        "overlap_comm": false, 
        "reduce_scatter": false, 
        "contiguous_gradients": false
    }
}
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00028395652770996094 seconds
[INFO|trainer.py:770] 2023-08-21 14:22:14,047 >> ***** Running training *****
[INFO|trainer.py:771] 2023-08-21 14:22:14,047 >>   Num examples = 489
[INFO|trainer.py:772] 2023-08-21 14:22:14,047 >>   Num Epochs = 10
[INFO|trainer.py:773] 2023-08-21 14:22:14,047 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:774] 2023-08-21 14:22:14,047 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:775] 2023-08-21 14:22:14,047 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:776] 2023-08-21 14:22:14,047 >>   Total optimization steps = 150
[INFO|trainer.py:777] 2023-08-21 14:22:14,050 >>   Number of trainable parameters = 15,517,456,384
  0%|          | 0/150 [00:00<?, ?it/s][WARNING|logging.py:280] 2023-08-21 14:22:14,078 >> You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:3030: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:3030: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:3030: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:3030: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
  1%|          | 1/150 [01:55<4:46:18, 115.30s/it]                                                  {'loss': 1.4502, 'learning_rate': 9.933333333333334e-05, 'epoch': 0.06, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 42.23, 'total_memory_available (GB)': 93.74}
  1%|          | 1/150 [01:55<4:46:18, 115.30s/it]  1%|▏         | 2/150 [02:52<3:20:44, 81.38s/it]                                                  {'loss': 1.3906, 'learning_rate': 9.866666666666668e-05, 'epoch': 0.13, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 46.98, 'total_memory_available (GB)': 93.74}
  1%|▏         | 2/150 [02:52<3:20:44, 81.38s/it]  2%|▏         | 3/150 [03:24<2:23:34, 58.60s/it]                                                 {'loss': 1.2461, 'learning_rate': 9.8e-05, 'epoch': 0.19, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 46.98, 'total_memory_available (GB)': 93.74}
  2%|▏         | 3/150 [03:24<2:23:34, 58.60s/it]  3%|▎         | 4/150 [03:54<1:55:14, 47.36s/it]                                                 {'loss': 1.0791, 'learning_rate': 9.733333333333335e-05, 'epoch': 0.26, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 46.98, 'total_memory_available (GB)': 93.74}
  3%|▎         | 4/150 [03:54<1:55:14, 47.36s/it]  3%|▎         | 5/150 [04:25<1:40:14, 41.48s/it]                                                 {'loss': 1.0156, 'learning_rate': 9.666666666666667e-05, 'epoch': 0.32, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 46.98, 'total_memory_available (GB)': 93.74}
  3%|▎         | 5/150 [04:25<1:40:14, 41.48s/it]  4%|▍         | 6/150 [04:55<1:30:24, 37.67s/it]                                                 {'loss': 1.1025, 'learning_rate': 9.6e-05, 'epoch': 0.39, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 46.98, 'total_memory_available (GB)': 93.74}
  4%|▍         | 6/150 [04:55<1:30:24, 37.67s/it]  5%|▍         | 7/150 [05:25<1:23:34, 35.07s/it]                                                 {'loss': 1.043, 'learning_rate': 9.533333333333334e-05, 'epoch': 0.45, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 46.98, 'total_memory_available (GB)': 93.74}
  5%|▍         | 7/150 [05:25<1:23:34, 35.07s/it]  5%|▌         | 8/150 [05:56<1:19:49, 33.73s/it]                                                 {'loss': 0.9839, 'learning_rate': 9.466666666666667e-05, 'epoch': 0.52, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 46.98, 'total_memory_available (GB)': 93.74}
  5%|▌         | 8/150 [05:56<1:19:49, 33.73s/it]  6%|▌         | 9/150 [06:25<1:15:31, 32.14s/it]                                                 {'loss': 0.9336, 'learning_rate': 9.4e-05, 'epoch': 0.58, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 46.98, 'total_memory_available (GB)': 93.74}
  6%|▌         | 9/150 [06:25<1:15:31, 32.14s/it]  7%|▋         | 10/150 [06:55<1:13:33, 31.53s/it]                                                  {'loss': 1.0215, 'learning_rate': 9.333333333333334e-05, 'epoch': 0.65, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 48.28, 'total_memory_available (GB)': 93.74}
  7%|▋         | 10/150 [06:55<1:13:33, 31.53s/it]  7%|▋         | 11/150 [07:23<1:11:01, 30.66s/it]                                                  {'loss': 1.0337, 'learning_rate': 9.266666666666666e-05, 'epoch': 0.71, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 48.28, 'total_memory_available (GB)': 93.74}
  7%|▋         | 11/150 [07:23<1:11:01, 30.66s/it]  8%|▊         | 12/150 [07:54<1:10:20, 30.58s/it]                                                  {'loss': 0.9497, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.77, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 48.28, 'total_memory_available (GB)': 93.74}
  8%|▊         | 12/150 [07:54<1:10:20, 30.58s/it]  9%|▊         | 13/150 [08:23<1:09:01, 30.23s/it]                                                  {'loss': 0.9824, 'learning_rate': 9.133333333333334e-05, 'epoch': 0.84, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 48.28, 'total_memory_available (GB)': 93.74}
  9%|▊         | 13/150 [08:23<1:09:01, 30.23s/it]  9%|▉         | 14/150 [08:55<1:09:24, 30.62s/it]                                                  {'loss': 0.9644, 'learning_rate': 9.066666666666667e-05, 'epoch': 0.9, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 48.28, 'total_memory_available (GB)': 93.74}
  9%|▉         | 14/150 [08:55<1:09:24, 30.62s/it] 10%|█         | 15/150 [09:24<1:07:49, 30.14s/it]                                                  {'loss': 0.9907, 'learning_rate': 9e-05, 'epoch': 0.97, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 48.28, 'total_memory_available (GB)': 93.74}
 10%|█         | 15/150 [09:24<1:07:49, 30.14s/it] 11%|█         | 16/150 [10:04<1:14:05, 33.18s/it]                                                  {'loss': 1.1548, 'learning_rate': 8.933333333333334e-05, 'epoch': 1.06, 'memory_allocated (GB)': 2.85, 'max_memory_allocated (GB)': 48.28, 'total_memory_available (GB)': 93.74}
 11%|█         | 16/150 [10:04<1:14:05, 33.18s/it] 11%|█▏        | 17/150 [10:34<1:11:36, 32.30s/it]                                                  {'loss': 0.6521, 'learning_rate': 8.866666666666668e-05, 'epoch': 1.13, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 48.28, 'total_memory_available (GB)': 93.74}
 11%|█▏        | 17/150 [10:34<1:11:36, 32.30s/it] 12%|█▏        | 18/150 [11:05<1:10:07, 31.87s/it]                                                  {'loss': 0.6069, 'learning_rate': 8.800000000000001e-05, 'epoch': 1.19, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 48.28, 'total_memory_available (GB)': 93.74}
 12%|█▏        | 18/150 [11:05<1:10:07, 31.87s/it] 13%|█▎        | 19/150 [11:34<1:07:44, 31.03s/it]                                                  {'loss': 0.5515, 'learning_rate': 8.733333333333333e-05, 'epoch': 1.26, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 48.28, 'total_memory_available (GB)': 93.74}
 13%|█▎        | 19/150 [11:34<1:07:44, 31.03s/it] 13%|█▎        | 20/150 [12:05<1:06:53, 30.88s/it]                                                  {'loss': 0.5518, 'learning_rate': 8.666666666666667e-05, 'epoch': 1.32, 'memory_allocated (GB)': 2.89, 'max_memory_allocated (GB)': 48.28, 'total_memory_available (GB)': 93.74}
 13%|█▎        | 20/150 [12:05<1:06:53, 30.88s/it] 14%|█▍        | 21/150 [12:35<1:05:49, 30.62s/it]                                                  {'loss': 0.562, 'learning_rate': 8.6e-05, 'epoch': 1.39, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 48.28, 'total_memory_available (GB)': 93.74}
 14%|█▍        | 21/150 [12:35<1:05:49, 30.62s/it] 15%|█▍        | 22/150 [13:05<1:04:58, 30.46s/it]                                                  {'loss': 0.5364, 'learning_rate': 8.533333333333334e-05, 'epoch': 1.45, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 48.28, 'total_memory_available (GB)': 93.74}
 15%|█▍        | 22/150 [13:05<1:04:58, 30.46s/it] 15%|█▌        | 23/150 [13:35<1:04:02, 30.26s/it]                                                  {'loss': 0.5054, 'learning_rate': 8.466666666666667e-05, 'epoch': 1.52, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 48.28, 'total_memory_available (GB)': 93.74}
 15%|█▌        | 23/150 [13:35<1:04:02, 30.26s/it] 16%|█▌        | 24/150 [14:04<1:03:15, 30.13s/it]                                                  {'loss': 0.5161, 'learning_rate': 8.4e-05, 'epoch': 1.58, 'memory_allocated (GB)': 2.87, 'max_memory_allocated (GB)': 48.28, 'total_memory_available (GB)': 93.74}
 16%|█▌        | 24/150 [14:05<1:03:15, 30.13s/it] 17%|█▋        | 25/150 [14:35<1:03:05, 30.28s/it]                                                  {'loss': 0.5298, 'learning_rate': 8.333333333333334e-05, 'epoch': 1.65, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 48.28, 'total_memory_available (GB)': 93.74}
 17%|█▋        | 25/150 [14:35<1:03:05, 30.28s/it] 17%|█▋        | 26/150 [15:06<1:02:44, 30.36s/it]                                                  {'loss': 0.502, 'learning_rate': 8.266666666666667e-05, 'epoch': 1.71, 'memory_allocated (GB)': 2.86, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 17%|█▋        | 26/150 [15:06<1:02:44, 30.36s/it] 18%|█▊        | 27/150 [15:36<1:02:30, 30.49s/it]                                                  {'loss': 0.4683, 'learning_rate': 8.2e-05, 'epoch': 1.77, 'memory_allocated (GB)': 2.84, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 18%|█▊        | 27/150 [15:37<1:02:30, 30.49s/it] 19%|█▊        | 28/150 [16:07<1:01:51, 30.42s/it]                                                  {'loss': 0.5793, 'learning_rate': 8.133333333333334e-05, 'epoch': 1.84, 'memory_allocated (GB)': 2.86, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 19%|█▊        | 28/150 [16:07<1:01:51, 30.42s/it] 19%|█▉        | 29/150 [16:36<1:00:27, 29.98s/it]                                                  {'loss': 0.5557, 'learning_rate': 8.066666666666667e-05, 'epoch': 1.9, 'memory_allocated (GB)': 2.84, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 19%|█▉        | 29/150 [16:36<1:00:27, 29.98s/it] 20%|██        | 30/150 [17:06<59:58, 29.98s/it]                                                  {'loss': 0.5281, 'learning_rate': 8e-05, 'epoch': 1.97, 'memory_allocated (GB)': 2.85, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 20%|██        | 30/150 [17:06<59:58, 29.98s/it][2023-08-21 14:40:11,085] [INFO] [timer.py:241:stop] 0/64, RunningAvgSamplesPerSec=1.0367661691328194, CurrSamplesPerSec=0.6915339448083506, MemAllocated=2.17GB, MaxMemAllocated=49.24GB
 21%|██        | 31/150 [17:57<1:11:52, 36.24s/it]                                                  {'loss': 0.5574, 'learning_rate': 7.866666666666666e-05, 'epoch': 2.06, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 21%|██        | 31/150 [17:57<1:11:52, 36.24s/it] 21%|██▏       | 32/150 [18:26<1:07:22, 34.26s/it]                                                  {'loss': 0.2598, 'learning_rate': 7.800000000000001e-05, 'epoch': 2.13, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 21%|██▏       | 32/150 [18:26<1:07:22, 34.26s/it] 22%|██▏       | 33/150 [18:55<1:03:54, 32.77s/it]                                                  {'loss': 0.2538, 'learning_rate': 7.733333333333333e-05, 'epoch': 2.19, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 22%|██▏       | 33/150 [18:56<1:03:54, 32.77s/it] 23%|██▎       | 34/150 [19:27<1:02:27, 32.31s/it]                                                  {'loss': 0.5481, 'learning_rate': 7.666666666666667e-05, 'epoch': 2.26, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 23%|██▎       | 34/150 [19:27<1:02:27, 32.31s/it] 23%|██▎       | 35/150 [19:57<1:00:59, 31.82s/it]                                                  {'loss': 0.23, 'learning_rate': 7.6e-05, 'epoch': 2.32, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 23%|██▎       | 35/150 [19:57<1:00:59, 31.82s/it] 24%|██▍       | 36/150 [20:27<58:58, 31.04s/it]                                                  {'loss': 0.306, 'learning_rate': 7.533333333333334e-05, 'epoch': 2.39, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 24%|██▍       | 36/150 [20:27<58:58, 31.04s/it] 25%|██▍       | 37/150 [20:56<57:44, 30.66s/it]                                                {'loss': 0.2771, 'learning_rate': 7.466666666666667e-05, 'epoch': 2.45, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 25%|██▍       | 37/150 [20:56<57:44, 30.66s/it] 25%|██▌       | 38/150 [21:26<56:38, 30.34s/it]                                                {'loss': 0.2393, 'learning_rate': 7.4e-05, 'epoch': 2.52, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 25%|██▌       | 38/150 [21:26<56:38, 30.34s/it] 26%|██▌       | 39/150 [21:55<55:28, 29.98s/it]                                                {'loss': 0.2646, 'learning_rate': 7.333333333333333e-05, 'epoch': 2.58, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 26%|██▌       | 39/150 [21:55<55:28, 29.98s/it] 27%|██▋       | 40/150 [22:25<54:56, 29.97s/it]                                                {'loss': 0.4672, 'learning_rate': 7.266666666666667e-05, 'epoch': 2.65, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 27%|██▋       | 40/150 [22:25<54:56, 29.97s/it] 27%|██▋       | 41/150 [22:54<53:58, 29.71s/it]                                                {'loss': 0.2446, 'learning_rate': 7.2e-05, 'epoch': 2.71, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 27%|██▋       | 41/150 [22:54<53:58, 29.71s/it] 28%|██▊       | 42/150 [23:25<53:53, 29.94s/it]                                                {'loss': 0.2572, 'learning_rate': 7.133333333333334e-05, 'epoch': 2.77, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 28%|██▊       | 42/150 [23:25<53:53, 29.94s/it] 29%|██▊       | 43/150 [23:55<53:36, 30.06s/it]                                                {'loss': 0.5028, 'learning_rate': 7.066666666666667e-05, 'epoch': 2.84, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 29%|██▊       | 43/150 [23:55<53:36, 30.06s/it] 29%|██▉       | 44/150 [24:25<52:53, 29.94s/it]                                                {'loss': 0.4269, 'learning_rate': 7e-05, 'epoch': 2.9, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 29%|██▉       | 44/150 [24:25<52:53, 29.94s/it] 30%|███       | 45/150 [24:55<52:52, 30.21s/it]                                                {'loss': 0.265, 'learning_rate': 6.933333333333334e-05, 'epoch': 2.97, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 30%|███       | 45/150 [24:55<52:52, 30.21s/it] 31%|███       | 46/150 [25:32<55:51, 32.22s/it]                                                {'loss': 0.2653, 'learning_rate': 6.866666666666666e-05, 'epoch': 3.06, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 31%|███       | 46/150 [25:32<55:51, 32.22s/it] 31%|███▏      | 47/150 [26:01<53:23, 31.11s/it]                                                {'loss': 0.1591, 'learning_rate': 6.800000000000001e-05, 'epoch': 3.13, 'memory_allocated (GB)': 2.84, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 31%|███▏      | 47/150 [26:01<53:23, 31.11s/it] 32%|███▏      | 48/150 [26:31<52:24, 30.83s/it]                                                {'loss': 0.9362, 'learning_rate': 6.733333333333333e-05, 'epoch': 3.19, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 32%|███▏      | 48/150 [26:31<52:24, 30.83s/it] 33%|███▎      | 49/150 [27:00<51:01, 30.31s/it]                                                {'loss': 0.155, 'learning_rate': 6.666666666666667e-05, 'epoch': 3.26, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 33%|███▎      | 49/150 [27:00<51:01, 30.31s/it] 33%|███▎      | 50/150 [27:31<50:32, 30.32s/it]                                                {'loss': 0.3856, 'learning_rate': 6.6e-05, 'epoch': 3.32, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 33%|███▎      | 50/150 [27:31<50:32, 30.32s/it] 34%|███▍      | 51/150 [27:59<48:59, 29.69s/it]                                                {'loss': 0.379, 'learning_rate': 6.533333333333334e-05, 'epoch': 3.39, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 34%|███▍      | 51/150 [27:59<48:59, 29.69s/it] 35%|███▍      | 52/150 [28:29<48:34, 29.74s/it]                                                {'loss': 0.3582, 'learning_rate': 6.466666666666666e-05, 'epoch': 3.45, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 35%|███▍      | 52/150 [28:29<48:34, 29.74s/it] 35%|███▌      | 53/150 [28:57<47:25, 29.34s/it]                                                {'loss': 0.1725, 'learning_rate': 6.400000000000001e-05, 'epoch': 3.52, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 35%|███▌      | 53/150 [28:57<47:25, 29.34s/it] 36%|███▌      | 54/150 [29:27<47:05, 29.43s/it]                                                {'loss': 0.1516, 'learning_rate': 6.333333333333333e-05, 'epoch': 3.58, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 36%|███▌      | 54/150 [29:27<47:05, 29.43s/it] 37%|███▋      | 55/150 [29:56<46:22, 29.29s/it]                                                {'loss': 0.1581, 'learning_rate': 6.266666666666667e-05, 'epoch': 3.65, 'memory_allocated (GB)': 2.84, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 37%|███▋      | 55/150 [29:56<46:22, 29.29s/it] 37%|███▋      | 56/150 [30:24<45:37, 29.12s/it]                                                {'loss': 0.7494, 'learning_rate': 6.2e-05, 'epoch': 3.71, 'memory_allocated (GB)': 2.88, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 37%|███▋      | 56/150 [30:25<45:37, 29.12s/it] 38%|███▊      | 57/150 [30:55<45:39, 29.46s/it]                                                {'loss': 0.3579, 'learning_rate': 6.133333333333334e-05, 'epoch': 3.77, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 38%|███▊      | 57/150 [30:55<45:39, 29.46s/it] 39%|███▊      | 58/150 [31:24<45:07, 29.42s/it]                                                {'loss': 0.1466, 'learning_rate': 6.066666666666667e-05, 'epoch': 3.84, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 39%|███▊      | 58/150 [31:24<45:07, 29.42s/it] 39%|███▉      | 59/150 [31:54<45:06, 29.74s/it]                                                {'loss': 0.1528, 'learning_rate': 6e-05, 'epoch': 3.9, 'memory_allocated (GB)': 2.85, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 39%|███▉      | 59/150 [31:55<45:06, 29.74s/it] 40%|████      | 60/150 [32:24<44:24, 29.60s/it]                                                {'loss': 0.1475, 'learning_rate': 5.9333333333333343e-05, 'epoch': 3.97, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 40%|████      | 60/150 [32:24<44:24, 29.60s/it] 41%|████      | 61/150 [33:16<54:14, 36.57s/it]                                                {'loss': 0.1575, 'learning_rate': 5.8e-05, 'epoch': 4.06, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 41%|████      | 61/150 [33:17<54:14, 36.57s/it][2023-08-21 14:55:59,862] [INFO] [logging.py:68:log_dist] [Rank 0] step=64, skipped=0, lr=[5.7333333333333336e-05], mom=[[0.9, 0.999]]
[2023-08-21 14:55:59,869] [INFO] [timer.py:241:stop] 0/128, RunningAvgSamplesPerSec=1.0608048101224454, CurrSamplesPerSec=0.7398456396352745, MemAllocated=2.17GB, MaxMemAllocated=49.24GB
 41%|████▏     | 62/150 [33:45<50:13, 34.24s/it]                                                {'loss': 0.1035, 'learning_rate': 5.7333333333333336e-05, 'epoch': 4.13, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 41%|████▏     | 62/150 [33:45<50:13, 34.24s/it] 42%|████▏     | 63/150 [34:14<47:24, 32.70s/it]                                                {'loss': 0.0876, 'learning_rate': 5.666666666666667e-05, 'epoch': 4.19, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 42%|████▏     | 63/150 [34:14<47:24, 32.70s/it] 43%|████▎     | 64/150 [34:44<45:41, 31.88s/it]                                                {'loss': 0.1008, 'learning_rate': 5.6000000000000006e-05, 'epoch': 4.26, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 43%|████▎     | 64/150 [34:44<45:41, 31.88s/it] 43%|████▎     | 65/150 [35:13<43:42, 30.86s/it]                                                {'loss': 0.085, 'learning_rate': 5.5333333333333334e-05, 'epoch': 4.32, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 43%|████▎     | 65/150 [35:13<43:42, 30.86s/it] 44%|████▍     | 66/150 [35:42<42:29, 30.36s/it]                                                {'loss': 0.7375, 'learning_rate': 5.466666666666666e-05, 'epoch': 4.39, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 44%|████▍     | 66/150 [35:42<42:29, 30.36s/it] 45%|████▍     | 67/150 [36:12<41:57, 30.33s/it]                                                {'loss': 0.5717, 'learning_rate': 5.4000000000000005e-05, 'epoch': 4.45, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 45%|████▍     | 67/150 [36:12<41:57, 30.33s/it] 45%|████▌     | 68/150 [36:42<41:01, 30.02s/it]                                                {'loss': 0.2996, 'learning_rate': 5.333333333333333e-05, 'epoch': 4.52, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 45%|████▌     | 68/150 [36:42<41:01, 30.02s/it] 46%|████▌     | 69/150 [37:11<40:08, 29.73s/it]                                                {'loss': 0.0928, 'learning_rate': 5.266666666666666e-05, 'epoch': 4.58, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 46%|████▌     | 69/150 [37:11<40:08, 29.73s/it] 47%|████▋     | 70/150 [37:41<39:59, 29.99s/it]                                                {'loss': 0.3287, 'learning_rate': 5.2000000000000004e-05, 'epoch': 4.65, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 47%|████▋     | 70/150 [37:41<39:59, 29.99s/it] 47%|████▋     | 71/150 [38:11<39:13, 29.79s/it]                                                {'loss': 0.098, 'learning_rate': 5.133333333333333e-05, 'epoch': 4.71, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 47%|████▋     | 71/150 [38:11<39:13, 29.79s/it] 48%|████▊     | 72/150 [38:39<38:21, 29.51s/it]                                                {'loss': 0.3015, 'learning_rate': 5.0666666666666674e-05, 'epoch': 4.77, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 48%|████▊     | 72/150 [38:39<38:21, 29.51s/it] 49%|████▊     | 73/150 [39:09<38:05, 29.68s/it]                                                {'loss': 0.0912, 'learning_rate': 5e-05, 'epoch': 4.84, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 49%|████▊     | 73/150 [39:10<38:05, 29.68s/it] 49%|████▉     | 74/150 [39:38<37:08, 29.33s/it]                                                {'loss': 0.0875, 'learning_rate': 4.933333333333334e-05, 'epoch': 4.9, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 49%|████▉     | 74/150 [39:38<37:08, 29.33s/it] 50%|█████     | 75/150 [40:07<36:29, 29.20s/it]                                                {'loss': 0.2867, 'learning_rate': 4.866666666666667e-05, 'epoch': 4.97, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.24, 'total_memory_available (GB)': 93.74}
 50%|█████     | 75/150 [40:07<36:29, 29.20s/it] 51%|█████     | 76/150 [40:44<38:53, 31.54s/it]                                                {'loss': 0.0943, 'learning_rate': 4.8e-05, 'epoch': 5.06, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 51%|█████     | 76/150 [40:44<38:53, 31.54s/it] 51%|█████▏    | 77/150 [41:14<37:53, 31.15s/it]                                                {'loss': 0.0529, 'learning_rate': 4.7333333333333336e-05, 'epoch': 5.13, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 51%|█████▏    | 77/150 [41:14<37:53, 31.15s/it] 52%|█████▏    | 78/150 [41:44<36:45, 30.63s/it]                                                {'loss': 0.4298, 'learning_rate': 4.666666666666667e-05, 'epoch': 5.19, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 52%|█████▏    | 78/150 [41:44<36:45, 30.63s/it] 53%|█████▎    | 79/150 [42:13<35:49, 30.27s/it]                                                {'loss': 0.2502, 'learning_rate': 4.600000000000001e-05, 'epoch': 5.26, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 53%|█████▎    | 79/150 [42:13<35:49, 30.27s/it] 53%|█████▎    | 80/150 [42:43<35:14, 30.21s/it]                                                {'loss': 0.048, 'learning_rate': 4.5333333333333335e-05, 'epoch': 5.32, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 53%|█████▎    | 80/150 [42:43<35:14, 30.21s/it] 54%|█████▍    | 81/150 [43:12<34:28, 29.98s/it]                                                {'loss': 0.0476, 'learning_rate': 4.466666666666667e-05, 'epoch': 5.39, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 54%|█████▍    | 81/150 [43:13<34:28, 29.98s/it] 55%|█████▍    | 82/150 [43:41<33:38, 29.68s/it]                                                {'loss': 0.0477, 'learning_rate': 4.4000000000000006e-05, 'epoch': 5.45, 'memory_allocated (GB)': 2.88, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 55%|█████▍    | 82/150 [43:42<33:38, 29.68s/it] 55%|█████▌    | 83/150 [44:12<33:17, 29.81s/it]                                                {'loss': 0.0344, 'learning_rate': 4.3333333333333334e-05, 'epoch': 5.52, 'memory_allocated (GB)': 2.84, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 55%|█████▌    | 83/150 [44:12<33:17, 29.81s/it] 56%|█████▌    | 84/150 [44:41<32:34, 29.61s/it]                                                {'loss': 0.2291, 'learning_rate': 4.266666666666667e-05, 'epoch': 5.58, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 56%|█████▌    | 84/150 [44:41<32:34, 29.61s/it] 57%|█████▋    | 85/150 [45:10<31:49, 29.37s/it]                                                {'loss': 0.2308, 'learning_rate': 4.2e-05, 'epoch': 5.65, 'memory_allocated (GB)': 2.85, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 57%|█████▋    | 85/150 [45:10<31:49, 29.37s/it] 57%|█████▋    | 86/150 [45:40<31:37, 29.65s/it]                                                {'loss': 0.0484, 'learning_rate': 4.133333333333333e-05, 'epoch': 5.71, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 57%|█████▋    | 86/150 [45:40<31:37, 29.65s/it] 58%|█████▊    | 87/150 [46:08<30:43, 29.26s/it]                                                {'loss': 0.2275, 'learning_rate': 4.066666666666667e-05, 'epoch': 5.77, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 58%|█████▊    | 87/150 [46:08<30:43, 29.26s/it] 59%|█████▊    | 88/150 [46:38<30:24, 29.42s/it]                                                {'loss': 0.0362, 'learning_rate': 4e-05, 'epoch': 5.84, 'memory_allocated (GB)': 2.86, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 59%|█████▊    | 88/150 [46:38<30:24, 29.42s/it] 59%|█████▉    | 89/150 [47:07<29:52, 29.38s/it]                                                {'loss': 0.2387, 'learning_rate': 3.933333333333333e-05, 'epoch': 5.9, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 59%|█████▉    | 89/150 [47:07<29:52, 29.38s/it] 60%|██████    | 90/150 [47:37<29:36, 29.61s/it]                                                {'loss': 0.0413, 'learning_rate': 3.866666666666667e-05, 'epoch': 5.97, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 60%|██████    | 90/150 [47:38<29:36, 29.61s/it] 61%|██████    | 91/150 [48:31<36:04, 36.68s/it]                                                {'loss': 0.2174, 'learning_rate': 3.733333333333334e-05, 'epoch': 6.06, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 61%|██████    | 91/150 [48:31<36:04, 36.68s/it] 61%|██████▏   | 92/150 [49:01<33:35, 34.76s/it]                                                {'loss': 0.0276, 'learning_rate': 3.6666666666666666e-05, 'epoch': 6.13, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 61%|██████▏   | 92/150 [49:01<33:35, 34.76s/it][2023-08-21 15:11:45,478] [INFO] [timer.py:241:stop] 0/192, RunningAvgSamplesPerSec=1.0701864426695187, CurrSamplesPerSec=0.7001910225644472, MemAllocated=2.17GB, MaxMemAllocated=49.33GB
 62%|██████▏   | 93/150 [49:31<31:40, 33.34s/it]                                                {'loss': 0.0182, 'learning_rate': 3.6e-05, 'epoch': 6.19, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 62%|██████▏   | 93/150 [49:31<31:40, 33.34s/it] 63%|██████▎   | 94/150 [50:00<29:55, 32.06s/it]                                                {'loss': 0.0213, 'learning_rate': 3.5333333333333336e-05, 'epoch': 6.26, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 63%|██████▎   | 94/150 [50:00<29:55, 32.06s/it] 63%|██████▎   | 95/150 [50:30<28:44, 31.36s/it]                                                {'loss': 0.1965, 'learning_rate': 3.466666666666667e-05, 'epoch': 6.32, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 63%|██████▎   | 95/150 [50:30<28:44, 31.36s/it] 64%|██████▍   | 96/150 [50:59<27:41, 30.76s/it]                                                {'loss': 0.0206, 'learning_rate': 3.4000000000000007e-05, 'epoch': 6.39, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 64%|██████▍   | 96/150 [50:59<27:41, 30.76s/it] 65%|██████▍   | 97/150 [51:29<27:01, 30.59s/it]                                                {'loss': 0.1965, 'learning_rate': 3.3333333333333335e-05, 'epoch': 6.45, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 65%|██████▍   | 97/150 [51:29<27:01, 30.59s/it] 65%|██████▌   | 98/150 [51:59<26:16, 30.31s/it]                                                {'loss': 0.0219, 'learning_rate': 3.266666666666667e-05, 'epoch': 6.52, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 65%|██████▌   | 98/150 [51:59<26:16, 30.31s/it] 66%|██████▌   | 99/150 [52:27<25:17, 29.75s/it]                                                {'loss': 0.3859, 'learning_rate': 3.2000000000000005e-05, 'epoch': 6.58, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 66%|██████▌   | 99/150 [52:27<25:17, 29.75s/it] 67%|██████▋   | 100/150 [52:58<24:54, 29.88s/it]                                                 {'loss': 0.1957, 'learning_rate': 3.1333333333333334e-05, 'epoch': 6.65, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 67%|██████▋   | 100/150 [52:58<24:54, 29.88s/it] 67%|██████▋   | 101/150 [53:26<24:05, 29.49s/it]                                                 {'loss': 0.019, 'learning_rate': 3.066666666666667e-05, 'epoch': 6.71, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 67%|██████▋   | 101/150 [53:26<24:05, 29.49s/it] 68%|██████▊   | 102/150 [53:55<23:30, 29.39s/it]                                                 {'loss': 0.0211, 'learning_rate': 3e-05, 'epoch': 6.77, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 68%|██████▊   | 102/150 [53:55<23:30, 29.39s/it] 69%|██████▊   | 103/150 [54:23<22:37, 28.88s/it]                                                 {'loss': 0.0172, 'learning_rate': 2.9333333333333336e-05, 'epoch': 6.84, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 69%|██████▊   | 103/150 [54:23<22:37, 28.88s/it] 69%|██████▉   | 104/150 [54:51<21:59, 28.69s/it]                                                 {'loss': 0.0186, 'learning_rate': 2.8666666666666668e-05, 'epoch': 6.9, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 69%|██████▉   | 104/150 [54:51<21:59, 28.69s/it] 70%|███████   | 105/150 [55:20<21:36, 28.80s/it]                                                 {'loss': 0.019, 'learning_rate': 2.8000000000000003e-05, 'epoch': 6.97, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 70%|███████   | 105/150 [55:20<21:36, 28.80s/it] 71%|███████   | 106/150 [55:55<22:31, 30.71s/it]                                                 {'loss': 0.3671, 'learning_rate': 2.733333333333333e-05, 'epoch': 7.06, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 71%|███████   | 106/150 [55:56<22:31, 30.71s/it] 71%|███████▏  | 107/150 [56:25<21:42, 30.29s/it]                                                 {'loss': 0.0128, 'learning_rate': 2.6666666666666667e-05, 'epoch': 7.13, 'memory_allocated (GB)': 2.88, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 71%|███████▏  | 107/150 [56:25<21:42, 30.29s/it] 72%|███████▏  | 108/150 [56:55<21:10, 30.25s/it]                                                 {'loss': 0.021, 'learning_rate': 2.6000000000000002e-05, 'epoch': 7.19, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 72%|███████▏  | 108/150 [56:55<21:10, 30.25s/it] 73%|███████▎  | 109/150 [57:25<20:33, 30.09s/it]                                                 {'loss': 0.3566, 'learning_rate': 2.5333333333333337e-05, 'epoch': 7.26, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 73%|███████▎  | 109/150 [57:25<20:33, 30.09s/it] 73%|███████▎  | 110/150 [57:53<19:44, 29.60s/it]                                                 {'loss': 0.0116, 'learning_rate': 2.466666666666667e-05, 'epoch': 7.32, 'memory_allocated (GB)': 2.84, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 73%|███████▎  | 110/150 [57:53<19:44, 29.60s/it] 74%|███████▍  | 111/150 [58:23<19:15, 29.62s/it]                                                 {'loss': 0.0132, 'learning_rate': 2.4e-05, 'epoch': 7.39, 'memory_allocated (GB)': 2.88, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 74%|███████▍  | 111/150 [58:23<19:15, 29.62s/it] 75%|███████▍  | 112/150 [58:52<18:42, 29.55s/it]                                                 {'loss': 0.1925, 'learning_rate': 2.3333333333333336e-05, 'epoch': 7.45, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 75%|███████▍  | 112/150 [58:52<18:42, 29.55s/it] 75%|███████▌  | 113/150 [59:22<18:14, 29.57s/it]                                                 {'loss': 0.1868, 'learning_rate': 2.2666666666666668e-05, 'epoch': 7.52, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 75%|███████▌  | 113/150 [59:22<18:14, 29.57s/it] 76%|███████▌  | 114/150 [59:51<17:42, 29.51s/it]                                                 {'loss': 0.1862, 'learning_rate': 2.2000000000000003e-05, 'epoch': 7.58, 'memory_allocated (GB)': 2.85, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 76%|███████▌  | 114/150 [59:51<17:42, 29.51s/it] 77%|███████▋  | 115/150 [1:00:20<17:10, 29.44s/it]                                                   {'loss': 0.012, 'learning_rate': 2.1333333333333335e-05, 'epoch': 7.65, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 77%|███████▋  | 115/150 [1:00:20<17:10, 29.44s/it] 77%|███████▋  | 116/150 [1:00:51<16:53, 29.80s/it]                                                   {'loss': 0.0121, 'learning_rate': 2.0666666666666666e-05, 'epoch': 7.71, 'memory_allocated (GB)': 2.84, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 77%|███████▋  | 116/150 [1:00:51<16:53, 29.80s/it] 78%|███████▊  | 117/150 [1:01:19<16:08, 29.35s/it]                                                   {'loss': 0.0127, 'learning_rate': 2e-05, 'epoch': 7.77, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 78%|███████▊  | 117/150 [1:01:20<16:08, 29.35s/it] 79%|███████▊  | 118/150 [1:01:48<15:30, 29.07s/it]                                                   {'loss': 0.0114, 'learning_rate': 1.9333333333333333e-05, 'epoch': 7.84, 'memory_allocated (GB)': 2.85, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 79%|███████▊  | 118/150 [1:01:48<15:30, 29.07s/it] 79%|███████▉  | 119/150 [1:02:18<15:14, 29.51s/it]                                                   {'loss': 0.0123, 'learning_rate': 1.866666666666667e-05, 'epoch': 7.9, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 79%|███████▉  | 119/150 [1:02:18<15:14, 29.51s/it] 80%|████████  | 120/150 [1:02:47<14:33, 29.12s/it]                                                   {'loss': 0.0138, 'learning_rate': 1.8e-05, 'epoch': 7.97, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 80%|████████  | 120/150 [1:02:47<14:33, 29.12s/it] 81%|████████  | 121/150 [1:03:38<17:19, 35.85s/it]                                                   {'loss': 0.019, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.06, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 81%|████████  | 121/150 [1:03:38<17:19, 35.85s/it] 81%|████████▏ | 122/150 [1:04:06<15:38, 33.52s/it]                                                   {'loss': 0.0111, 'learning_rate': 1.6000000000000003e-05, 'epoch': 8.13, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 81%|████████▏ | 122/150 [1:04:06<15:38, 33.52s/it] 82%|████████▏ | 123/150 [1:04:34<14:22, 31.94s/it]                                                   {'loss': 0.0107, 'learning_rate': 1.5333333333333334e-05, 'epoch': 8.19, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 82%|████████▏ | 123/150 [1:04:34<14:22, 31.94s/it][2023-08-21 15:27:17,603] [INFO] [logging.py:68:log_dist] [Rank 0] step=128, skipped=0, lr=[1.4666666666666668e-05], mom=[[0.9, 0.999]]
[2023-08-21 15:27:17,610] [INFO] [timer.py:241:stop] 0/256, RunningAvgSamplesPerSec=1.0787038654503636, CurrSamplesPerSec=0.735313026881904, MemAllocated=2.17GB, MaxMemAllocated=49.33GB
 83%|████████▎ | 124/150 [1:05:03<13:24, 30.95s/it]                                                   {'loss': 0.012, 'learning_rate': 1.4666666666666668e-05, 'epoch': 8.26, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 83%|████████▎ | 124/150 [1:05:03<13:24, 30.95s/it] 83%|████████▎ | 125/150 [1:05:32<12:37, 30.29s/it]                                                   {'loss': 0.18, 'learning_rate': 1.4000000000000001e-05, 'epoch': 8.32, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 83%|████████▎ | 125/150 [1:05:32<12:37, 30.29s/it] 84%|████████▍ | 126/150 [1:06:01<11:57, 29.88s/it]                                                   {'loss': 0.0103, 'learning_rate': 1.3333333333333333e-05, 'epoch': 8.39, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 84%|████████▍ | 126/150 [1:06:01<11:57, 29.88s/it] 85%|████████▍ | 127/150 [1:06:30<11:20, 29.58s/it]                                                   {'loss': 0.3461, 'learning_rate': 1.2666666666666668e-05, 'epoch': 8.45, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 85%|████████▍ | 127/150 [1:06:30<11:20, 29.58s/it] 85%|████████▌ | 128/150 [1:06:58<10:45, 29.34s/it]                                                   {'loss': 0.1876, 'learning_rate': 1.2e-05, 'epoch': 8.52, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 85%|████████▌ | 128/150 [1:06:58<10:45, 29.34s/it] 86%|████████▌ | 129/150 [1:07:27<10:14, 29.24s/it]                                                   {'loss': 0.1809, 'learning_rate': 1.1333333333333334e-05, 'epoch': 8.58, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 86%|████████▌ | 129/150 [1:07:27<10:14, 29.24s/it] 87%|████████▋ | 130/150 [1:07:57<09:46, 29.34s/it]                                                   {'loss': 0.0102, 'learning_rate': 1.0666666666666667e-05, 'epoch': 8.65, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 87%|████████▋ | 130/150 [1:07:57<09:46, 29.34s/it] 87%|████████▋ | 131/150 [1:08:26<09:15, 29.25s/it]                                                   {'loss': 0.0121, 'learning_rate': 1e-05, 'epoch': 8.71, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 87%|████████▋ | 131/150 [1:08:26<09:15, 29.25s/it] 88%|████████▊ | 132/150 [1:08:55<08:43, 29.11s/it]                                                   {'loss': 0.0107, 'learning_rate': 9.333333333333334e-06, 'epoch': 8.77, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 88%|████████▊ | 132/150 [1:08:55<08:43, 29.11s/it] 89%|████████▊ | 133/150 [1:09:24<08:14, 29.12s/it]                                                   {'loss': 0.0107, 'learning_rate': 8.666666666666668e-06, 'epoch': 8.84, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 89%|████████▊ | 133/150 [1:09:24<08:14, 29.12s/it] 89%|████████▉ | 134/150 [1:09:53<07:45, 29.10s/it]                                                   {'loss': 0.183, 'learning_rate': 8.000000000000001e-06, 'epoch': 8.9, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 89%|████████▉ | 134/150 [1:09:53<07:45, 29.10s/it] 90%|█████████ | 135/150 [1:10:21<07:11, 28.76s/it]                                                   {'loss': 0.0131, 'learning_rate': 7.333333333333334e-06, 'epoch': 8.97, 'memory_allocated (GB)': 2.17, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 90%|█████████ | 135/150 [1:10:21<07:11, 28.76s/it] 91%|█████████ | 136/150 [1:10:56<07:08, 30.61s/it]                                                   {'loss': 0.0167, 'learning_rate': 6.666666666666667e-06, 'epoch': 9.06, 'memory_allocated (GB)': 2.88, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 91%|█████████ | 136/150 [1:10:56<07:08, 30.61s/it] 91%|█████████▏| 137/150 [1:11:26<06:34, 30.33s/it]                                                   {'loss': 0.1738, 'learning_rate': 6e-06, 'epoch': 9.13, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 91%|█████████▏| 137/150 [1:11:26<06:34, 30.33s/it] 92%|█████████▏| 138/150 [1:11:55<06:00, 30.02s/it]                                                   {'loss': 0.1774, 'learning_rate': 5.333333333333334e-06, 'epoch': 9.19, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 92%|█████████▏| 138/150 [1:11:55<06:00, 30.02s/it] 93%|█████████▎| 139/150 [1:12:24<05:28, 29.90s/it]                                                   {'loss': 0.1796, 'learning_rate': 4.666666666666667e-06, 'epoch': 9.26, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 93%|█████████▎| 139/150 [1:12:25<05:28, 29.90s/it] 93%|█████████▎| 140/150 [1:12:54<04:58, 29.84s/it]                                                   {'loss': 0.1838, 'learning_rate': 4.000000000000001e-06, 'epoch': 9.32, 'memory_allocated (GB)': 2.84, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 93%|█████████▎| 140/150 [1:12:54<04:58, 29.84s/it] 94%|█████████▍| 141/150 [1:13:22<04:23, 29.23s/it]                                                   {'loss': 0.0108, 'learning_rate': 3.3333333333333333e-06, 'epoch': 9.39, 'memory_allocated (GB)': 2.86, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 94%|█████████▍| 141/150 [1:13:22<04:23, 29.23s/it] 95%|█████████▍| 142/150 [1:13:51<03:53, 29.13s/it]                                                   {'loss': 0.0208, 'learning_rate': 2.666666666666667e-06, 'epoch': 9.45, 'memory_allocated (GB)': 2.88, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 95%|█████████▍| 142/150 [1:13:51<03:53, 29.13s/it] 95%|█████████▌| 143/150 [1:14:20<03:23, 29.09s/it]                                                   {'loss': 0.0103, 'learning_rate': 2.0000000000000003e-06, 'epoch': 9.52, 'memory_allocated (GB)': 2.88, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 95%|█████████▌| 143/150 [1:14:20<03:23, 29.09s/it] 96%|█████████▌| 144/150 [1:14:48<02:52, 28.76s/it]                                                   {'loss': 0.0108, 'learning_rate': 1.3333333333333334e-06, 'epoch': 9.58, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 96%|█████████▌| 144/150 [1:14:48<02:52, 28.76s/it] 97%|█████████▋| 145/150 [1:15:18<02:25, 29.14s/it]                                                   {'loss': 0.0098, 'learning_rate': 6.666666666666667e-07, 'epoch': 9.65, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 97%|█████████▋| 145/150 [1:15:18<02:25, 29.14s/it] 97%|█████████▋| 146/150 [1:15:46<01:55, 28.80s/it]                                                   {'loss': 0.0105, 'learning_rate': 0.0, 'epoch': 9.71, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 97%|█████████▋| 146/150 [1:15:46<01:55, 28.80s/it] 98%|█████████▊| 147/150 [1:16:16<01:27, 29.05s/it]                                                   {'loss': 0.0103, 'learning_rate': 0.0, 'epoch': 9.77, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 98%|█████████▊| 147/150 [1:16:16<01:27, 29.05s/it] 99%|█████████▊| 148/150 [1:16:46<00:58, 29.38s/it]                                                   {'loss': 0.1799, 'learning_rate': 0.0, 'epoch': 9.84, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 99%|█████████▊| 148/150 [1:16:46<00:58, 29.38s/it] 99%|█████████▉| 149/150 [1:17:15<00:29, 29.36s/it]                                                   {'loss': 0.0098, 'learning_rate': 0.0, 'epoch': 9.9, 'memory_allocated (GB)': 2.83, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
 99%|█████████▉| 149/150 [1:17:15<00:29, 29.36s/it]100%|██████████| 150/150 [1:17:43<00:00, 28.85s/it]                                                   {'loss': 0.0103, 'learning_rate': 0.0, 'epoch': 9.97, 'memory_allocated (GB)': 2.88, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
100%|██████████| 150/150 [1:17:43<00:00, 28.85s/it][INFO|trainer.py:1041] 2023-08-21 15:39:57,456 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   {'train_runtime': 4663.4068, 'train_samples_per_second': 1.075, 'train_steps_per_second': 0.033, 'train_loss': 0.30185574849446617, 'epoch': 9.97, 'memory_allocated (GB)': 2.88, 'max_memory_allocated (GB)': 49.33, 'total_memory_available (GB)': 93.74}
100%|██████████| 150/150 [1:17:43<00:00, 28.85s/it]100%|██████████| 150/150 [1:17:43<00:00, 31.09s/it]
Traceback (most recent call last):
  File "run_clm_starcoder.py", line 568, in <module>
    main()
  File "run_clm_starcoder.py", line 564, in main
    model.save_pretrained(training_args.output_dir, state_dict=unwrapped_model.state_dict())
NameError: name 'unwrapped_model' is not defined
[2023-08-21 15:40:06,043] [INFO] [launch.py:354:main] Process 3411092 exits successfully.
[2023-08-21 15:40:06,044] [INFO] [launch.py:354:main] Process 3411093 exits successfully.
[2023-08-21 15:40:07,046] [INFO] [launch.py:322:sigkill_handler] Killing subprocess 3411091
[2023-08-21 15:40:07,047] [INFO] [launch.py:322:sigkill_handler] Killing subprocess 3411092
[2023-08-21 15:40:07,047] [INFO] [launch.py:322:sigkill_handler] Killing subprocess 3411093
[2023-08-21 15:40:07,047] [INFO] [launch.py:322:sigkill_handler] Killing subprocess 3411094
[2023-08-21 15:40:07,055] [ERROR] [launch.py:328:sigkill_handler] ['/usr/bin/python3', '-u', 'run_clm_starcoder.py', '--model_name_or_path', 'bigcode/starcoder', '--train_file', '/starcoder/finetune-data/roblox-train.json', '--validation_file', '/starcoder/finetune-data/roblox-validation.json', '--report_to', 'tensorboard', '--bf16', 'True', '--output_dir', './checkpoints/starcoder-full-deepspeed-offload-4-hpu-4-2-adamw', '--num_train_epochs', '10', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'no', '--learning_rate', '1e-4', '--logging_steps', '1', '--dataset_concatenation', '--do_train', '--use_habana', '--use_lazy_mode', '--throughput_warmup_steps', '3', '--cache_dir', '/starcoder/cache/', '--use_hpu_graphs_for_inference', '--deepspeed', 'deepspeed_config_s3_offload.json'] exits with return code = 1
[ERROR|distributed_runner.py:218] 2023-08-21 15:40:07,839 >> deepspeed --num_nodes 1 --num_gpus 4 --no_local_rank run_clm_starcoder.py --model_name_or_path bigcode/starcoder --train_file /starcoder/finetune-data/roblox-train.json --validation_file /starcoder/finetune-data/roblox-validation.json --report_to tensorboard --bf16 True --output_dir ./checkpoints/starcoder-full-deepspeed-offload-4-hpu-4-2-adamw --num_train_epochs 10 --per_device_train_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy no --learning_rate 1e-4 --logging_steps 1 --dataset_concatenation --do_train --use_habana --use_lazy_mode --throughput_warmup_steps 3 --cache_dir /starcoder/cache/ --use_hpu_graphs_for_inference --deepspeed deepspeed_config_s3_offload.json  exited with status = 1
