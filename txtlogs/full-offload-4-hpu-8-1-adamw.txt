DistributedRunner run(): command = deepspeed --num_nodes 1 --num_gpus 4 --no_local_rank run_clm_starcoder.py --model_name_or_path bigcode/starcoder --train_file /starcoder/finetune-data/roblox-train.json --validation_file /starcoder/finetune-data/roblox-validation.json --report_to tensorboard --bf16 True --output_dir ./checkpoints/starcoder-full-deepspeed-offload-4-hpu-8-1-adamw --num_train_epochs 10 --per_device_train_batch_size 8 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy no --learning_rate 1e-4 --logging_steps 1 --dataset_concatenation --do_train --use_habana --use_lazy_mode --throughput_warmup_steps 3 --cache_dir /starcoder/cache/ --use_hpu_graphs_for_inference --deepspeed deepspeed_config_s3_offload.json
[2023-08-21 15:44:44,021] [WARNING] [runner.py:185:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Namespace(autotuning='', elastic_training=False, exclude='', force_multi=False, hostfile='/job/hostfile', include='', launcher='pdsh', launcher_args='', master_addr='', master_port=29500, max_elastic_nodes=-1, min_elastic_nodes=-1, module=False, no_local_rank=True, no_python=False, no_ssh_check=False, num_gpus=4, num_nodes=1, save_pid=False, use_hpu=True, user_args=['--model_name_or_path', 'bigcode/starcoder', '--train_file', '/starcoder/finetune-data/roblox-train.json', '--validation_file', '/starcoder/finetune-data/roblox-validation.json', '--report_to', 'tensorboard', '--bf16', 'True', '--output_dir', './checkpoints/starcoder-full-deepspeed-offload-4-hpu-8-1-adamw', '--num_train_epochs', '10', '--per_device_train_batch_size', '8', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'no', '--learning_rate', '1e-4', '--logging_steps', '1', '--dataset_concatenation', '--do_train', '--use_habana', '--use_lazy_mode', '--throughput_warmup_steps', '3', '--cache_dir', '/starcoder/cache/', '--use_hpu_graphs_for_inference', '--deepspeed', 'deepspeed_config_s3_offload.json'], user_script='run_clm_starcoder.py')
[2023-08-21 15:44:45,683] [INFO] [runner.py:543:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --no_local_rank run_clm_starcoder.py --model_name_or_path bigcode/starcoder --train_file /starcoder/finetune-data/roblox-train.json --validation_file /starcoder/finetune-data/roblox-validation.json --report_to tensorboard --bf16 True --output_dir ./checkpoints/starcoder-full-deepspeed-offload-4-hpu-8-1-adamw --num_train_epochs 10 --per_device_train_batch_size 8 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy no --learning_rate 1e-4 --logging_steps 1 --dataset_concatenation --do_train --use_habana --use_lazy_mode --throughput_warmup_steps 3 --cache_dir /starcoder/cache/ --use_hpu_graphs_for_inference --deepspeed deepspeed_config_s3_offload.json
[2023-08-21 15:44:47,510] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2023-08-21 15:44:47,510] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2023-08-21 15:44:47,510] [INFO] [launch.py:164:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2023-08-21 15:44:47,510] [INFO] [launch.py:165:main] dist_world_size=4
[2023-08-21 15:44:53,270] [INFO] [comm.py:762:init_distributed] Initializing TorchBackend in DeepSpeed with backend hccl
08/21/2023 15:44:54 - WARNING - __main__ -   Process rank: 0, device: hpu
distributed training: True, 16-bits training: True
08/21/2023 15:44:54 - INFO - __main__ -   Training/evaluation parameters GaudiTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-06,
adjust_throughput=False,
auto_find_batch_size=False,
bf16=True,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=230,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=deepspeed_config_s3_offload.json,
disable_tqdm=False,
distribution_strategy=ddp,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fsdp_config=None,
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gaudi_config_name=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=hpu_amp,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./checkpoints/starcoder-full-deepspeed-offload-4-hpu-8-1-adamw/runs/Aug21_15-44-53_sysid674631,
logging_first_step=False,
logging_nan_inf_filter=False,
logging_steps=1,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
no_cuda=False,
non_blocking_data_copy=False,
num_train_epochs=10.0,
optim=adamw_torch,
optim_args=None,
output_dir=./checkpoints/starcoder-full-deepspeed-offload-4-hpu-8-1-adamw,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
pipelining_fwd_bwd=False,
prediction_loss_only=False,
profiling_steps=0,
profiling_warmup_steps=0,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=./checkpoints/starcoder-full-deepspeed-offload-4-hpu-8-1-adamw,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=no,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
throughput_warmup_steps=3,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
use_habana=True,
use_hpu_graphs=False,
use_hpu_graphs_for_inference=True,
use_hpu_graphs_for_training=False,
use_ipex=False,
use_lazy_mode=True,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
08/21/2023 15:44:54 - WARNING - __main__ -   Process rank: 2, device: hpu
distributed training: True, 16-bits training: True
08/21/2023 15:44:54 - WARNING - __main__ -   Process rank: 1, device: hpu
distributed training: True, 16-bits training: True
08/21/2023 15:44:54 - WARNING - __main__ -   Process rank: 3, device: hpu
distributed training: True, 16-bits training: True
[INFO|configuration_utils.py:668] 2023-08-21 15:44:54,211 >> loading configuration file config.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/config.json
[INFO|configuration_utils.py:720] 2023-08-21 15:44:54,214 >> Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoder",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 6144,
  "n_head": 48,
  "n_inner": 24576,
  "n_layer": 40,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

[INFO|tokenization_utils_base.py:1809] 2023-08-21 15:44:54,357 >> loading file vocab.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/vocab.json
[INFO|tokenization_utils_base.py:1809] 2023-08-21 15:44:54,357 >> loading file merges.txt from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/merges.txt
[INFO|tokenization_utils_base.py:1809] 2023-08-21 15:44:54,357 >> loading file tokenizer.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/tokenizer.json
[INFO|tokenization_utils_base.py:1809] 2023-08-21 15:44:54,357 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2023-08-21 15:44:54,357 >> loading file special_tokens_map.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/special_tokens_map.json
[INFO|tokenization_utils_base.py:1809] 2023-08-21 15:44:54,357 >> loading file tokenizer_config.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/tokenizer_config.json
/usr/local/lib/python3.8/dist-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
[INFO|modeling_utils.py:2534] 2023-08-21 15:44:54,957 >> loading weights file pytorch_model.bin from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/pytorch_model.bin.index.json
[INFO|modeling_utils.py:1176] 2023-08-21 15:44:54,958 >> Instantiating GaudiGPTBigCodeForCausalLM model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2623] 2023-08-21 15:44:54,958 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:575] 2023-08-21 15:44:54,965 >> Generate config GaudiGenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "eos_token_id": 0,
  "ignore_eos": null,
  "static_shapes": null,
  "transformers_version": "4.28.1"
}

=============================HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_HPU_LAZY_EAGER_OPTIM_CACHE = 1
 PT_HPU_ENABLE_COMPILE_THREAD = 0
 PT_HPU_ENABLE_EXECUTION_THREAD = 1
 PT_HPU_ENABLE_LAZY_EAGER_EXECUTION_THREAD = 1
 PT_ENABLE_INTER_HOST_CACHING = 0
 PT_ENABLE_INFERENCE_MODE = 1
 PT_ENABLE_HABANA_CACHING = 1
 PT_HPU_MAX_RECIPE_SUBMISSION_LIMIT = 0
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_MAX_COMPOUND_OP_SIZE_SS = 10
 PT_HPU_ENABLE_STAGE_SUBMISSION = 1
 PT_HPU_STAGE_SUBMISSION_MODE = 2
 PT_HPU_PGM_ENABLE_CACHE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 0
 PT_HCCL_SLICE_SIZE_MB = 16
 PT_HCCL_MEMORY_ALLOWANCE_MB = 0
 PT_HPU_INITIAL_WORKSPACE_SIZE = 0
 PT_HABANA_POOL_SIZE = 24
 PT_HPU_POOL_STRATEGY = 5
 PT_HPU_POOL_LOG_FRAGMENTATION_INFO = 0
 PT_ENABLE_MEMORY_DEFRAGMENTATION = 1
 PT_ENABLE_DEFRAGMENTATION_INFO = 0
 PT_HPU_ENABLE_SYNAPSE_OUTPUT_PERMUTE = 1
 PT_HPU_ENABLE_VALID_DATA_RANGE_CHECK = 1
 PT_HPU_FORCE_USE_DEFAULT_STREAM = 0
 PT_RECIPE_CACHE_PATH = 
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_DYNAMIC_MIN_POLICY_ORDER = 4,5,3,1
 PT_HPU_DYNAMIC_MAX_POLICY_ORDER = 2,4,5,3,1
 PT_HPU_LAZY_ACC_PAR_MODE = 0
 PT_HPU_CLUSTERED_PROGRAM = 0
 PT_HPU_CLUSTERED_PROGRAM_ENFORCE = 0
 PT_HPU_CLUSTERED_PROGRAM_SPLIT_STR = default
 PT_HPU_CLUSTERED_PROGRAM_SCHED_STR = default
=============================SYSTEM CONFIGURATION ========================================= 
Num CPU Cores = 160
CPU RAM = 1056389528 KB 
============================================================================================ 
[2023-08-21 15:45:07,823] [INFO] [partition_parameters.py:481:__exit__] finished initializing model with 15.82B parameters
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
Loading checkpoint shards:  14%|█▍        | 1/7 [00:07<00:47,  7.84s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:07<00:47,  7.89s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:07<00:47,  7.85s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:07<00:47,  7.85s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:12<00:28,  5.71s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:12<00:28,  5.71s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:12<00:28,  5.72s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:12<00:28,  5.73s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:16<00:19,  4.95s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:16<00:19,  4.96s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:16<00:19,  4.96s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:16<00:19,  4.97s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:20<00:14,  4.73s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:20<00:14,  4.72s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:20<00:14,  4.72s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:20<00:14,  4.73s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:24<00:09,  4.53s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:24<00:09,  4.53s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:24<00:09,  4.53s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:24<00:09,  4.53s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:28<00:04,  4.42s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:28<00:04,  4.42s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:28<00:04,  4.43s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:28<00:04,  4.43s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:31<00:00,  3.67s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:31<00:00,  4.43s/it]
[INFO|modeling_utils.py:3190] 2023-08-21 15:45:38,962 >> All model checkpoint weights were used when initializing GaudiGPTBigCodeForCausalLM.

[INFO|modeling_utils.py:3198] 2023-08-21 15:45:38,962 >> All the weights of GaudiGPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoder.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GaudiGPTBigCodeForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 7/7 [00:31<00:00,  3.67s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:31<00:00,  4.43s/it]
Loading checkpoint shards: 100%|██████████| 7/7 [00:31<00:00,  3.67s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:31<00:00,  4.43s/it]
Loading checkpoint shards: 100%|██████████| 7/7 [00:31<00:00,  3.67s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:31<00:00,  4.44s/it]
[INFO|configuration_utils.py:537] 2023-08-21 15:45:39,520 >> loading configuration file generation_config.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/generation_config.json
[INFO|configuration_utils.py:575] 2023-08-21 15:45:39,521 >> Generate config GaudiGenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "eos_token_id": 0,
  "ignore_eos": null,
  "static_shapes": null,
  "transformers_version": "4.28.1"
}

08/21/2023 15:45:40 - INFO - __main__ -   Using data collator of type DataCollatorForSeq2Seq
Number of trainable paraemters: 0
[INFO|trainer.py:621] 2023-08-21 15:45:40,163 >> Using hpu_amp half precision backend
[2023-08-21 15:45:40,213] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.7.7+hpu.synapse.v1.10.0, git-hash=4bc77a6, git-branch=1.10.0
Number of trainable paraemters: 0
Number of trainable paraemters: 0
Number of trainable paraemters: 0
[2023-08-21 15:45:40,406] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py38_cpu/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.153162956237793 seconds
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py38_cpu/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Emitting ninja build file /root/.cache/torch_extensions/py38_cpu/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.8424694538116455 seconds
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.731805086135864 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.762553930282593 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 5.401409149169922 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2023-08-21 15:45:49,146] [INFO] [logging.py:68:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
[2023-08-21 15:45:49,218] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-08-21 15:45:49,219] [INFO] [utils.py:58:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-08-21 15:45:49,219] [INFO] [logging.py:68:log_dist] [Rank 0] Creating bf16 ZeRO stage 3 optimizer
[2023-08-21 15:45:49,219] [INFO] [logging.py:68:log_dist] [Rank 0] Creating bf16 ZeRO stage 3 optimizer
[2023-08-21 15:45:49,324] [INFO] [utils.py:930:see_memory_usage] Stage 3 initialize beginning
[2023-08-21 15:45:49,334] [WARNING] [utils.py:49:not_implemented] max_memory_reserved is not implemented on HPU
[2023-08-21 15:45:49,334] [INFO] [utils.py:931:see_memory_usage] MA 0.07 GB         Max_MA 1.75 GB         CA 0.07 GB         Max_CA 0 GB 
[2023-08-21 15:45:49,335] [INFO] [utils.py:939:see_memory_usage] CPU Virtual Memory:  used = 122.56 GB, percent = 12.2%
[2023-08-21 15:45:49,339] [INFO] [stage3.py:116:__init__] Reduce bucket size 500,000,000
[2023-08-21 15:45:49,339] [INFO] [stage3.py:117:__init__] Prefetch bucket size 50,000,000
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py38_cpu/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 1.1625335216522217 seconds
Loading extension module utils...
Time to load utils op: 1.0051980018615723 seconds
Loading extension module utils...
Time to load utils op: 1.2049689292907715 seconds
[2023-08-21 15:45:50,427] [INFO] [utils.py:930:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2023-08-21 15:45:50,438] [WARNING] [utils.py:49:not_implemented] max_memory_reserved is not implemented on HPU
[2023-08-21 15:45:50,438] [INFO] [utils.py:931:see_memory_usage] MA 0.07 GB         Max_MA 0.0 GB         CA 0.07 GB         Max_CA 0 GB 
[2023-08-21 15:45:50,438] [INFO] [utils.py:939:see_memory_usage] CPU Virtual Memory:  used = 122.54 GB, percent = 12.2%
Parameter Offload: Total persistent parameters: 2725888 in 322 params
[2023-08-21 15:45:50,545] [INFO] [utils.py:930:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2023-08-21 15:45:50,556] [WARNING] [utils.py:49:not_implemented] max_memory_reserved is not implemented on HPU
[2023-08-21 15:45:50,556] [INFO] [utils.py:931:see_memory_usage] MA 0.07 GB         Max_MA 0.0 GB         CA 0.07 GB         Max_CA 0 GB 
[2023-08-21 15:45:50,556] [INFO] [utils.py:939:see_memory_usage] CPU Virtual Memory:  used = 122.55 GB, percent = 12.2%
tcmalloc: large alloc 7758733312 bytes == 0x28b268000 @  0x7f5c4036e680 0x7f5c4038f824 0x7f5c4038fb8a 0x7f5b80ea4adc 0x7f5b80e7eef7 0x7f5b821a22f0 0x7f5b8219be38 0x7f5b8219beb8 0x7f5b8219bf38 0x7f5b829010c3 0x7f5b8356e290 0x7f5b8356e30f 0x7f5b831a1e7b 0x7f5b83529a93 0x7f5b831e6c65 0x7f5b8d22e33a 0x5f6939 0x5f7506 0x571019 0x5f6ce6 0x56b619 0x5697da 0x5f6ec3 0x56b619 0x5697da 0x5f6ec3 0x59c427 0x5f746f 0x571019 0x50b07e 0x570556
tcmalloc: large alloc 7758733312 bytes == 0x28ae82000 @  0x7ff0c4d93680 0x7ff0c4db4824 0x7ff0c4db4b8a 0x7ff0058ccadc 0x7ff0058a6ef7 0x7ff006bca2f0 0x7ff006bc3e38 0x7ff006bc3eb8 0x7ff006bc3f38 0x7ff0073290c3 0x7ff007f96290 0x7ff007f9630f 0x7ff007bc9e7b 0x7ff007f51a93 0x7ff007c0ec65 0x7ff011c6533a 0x5f6939 0x5f7506 0x571019 0x5f6ce6 0x56b619 0x5697da 0x5f6ec3 0x56b619 0x5697da 0x5f6ec3 0x59c427 0x5f746f 0x571019 0x50b07e 0x570556
tcmalloc: large alloc 7758733312 bytes == 0x28a8bc000 @  0x7f3b40b1a680 0x7f3b40b3b824 0x7f3b40b3bb8a 0x7f3b025bdadc 0x7f3b02597ef7 0x7f3a825762f0 0x7f3a8256fe38 0x7f3a8256feb8 0x7f3a8256ff38 0x7f3a82cd50c3 0x7f3a83942290 0x7f3a8394230f 0x7f3a83575e7b 0x7f3a838fda93 0x7f3a835bac65 0x7f3a8d60233a 0x5f6939 0x5f7506 0x571019 0x5f6ce6 0x56b619 0x5697da 0x5f6ec3 0x56b619 0x5697da 0x5f6ec3 0x59c427 0x5f746f 0x571019 0x50b07e 0x570556
tcmalloc: large alloc 7758733312 bytes == 0x28ad50000 @  0x7ffb05f06680 0x7ffb05f27824 0x7ffb05f27b8a 0x7ffaca98fadc 0x7ffaca969ef7 0x7ffa479622f0 0x7ffa4795be38 0x7ffa4795beb8 0x7ffa4795bf38 0x7ffa480c10c3 0x7ffa48d2e290 0x7ffa48d2e30f 0x7ffa48961e7b 0x7ffa48ce9a93 0x7ffa489a6c65 0x7ffa529ee33a 0x5f6939 0x5f7506 0x571019 0x5f6ce6 0x56b619 0x5697da 0x5f6ec3 0x56b619 0x5697da 0x5f6ec3 0x59c427 0x5f746f 0x571019 0x50b07e 0x570556
[2023-08-21 15:46:05,589] [INFO] [stage3.py:378:_setup_for_real_optimizer] optimizer state initialized
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...Time to load utils op: 0.0006470680236816406 seconds

Loading extension module utils...
Time to load utils op: 0.0006794929504394531 seconds
Time to load utils op: 0.0006999969482421875 seconds
You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
[2023-08-21 15:46:09,352] [INFO] [utils.py:930:see_memory_usage] After initializing ZeRO optimizer
[2023-08-21 15:46:09,359] [WARNING] [utils.py:49:not_implemented] max_memory_reserved is not implemented on HPU
[2023-08-21 15:46:09,359] [INFO] [utils.py:931:see_memory_usage] MA 0.07 GB         Max_MA 1.78 GB         CA 0.07 GB         Max_CA 0 GB 
[2023-08-21 15:46:09,359] [INFO] [utils.py:939:see_memory_usage] CPU Virtual Memory:  used = 317.11 GB, percent = 31.5%
[2023-08-21 15:46:09,361] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2023-08-21 15:46:09,361] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed using client callable to create LR scheduler
[2023-08-21 15:46:09,361] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f59b3435f70>
[2023-08-21 15:46:09,361] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2023-08-21 15:46:09,362] [INFO] [config.py:1028:print] DeepSpeedEngine configuration:
[2023-08-21 15:46:09,362] [INFO] [config.py:1032:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-21 15:46:09,362] [INFO] [config.py:1032:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-21 15:46:09,362] [INFO] [config.py:1032:print]   amp_enabled .................. False
[2023-08-21 15:46:09,362] [INFO] [config.py:1032:print]   amp_params ................... False
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   bfloat16_enabled ............. True
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   checkpoint_parallel_write_pipeline  False
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   checkpoint_tag_validation_enabled  True
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   checkpoint_tag_validation_fail  False
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f59b49ec2e0>
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   communication_data_type ...... None
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   curriculum_enabled ........... False
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   curriculum_params ............ False
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   dataloader_drop_last ......... False
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   disable_allgather ............ False
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   dump_state ................... False
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   dynamic_loss_scale_args ...... None
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   eigenvalue_enabled ........... False
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   eigenvalue_layer_num ......... 0
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   eigenvalue_max_iter .......... 100
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   eigenvalue_stability ......... 1e-06
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   eigenvalue_tol ............... 0.01
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   eigenvalue_verbose ........... False
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   elasticity_enabled ........... False
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   fp16_auto_cast ............... None
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   fp16_enabled ................. False
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   fp16_master_weights_and_gradients  False
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   global_rank .................. 0
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   grad_accum_dtype ............. None
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   gradient_accumulation_steps .. 1
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   gradient_clipping ............ 1
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   gradient_predivide_factor .... 1.0
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   initial_dynamic_scale ........ 1
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   load_universal_checkpoint .... False
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   loss_scale ................... 1.0
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   memory_breakdown ............. False
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7f59b49ec490>
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   optimizer_legacy_fusion ...... False
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   optimizer_name ............... adamw
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   optimizer_params ............. {'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-06, 'weight_decay': 0.0}
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-21 15:46:09,363] [INFO] [config.py:1032:print]   pld_enabled .................. False
[2023-08-21 15:46:09,364] [INFO] [config.py:1032:print]   pld_params ................... False
[2023-08-21 15:46:09,364] [INFO] [config.py:1032:print]   prescale_gradients ........... False
[2023-08-21 15:46:09,364] [INFO] [config.py:1032:print]   scheduler_name ............... None
[2023-08-21 15:46:09,364] [INFO] [config.py:1032:print]   scheduler_params ............. None
[2023-08-21 15:46:09,364] [INFO] [config.py:1032:print]   sparse_attention ............. None
[2023-08-21 15:46:09,364] [INFO] [config.py:1032:print]   sparse_gradients_enabled ..... False
[2023-08-21 15:46:09,364] [INFO] [config.py:1032:print]   steps_per_print .............. 64
[2023-08-21 15:46:09,364] [INFO] [config.py:1032:print]   train_batch_size ............. 32
[2023-08-21 15:46:09,364] [INFO] [config.py:1032:print]   train_micro_batch_size_per_gpu  8
[2023-08-21 15:46:09,364] [INFO] [config.py:1032:print]   use_node_local_storage ....... False
[2023-08-21 15:46:09,364] [INFO] [config.py:1032:print]   wall_clock_breakdown ......... False
[2023-08-21 15:46:09,364] [INFO] [config.py:1032:print]   world_size ................... 4
[2023-08-21 15:46:09,364] [INFO] [config.py:1032:print]   zero_allow_comm_data_type_fp32  False
[2023-08-21 15:46:09,364] [INFO] [config.py:1032:print]   zero_allow_untested_optimizer  False
[2023-08-21 15:46:09,364] [INFO] [config.py:1032:print]   zero_config .................. stage=3 contiguous_gradients=False reduce_scatter=False reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False max_group_size=4000000000 load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2023-08-21 15:46:09,364] [INFO] [config.py:1032:print]   zero_enabled ................. True
[2023-08-21 15:46:09,364] [INFO] [config.py:1032:print]   zero_optimization_stage ...... 3
[2023-08-21 15:46:09,364] [INFO] [config.py:1017:print_user_config]   json = {
    "steps_per_print": 64, 
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 8, 
    "gradient_accumulation_steps": 1, 
    "bf16": {
        "enabled": true
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0001, 
            "betas": [0.9, 0.999], 
            "eps": 1e-06, 
            "weight_decay": 0.0
        }
    }, 
    "gradient_clipping": 1, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "cpu"
        }, 
        "offload_param": {
            "device": "cpu"
        }, 
        "overlap_comm": false, 
        "reduce_scatter": false, 
        "contiguous_gradients": false
    }
}
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0003223419189453125 seconds
[INFO|trainer.py:770] 2023-08-21 15:46:09,364 >> ***** Running training *****
[INFO|trainer.py:771] 2023-08-21 15:46:09,365 >>   Num examples = 489
[INFO|trainer.py:772] 2023-08-21 15:46:09,365 >>   Num Epochs = 10
[INFO|trainer.py:773] 2023-08-21 15:46:09,365 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:774] 2023-08-21 15:46:09,365 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:775] 2023-08-21 15:46:09,365 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:776] 2023-08-21 15:46:09,365 >>   Total optimization steps = 160
[INFO|trainer.py:777] 2023-08-21 15:46:09,368 >>   Number of trainable parameters = 15,517,456,384
  0%|          | 0/160 [00:00<?, ?it/s][WARNING|logging.py:280] 2023-08-21 15:46:09,407 >> You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:3030: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:3030: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:3030: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:3030: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
  1%|          | 1/160 [01:10<3:05:53, 70.15s/it]                                                 {'loss': 1.4453, 'learning_rate': 9.9375e-05, 'epoch': 0.06, 'memory_allocated (GB)': 2.0, 'max_memory_allocated (GB)': 78.29, 'total_memory_available (GB)': 93.74}
  1%|          | 1/160 [01:10<3:05:53, 70.15s/it]  1%|▏         | 2/160 [02:10<2:49:49, 64.49s/it]                                                 {'loss': 1.3848, 'learning_rate': 9.875000000000002e-05, 'epoch': 0.12, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.29, 'total_memory_available (GB)': 93.74}
  1%|▏         | 2/160 [02:10<2:49:49, 64.49s/it]  2%|▏         | 3/160 [02:34<2:00:14, 45.95s/it]                                                 {'loss': 1.2441, 'learning_rate': 9.8125e-05, 'epoch': 0.19, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.36, 'total_memory_available (GB)': 93.74}
  2%|▏         | 3/160 [02:34<2:00:14, 45.95s/it]  2%|▎         | 4/160 [02:53<1:31:41, 35.27s/it]                                                 {'loss': 1.0957, 'learning_rate': 9.75e-05, 'epoch': 0.25, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.36, 'total_memory_available (GB)': 93.74}
  2%|▎         | 4/160 [02:53<1:31:41, 35.27s/it]  3%|▎         | 5/160 [03:12<1:15:37, 29.27s/it]                                                 {'loss': 1.0205, 'learning_rate': 9.687500000000001e-05, 'epoch': 0.31, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.36, 'total_memory_available (GB)': 93.74}
  3%|▎         | 5/160 [03:12<1:15:37, 29.27s/it]  4%|▍         | 6/160 [03:30<1:06:01, 25.73s/it]                                                 {'loss': 1.1133, 'learning_rate': 9.625000000000001e-05, 'epoch': 0.38, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.36, 'total_memory_available (GB)': 93.74}
  4%|▍         | 6/160 [03:30<1:06:01, 25.73s/it]  4%|▍         | 7/160 [03:49<59:25, 23.31s/it]                                                 {'loss': 1.0488, 'learning_rate': 9.562500000000001e-05, 'epoch': 0.44, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.36, 'total_memory_available (GB)': 93.74}
  4%|▍         | 7/160 [03:49<59:25, 23.31s/it]  5%|▌         | 8/160 [04:07<55:21, 21.85s/it]                                               {'loss': 0.9883, 'learning_rate': 9.5e-05, 'epoch': 0.5, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.72, 'total_memory_available (GB)': 93.74}
  5%|▌         | 8/160 [04:08<55:21, 21.85s/it]  6%|▌         | 9/160 [04:26<52:22, 20.81s/it]                                               {'loss': 0.9355, 'learning_rate': 9.4375e-05, 'epoch': 0.56, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.72, 'total_memory_available (GB)': 93.74}
  6%|▌         | 9/160 [04:26<52:22, 20.81s/it]  6%|▋         | 10/160 [04:45<50:16, 20.11s/it]                                                {'loss': 1.0244, 'learning_rate': 9.375e-05, 'epoch': 0.62, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.72, 'total_memory_available (GB)': 93.74}
  6%|▋         | 10/160 [04:45<50:16, 20.11s/it]  7%|▋         | 11/160 [05:03<48:37, 19.58s/it]                                                {'loss': 1.0312, 'learning_rate': 9.3125e-05, 'epoch': 0.69, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.72, 'total_memory_available (GB)': 93.74}
  7%|▋         | 11/160 [05:03<48:37, 19.58s/it]  8%|▊         | 12/160 [05:21<47:20, 19.19s/it]                                                {'loss': 0.958, 'learning_rate': 9.250000000000001e-05, 'epoch': 0.75, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.72, 'total_memory_available (GB)': 93.74}
  8%|▊         | 12/160 [05:21<47:20, 19.19s/it]  8%|▊         | 13/160 [05:40<46:44, 19.08s/it]                                                {'loss': 0.9893, 'learning_rate': 9.1875e-05, 'epoch': 0.81, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.72, 'total_memory_available (GB)': 93.74}
  8%|▊         | 13/160 [05:40<46:44, 19.08s/it]  9%|▉         | 14/160 [05:58<45:49, 18.83s/it]                                                {'loss': 0.9688, 'learning_rate': 9.125e-05, 'epoch': 0.88, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.72, 'total_memory_available (GB)': 93.74}
  9%|▉         | 14/160 [05:58<45:49, 18.83s/it]  9%|▉         | 15/160 [06:17<45:12, 18.71s/it]                                                {'loss': 1.125, 'learning_rate': 9.062500000000001e-05, 'epoch': 0.94, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.72, 'total_memory_available (GB)': 93.74}
  9%|▉         | 15/160 [06:17<45:12, 18.71s/it] 10%|█         | 16/160 [06:35<44:52, 18.70s/it]                                                {'loss': 0.8301, 'learning_rate': 9e-05, 'epoch': 1.0, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.72, 'total_memory_available (GB)': 93.74}
 10%|█         | 16/160 [06:35<44:52, 18.70s/it] 11%|█         | 17/160 [06:54<44:23, 18.62s/it]                                                {'loss': 0.6572, 'learning_rate': 8.9375e-05, 'epoch': 1.06, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.72, 'total_memory_available (GB)': 93.74}
 11%|█         | 17/160 [06:54<44:23, 18.62s/it] 11%|█▏        | 18/160 [07:12<43:49, 18.52s/it]                                                {'loss': 0.6484, 'learning_rate': 8.875e-05, 'epoch': 1.12, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.72, 'total_memory_available (GB)': 93.74}
 11%|█▏        | 18/160 [07:12<43:49, 18.52s/it] 12%|█▏        | 19/160 [07:30<43:21, 18.45s/it]                                                {'loss': 0.6055, 'learning_rate': 8.8125e-05, 'epoch': 1.19, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.72, 'total_memory_available (GB)': 93.74}
 12%|█▏        | 19/160 [07:30<43:21, 18.45s/it] 12%|█▎        | 20/160 [07:51<44:29, 19.07s/it]                                                {'loss': 0.5352, 'learning_rate': 8.75e-05, 'epoch': 1.25, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.72, 'total_memory_available (GB)': 93.74}
 12%|█▎        | 20/160 [07:51<44:29, 19.07s/it] 13%|█▎        | 21/160 [08:09<43:07, 18.61s/it]                                                {'loss': 0.5547, 'learning_rate': 8.687500000000001e-05, 'epoch': 1.31, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.72, 'total_memory_available (GB)': 93.74}
 13%|█▎        | 21/160 [08:09<43:07, 18.61s/it] 14%|█▍        | 22/160 [08:27<42:28, 18.47s/it]                                                {'loss': 0.5601, 'learning_rate': 8.625000000000001e-05, 'epoch': 1.38, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.72, 'total_memory_available (GB)': 93.74}
 14%|█▍        | 22/160 [08:27<42:28, 18.47s/it] 14%|█▍        | 23/160 [08:44<41:40, 18.25s/it]                                                {'loss': 0.5264, 'learning_rate': 8.5625e-05, 'epoch': 1.44, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.72, 'total_memory_available (GB)': 93.74}
 14%|█▍        | 23/160 [08:44<41:40, 18.25s/it] 15%|█▌        | 24/160 [09:02<41:07, 18.14s/it]                                                {'loss': 0.5171, 'learning_rate': 8.5e-05, 'epoch': 1.5, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.72, 'total_memory_available (GB)': 93.74}
 15%|█▌        | 24/160 [09:02<41:07, 18.14s/it] 16%|█▌        | 25/160 [09:20<40:29, 18.00s/it]                                                {'loss': 0.4805, 'learning_rate': 8.4375e-05, 'epoch': 1.56, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.72, 'total_memory_available (GB)': 93.74}
 16%|█▌        | 25/160 [09:20<40:29, 18.00s/it] 16%|█▋        | 26/160 [09:38<40:19, 18.05s/it]                                                {'loss': 0.5454, 'learning_rate': 8.375e-05, 'epoch': 1.62, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.72, 'total_memory_available (GB)': 93.74}
 16%|█▋        | 26/160 [09:38<40:19, 18.05s/it] 17%|█▋        | 27/160 [09:56<39:57, 18.02s/it]                                                {'loss': 0.4883, 'learning_rate': 8.312500000000001e-05, 'epoch': 1.69, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.72, 'total_memory_available (GB)': 93.74}
 17%|█▋        | 27/160 [09:56<39:57, 18.02s/it] 18%|█▊        | 28/160 [10:14<39:33, 17.98s/it]                                                {'loss': 0.4678, 'learning_rate': 8.25e-05, 'epoch': 1.75, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.72, 'total_memory_available (GB)': 93.74}
 18%|█▊        | 28/160 [10:14<39:33, 17.98s/it] 18%|█▊        | 29/160 [10:32<39:00, 17.86s/it]                                                {'loss': 0.5542, 'learning_rate': 8.1875e-05, 'epoch': 1.81, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 78.72, 'total_memory_available (GB)': 93.74}
 18%|█▊        | 29/160 [10:32<39:00, 17.86s/it] 19%|█▉        | 30/160 [10:49<38:27, 17.75s/it]                                                {'loss': 0.5781, 'learning_rate': 8.125000000000001e-05, 'epoch': 1.88, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.15, 'total_memory_available (GB)': 93.74}
 19%|█▉        | 30/160 [10:49<38:27, 17.75s/it] 19%|█▉        | 31/160 [11:07<38:14, 17.78s/it]                                                {'loss': 0.5688, 'learning_rate': 8.062500000000001e-05, 'epoch': 1.94, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.15, 'total_memory_available (GB)': 93.74}
 19%|█▉        | 31/160 [11:07<38:14, 17.78s/it] 20%|██        | 32/160 [11:26<38:33, 18.07s/it]                                                {'loss': 0.4268, 'learning_rate': 8e-05, 'epoch': 2.0, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.15, 'total_memory_available (GB)': 93.74}
 20%|██        | 32/160 [11:26<38:33, 18.07s/it] 21%|██        | 33/160 [11:44<38:08, 18.02s/it]                                                {'loss': 0.3013, 'learning_rate': 7.9375e-05, 'epoch': 2.06, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.15, 'total_memory_available (GB)': 93.74}
 21%|██        | 33/160 [11:44<38:08, 18.02s/it] 21%|██▏       | 34/160 [12:01<37:47, 17.99s/it]                                                {'loss': 0.2322, 'learning_rate': 7.875e-05, 'epoch': 2.12, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.15, 'total_memory_available (GB)': 93.74}
 21%|██▏       | 34/160 [12:01<37:47, 17.99s/it] 22%|██▏       | 35/160 [12:19<37:28, 17.99s/it]                                                {'loss': 0.2512, 'learning_rate': 7.8125e-05, 'epoch': 2.19, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.15, 'total_memory_available (GB)': 93.74}
 22%|██▏       | 35/160 [12:19<37:28, 17.99s/it] 22%|██▎       | 36/160 [12:37<37:02, 17.92s/it]                                                {'loss': 0.2219, 'learning_rate': 7.75e-05, 'epoch': 2.25, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.15, 'total_memory_available (GB)': 93.74}
 22%|██▎       | 36/160 [12:37<37:02, 17.92s/it] 23%|██▎       | 37/160 [12:56<37:08, 18.12s/it]                                                {'loss': 0.2302, 'learning_rate': 7.687500000000001e-05, 'epoch': 2.31, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.15, 'total_memory_available (GB)': 93.74}
 23%|██▎       | 37/160 [12:56<37:08, 18.12s/it] 24%|██▍       | 38/160 [13:14<36:47, 18.10s/it]                                                {'loss': 0.2559, 'learning_rate': 7.625e-05, 'epoch': 2.38, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.15, 'total_memory_available (GB)': 93.74}
 24%|██▍       | 38/160 [13:14<36:47, 18.10s/it] 24%|██▍       | 39/160 [13:32<36:42, 18.20s/it]                                                {'loss': 0.2427, 'learning_rate': 7.5625e-05, 'epoch': 2.44, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.15, 'total_memory_available (GB)': 93.74}
 24%|██▍       | 39/160 [13:32<36:42, 18.20s/it] 25%|██▌       | 40/160 [13:50<36:03, 18.03s/it]                                                {'loss': 0.2178, 'learning_rate': 7.500000000000001e-05, 'epoch': 2.5, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.15, 'total_memory_available (GB)': 93.74}
 25%|██▌       | 40/160 [13:50<36:03, 18.03s/it] 26%|██▌       | 41/160 [14:08<35:38, 17.97s/it]                                                {'loss': 0.2371, 'learning_rate': 7.4375e-05, 'epoch': 2.56, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.15, 'total_memory_available (GB)': 93.74}
 26%|██▌       | 41/160 [14:08<35:38, 17.97s/it] 26%|██▋       | 42/160 [14:25<35:08, 17.87s/it]                                                {'loss': 0.23, 'learning_rate': 7.375e-05, 'epoch': 2.62, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.15, 'total_memory_available (GB)': 93.74}
 26%|██▋       | 42/160 [14:25<35:08, 17.87s/it] 27%|██▋       | 43/160 [14:43<34:56, 17.92s/it]                                                {'loss': 0.2471, 'learning_rate': 7.3125e-05, 'epoch': 2.69, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.15, 'total_memory_available (GB)': 93.74}
 27%|██▋       | 43/160 [14:43<34:56, 17.92s/it] 28%|██▊       | 44/160 [15:02<34:58, 18.09s/it]                                                {'loss': 0.2488, 'learning_rate': 7.25e-05, 'epoch': 2.75, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.15, 'total_memory_available (GB)': 93.74}
 28%|██▊       | 44/160 [15:02<34:58, 18.09s/it] 28%|██▊       | 45/160 [15:20<34:35, 18.05s/it]                                                {'loss': 0.2695, 'learning_rate': 7.1875e-05, 'epoch': 2.81, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.15, 'total_memory_available (GB)': 93.74}
 28%|██▊       | 45/160 [15:20<34:35, 18.05s/it] 29%|██▉       | 46/160 [15:38<34:23, 18.10s/it]                                                {'loss': 0.2166, 'learning_rate': 7.125000000000001e-05, 'epoch': 2.88, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.15, 'total_memory_available (GB)': 93.74}
 29%|██▉       | 46/160 [15:38<34:23, 18.10s/it] 29%|██▉       | 47/160 [15:56<34:01, 18.07s/it]                                                {'loss': 0.2444, 'learning_rate': 7.062500000000001e-05, 'epoch': 2.94, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.15, 'total_memory_available (GB)': 93.74}
 29%|██▉       | 47/160 [15:56<34:01, 18.07s/it] 30%|███       | 48/160 [16:14<33:32, 17.97s/it]                                                {'loss': 0.189, 'learning_rate': 7e-05, 'epoch': 3.0, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 30%|███       | 48/160 [16:14<33:32, 17.97s/it] 31%|███       | 49/160 [16:32<33:33, 18.14s/it]                                                {'loss': 0.1346, 'learning_rate': 6.9375e-05, 'epoch': 3.06, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 31%|███       | 49/160 [16:32<33:33, 18.14s/it] 31%|███▏      | 50/160 [16:50<33:04, 18.04s/it]                                                {'loss': 0.1335, 'learning_rate': 6.875e-05, 'epoch': 3.12, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 31%|███▏      | 50/160 [16:50<33:04, 18.04s/it] 32%|███▏      | 51/160 [17:08<32:42, 18.00s/it]                                                {'loss': 0.1338, 'learning_rate': 6.8125e-05, 'epoch': 3.19, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 32%|███▏      | 51/160 [17:08<32:42, 18.00s/it] 32%|███▎      | 52/160 [17:26<32:14, 17.91s/it]                                                {'loss': 0.1289, 'learning_rate': 6.750000000000001e-05, 'epoch': 3.25, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 32%|███▎      | 52/160 [17:26<32:14, 17.91s/it] 33%|███▎      | 53/160 [17:44<31:58, 17.93s/it]                                                {'loss': 0.1294, 'learning_rate': 6.6875e-05, 'epoch': 3.31, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 33%|███▎      | 53/160 [17:44<31:58, 17.93s/it] 34%|███▍      | 54/160 [18:02<31:36, 17.89s/it]                                                {'loss': 0.1331, 'learning_rate': 6.625e-05, 'epoch': 3.38, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 34%|███▍      | 54/160 [18:02<31:36, 17.89s/it] 34%|███▍      | 55/160 [18:19<31:11, 17.82s/it]                                                {'loss': 0.127, 'learning_rate': 6.562500000000001e-05, 'epoch': 3.44, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 34%|███▍      | 55/160 [18:19<31:11, 17.82s/it] 35%|███▌      | 56/160 [18:37<30:45, 17.75s/it]                                                {'loss': 0.1285, 'learning_rate': 6.500000000000001e-05, 'epoch': 3.5, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 35%|███▌      | 56/160 [18:37<30:45, 17.75s/it] 36%|███▌      | 57/160 [18:56<31:12, 18.18s/it]                                                {'loss': 0.1322, 'learning_rate': 6.4375e-05, 'epoch': 3.56, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 36%|███▌      | 57/160 [18:56<31:12, 18.18s/it] 36%|███▋      | 58/160 [19:14<30:49, 18.13s/it]                                                {'loss': 0.1353, 'learning_rate': 6.375e-05, 'epoch': 3.62, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 36%|███▋      | 58/160 [19:14<30:49, 18.13s/it] 37%|███▋      | 59/160 [19:32<30:18, 18.01s/it]                                                {'loss': 0.1169, 'learning_rate': 6.3125e-05, 'epoch': 3.69, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 37%|███▋      | 59/160 [19:32<30:18, 18.01s/it] 38%|███▊      | 60/160 [19:50<29:55, 17.96s/it]                                                {'loss': 0.1475, 'learning_rate': 6.25e-05, 'epoch': 3.75, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 38%|███▊      | 60/160 [19:50<29:55, 17.96s/it] 38%|███▊      | 61/160 [20:07<29:28, 17.86s/it]                                                {'loss': 0.141, 'learning_rate': 6.1875e-05, 'epoch': 3.81, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 38%|███▊      | 61/160 [20:07<29:28, 17.86s/it] 39%|███▉      | 62/160 [20:25<29:11, 17.87s/it]                                                {'loss': 0.1365, 'learning_rate': 6.125000000000001e-05, 'epoch': 3.88, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 39%|███▉      | 62/160 [20:25<29:11, 17.87s/it] 39%|███▉      | 63/160 [20:43<28:46, 17.80s/it]                                                {'loss': 0.1306, 'learning_rate': 6.0624999999999996e-05, 'epoch': 3.94, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 39%|███▉      | 63/160 [20:43<28:46, 17.80s/it][2023-08-21 16:07:10,315] [INFO] [logging.py:68:log_dist] [Rank 0] step=64, skipped=0, lr=[6e-05], mom=[[0.9, 0.999]]
[2023-08-21 16:07:10,322] [INFO] [timer.py:241:stop] 0/64, RunningAvgSamplesPerSec=1.759465362902366, CurrSamplesPerSec=1.8102888487353976, MemAllocated=2.3GB, MaxMemAllocated=79.34GB
 40%|████      | 64/160 [21:00<28:27, 17.79s/it]                                                {'loss': 0.0964, 'learning_rate': 6e-05, 'epoch': 4.0, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 40%|████      | 64/160 [21:00<28:27, 17.79s/it] 41%|████      | 65/160 [21:18<28:08, 17.77s/it]                                                {'loss': 0.083, 'learning_rate': 5.9375e-05, 'epoch': 4.06, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 41%|████      | 65/160 [21:18<28:08, 17.77s/it] 41%|████▏     | 66/160 [21:36<27:59, 17.87s/it]                                                {'loss': 0.0806, 'learning_rate': 5.8750000000000005e-05, 'epoch': 4.12, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 41%|████▏     | 66/160 [21:36<27:59, 17.87s/it] 42%|████▏     | 67/160 [21:55<27:54, 18.00s/it]                                                {'loss': 0.0704, 'learning_rate': 5.812500000000001e-05, 'epoch': 4.19, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 42%|████▏     | 67/160 [21:55<27:54, 18.00s/it] 42%|████▎     | 68/160 [22:13<27:39, 18.03s/it]                                                {'loss': 0.089, 'learning_rate': 5.7499999999999995e-05, 'epoch': 4.25, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 42%|████▎     | 68/160 [22:13<27:39, 18.03s/it] 43%|████▎     | 69/160 [22:31<27:19, 18.02s/it]                                                {'loss': 0.0746, 'learning_rate': 5.6875e-05, 'epoch': 4.31, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 43%|████▎     | 69/160 [22:31<27:19, 18.02s/it] 44%|████▍     | 70/160 [22:48<26:46, 17.85s/it]                                                {'loss': 0.0818, 'learning_rate': 5.6250000000000005e-05, 'epoch': 4.38, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 44%|████▍     | 70/160 [22:48<26:46, 17.85s/it] 44%|████▍     | 71/160 [23:06<26:33, 17.90s/it]                                                {'loss': 0.0946, 'learning_rate': 5.5625000000000004e-05, 'epoch': 4.44, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 44%|████▍     | 71/160 [23:06<26:33, 17.90s/it] 45%|████▌     | 72/160 [23:24<26:21, 17.97s/it]                                                {'loss': 0.0881, 'learning_rate': 5.500000000000001e-05, 'epoch': 4.5, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 45%|████▌     | 72/160 [23:24<26:21, 17.97s/it] 46%|████▌     | 73/160 [23:42<25:58, 17.91s/it]                                                {'loss': 0.0765, 'learning_rate': 5.4375e-05, 'epoch': 4.56, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 46%|████▌     | 73/160 [23:42<25:58, 17.91s/it] 46%|████▋     | 74/160 [24:00<25:47, 18.00s/it]                                                {'loss': 0.0793, 'learning_rate': 5.375e-05, 'epoch': 4.62, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 46%|████▋     | 74/160 [24:00<25:47, 18.00s/it] 47%|████▋     | 75/160 [24:18<25:19, 17.88s/it]                                                {'loss': 0.0775, 'learning_rate': 5.3125000000000004e-05, 'epoch': 4.69, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 47%|████▋     | 75/160 [24:18<25:19, 17.88s/it] 48%|████▊     | 76/160 [24:36<25:05, 17.92s/it]                                                {'loss': 0.0868, 'learning_rate': 5.25e-05, 'epoch': 4.75, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 48%|████▊     | 76/160 [24:36<25:05, 17.92s/it] 48%|████▊     | 77/160 [24:54<24:45, 17.90s/it]                                                {'loss': 0.0764, 'learning_rate': 5.187500000000001e-05, 'epoch': 4.81, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 48%|████▊     | 77/160 [24:54<24:45, 17.90s/it] 49%|████▉     | 78/160 [25:11<24:19, 17.80s/it]                                                {'loss': 0.0763, 'learning_rate': 5.125e-05, 'epoch': 4.88, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 49%|████▉     | 78/160 [25:11<24:19, 17.80s/it] 49%|████▉     | 79/160 [25:29<24:09, 17.90s/it]                                                {'loss': 0.0797, 'learning_rate': 5.0625e-05, 'epoch': 4.94, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 49%|████▉     | 79/160 [25:29<24:09, 17.90s/it] 50%|█████     | 80/160 [25:47<23:46, 17.84s/it]                                                {'loss': 0.058, 'learning_rate': 5e-05, 'epoch': 5.0, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 50%|█████     | 80/160 [25:47<23:46, 17.84s/it] 51%|█████     | 81/160 [26:05<23:31, 17.86s/it]                                                {'loss': 0.0526, 'learning_rate': 4.937500000000001e-05, 'epoch': 5.06, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 51%|█████     | 81/160 [26:05<23:31, 17.86s/it] 51%|█████▏    | 82/160 [26:23<23:04, 17.76s/it]                                                {'loss': 0.0762, 'learning_rate': 4.875e-05, 'epoch': 5.12, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.34, 'total_memory_available (GB)': 93.74}
 51%|█████▏    | 82/160 [26:23<23:04, 17.76s/it] 52%|█████▏    | 83/160 [26:40<22:50, 17.80s/it]                                                {'loss': 0.0745, 'learning_rate': 4.8125000000000004e-05, 'epoch': 5.19, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 52%|█████▏    | 83/160 [26:40<22:50, 17.80s/it] 52%|█████▎    | 84/160 [26:58<22:32, 17.80s/it]                                                {'loss': 0.0615, 'learning_rate': 4.75e-05, 'epoch': 5.25, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 52%|█████▎    | 84/160 [26:58<22:32, 17.80s/it] 53%|█████▎    | 85/160 [27:16<22:05, 17.68s/it]                                                {'loss': 0.0645, 'learning_rate': 4.6875e-05, 'epoch': 5.31, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 53%|█████▎    | 85/160 [27:16<22:05, 17.68s/it] 54%|█████▍    | 86/160 [27:34<21:53, 17.74s/it]                                                {'loss': 0.0345, 'learning_rate': 4.6250000000000006e-05, 'epoch': 5.38, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 54%|█████▍    | 86/160 [27:34<21:53, 17.74s/it] 54%|█████▍    | 87/160 [27:52<21:47, 17.92s/it]                                                {'loss': 0.0483, 'learning_rate': 4.5625e-05, 'epoch': 5.44, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 54%|█████▍    | 87/160 [27:52<21:47, 17.92s/it] 55%|█████▌    | 88/160 [28:10<21:27, 17.89s/it]                                                {'loss': 0.0388, 'learning_rate': 4.5e-05, 'epoch': 5.5, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 55%|█████▌    | 88/160 [28:10<21:27, 17.89s/it] 56%|█████▌    | 89/160 [28:28<21:24, 18.09s/it]                                                {'loss': 0.0352, 'learning_rate': 4.4375e-05, 'epoch': 5.56, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 56%|█████▌    | 89/160 [28:28<21:24, 18.09s/it] 56%|█████▋    | 90/160 [28:46<20:55, 17.93s/it]                                                {'loss': 0.0501, 'learning_rate': 4.375e-05, 'epoch': 5.62, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 56%|█████▋    | 90/160 [28:46<20:55, 17.93s/it] 57%|█████▋    | 91/160 [29:03<20:25, 17.76s/it]                                                {'loss': 0.0366, 'learning_rate': 4.3125000000000005e-05, 'epoch': 5.69, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 57%|█████▋    | 91/160 [29:03<20:25, 17.76s/it] 57%|█████▊    | 92/160 [29:21<20:06, 17.75s/it]                                                {'loss': 0.0561, 'learning_rate': 4.25e-05, 'epoch': 5.75, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 57%|█████▊    | 92/160 [29:21<20:06, 17.75s/it] 58%|█████▊    | 93/160 [29:39<19:57, 17.87s/it]                                                {'loss': 0.0332, 'learning_rate': 4.1875e-05, 'epoch': 5.81, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 58%|█████▊    | 93/160 [29:39<19:57, 17.87s/it] 59%|█████▉    | 94/160 [29:57<19:41, 17.90s/it]                                                {'loss': 0.0365, 'learning_rate': 4.125e-05, 'epoch': 5.88, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 59%|█████▉    | 94/160 [29:57<19:41, 17.90s/it] 59%|█████▉    | 95/160 [30:15<19:20, 17.85s/it]                                                {'loss': 0.032, 'learning_rate': 4.0625000000000005e-05, 'epoch': 5.94, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 59%|█████▉    | 95/160 [30:15<19:20, 17.85s/it] 60%|██████    | 96/160 [30:33<19:02, 17.85s/it]                                                {'loss': 0.0246, 'learning_rate': 4e-05, 'epoch': 6.0, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 60%|██████    | 96/160 [30:33<19:02, 17.85s/it] 61%|██████    | 97/160 [30:50<18:36, 17.72s/it]                                                {'loss': 0.0229, 'learning_rate': 3.9375e-05, 'epoch': 6.06, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 61%|██████    | 97/160 [30:50<18:36, 17.72s/it] 61%|██████▏   | 98/160 [31:08<18:15, 17.67s/it]                                                {'loss': 0.8418, 'learning_rate': 3.875e-05, 'epoch': 6.12, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 61%|██████▏   | 98/160 [31:08<18:15, 17.67s/it] 62%|██████▏   | 99/160 [31:26<18:09, 17.87s/it]                                                {'loss': 0.0185, 'learning_rate': 3.8125e-05, 'epoch': 6.19, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 62%|██████▏   | 99/160 [31:26<18:09, 17.87s/it] 62%|██████▎   | 100/160 [31:44<17:54, 17.91s/it]                                                 {'loss': 0.0208, 'learning_rate': 3.7500000000000003e-05, 'epoch': 6.25, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 62%|██████▎   | 100/160 [31:44<17:54, 17.91s/it] 63%|██████▎   | 101/160 [32:01<17:25, 17.72s/it]                                                 {'loss': 0.019, 'learning_rate': 3.6875e-05, 'epoch': 6.31, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 63%|██████▎   | 101/160 [32:01<17:25, 17.72s/it] 64%|██████▍   | 102/160 [32:19<17:18, 17.90s/it]                                                 {'loss': 0.0198, 'learning_rate': 3.625e-05, 'epoch': 6.38, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 64%|██████▍   | 102/160 [32:20<17:18, 17.90s/it] 64%|██████▍   | 103/160 [32:38<17:08, 18.04s/it]                                                 {'loss': 0.0183, 'learning_rate': 3.5625000000000005e-05, 'epoch': 6.44, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 64%|██████▍   | 103/160 [32:38<17:08, 18.04s/it] 65%|██████▌   | 104/160 [32:56<16:43, 17.93s/it]                                                 {'loss': 0.0175, 'learning_rate': 3.5e-05, 'epoch': 6.5, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 65%|██████▌   | 104/160 [32:56<16:43, 17.93s/it] 66%|██████▌   | 105/160 [33:15<16:58, 18.51s/it]                                                 {'loss': 0.0247, 'learning_rate': 3.4375e-05, 'epoch': 6.56, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 66%|██████▌   | 105/160 [33:15<16:58, 18.51s/it] 66%|██████▋   | 106/160 [33:33<16:32, 18.39s/it]                                                 {'loss': 0.0298, 'learning_rate': 3.375000000000001e-05, 'epoch': 6.62, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 66%|██████▋   | 106/160 [33:34<16:32, 18.39s/it] 67%|██████▋   | 107/160 [33:51<16:01, 18.14s/it]                                                 {'loss': 0.0169, 'learning_rate': 3.3125e-05, 'epoch': 6.69, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 67%|██████▋   | 107/160 [33:51<16:01, 18.14s/it] 68%|██████▊   | 108/160 [34:09<15:36, 18.00s/it]                                                 {'loss': 0.0202, 'learning_rate': 3.2500000000000004e-05, 'epoch': 6.75, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 68%|██████▊   | 108/160 [34:09<15:36, 18.00s/it] 68%|██████▊   | 109/160 [34:26<15:13, 17.91s/it]                                                 {'loss': 0.0142, 'learning_rate': 3.1875e-05, 'epoch': 6.81, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 68%|██████▊   | 109/160 [34:26<15:13, 17.91s/it] 69%|██████▉   | 110/160 [34:44<14:47, 17.75s/it]                                                 {'loss': 0.0196, 'learning_rate': 3.125e-05, 'epoch': 6.88, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 69%|██████▉   | 110/160 [34:44<14:47, 17.75s/it] 69%|██████▉   | 111/160 [35:01<14:28, 17.72s/it]                                                 {'loss': 0.0175, 'learning_rate': 3.0625000000000006e-05, 'epoch': 6.94, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 69%|██████▉   | 111/160 [35:01<14:28, 17.72s/it] 70%|███████   | 112/160 [35:20<14:24, 18.00s/it]                                                 {'loss': 0.0164, 'learning_rate': 3e-05, 'epoch': 7.0, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 70%|███████   | 112/160 [35:20<14:24, 18.00s/it] 71%|███████   | 113/160 [35:38<14:01, 17.91s/it]                                                 {'loss': 0.0116, 'learning_rate': 2.9375000000000003e-05, 'epoch': 7.06, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 71%|███████   | 113/160 [35:38<14:01, 17.91s/it] 71%|███████▏  | 114/160 [35:56<13:46, 17.98s/it]                                                 {'loss': 0.0108, 'learning_rate': 2.8749999999999997e-05, 'epoch': 7.12, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 71%|███████▏  | 114/160 [35:56<13:46, 17.98s/it] 72%|███████▏  | 115/160 [36:13<13:23, 17.85s/it]                                                 {'loss': 0.0146, 'learning_rate': 2.8125000000000003e-05, 'epoch': 7.19, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 72%|███████▏  | 115/160 [36:14<13:23, 17.85s/it] 72%|███████▎  | 116/160 [36:32<13:10, 17.98s/it]                                                 {'loss': 0.0208, 'learning_rate': 2.7500000000000004e-05, 'epoch': 7.25, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 72%|███████▎  | 116/160 [36:32<13:10, 17.98s/it] 73%|███████▎  | 117/160 [36:50<12:50, 17.91s/it]                                                 {'loss': 0.011, 'learning_rate': 2.6875e-05, 'epoch': 7.31, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 73%|███████▎  | 117/160 [36:50<12:50, 17.91s/it] 74%|███████▍  | 118/160 [37:07<12:26, 17.77s/it]                                                 {'loss': 0.0136, 'learning_rate': 2.625e-05, 'epoch': 7.38, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 74%|███████▍  | 118/160 [37:07<12:26, 17.77s/it] 74%|███████▍  | 119/160 [37:24<12:03, 17.65s/it]                                                 {'loss': 0.0127, 'learning_rate': 2.5625e-05, 'epoch': 7.44, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 74%|███████▍  | 119/160 [37:24<12:03, 17.65s/it] 75%|███████▌  | 120/160 [37:42<11:51, 17.80s/it]                                                 {'loss': 0.0152, 'learning_rate': 2.5e-05, 'epoch': 7.5, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 75%|███████▌  | 120/160 [37:43<11:51, 17.80s/it] 76%|███████▌  | 121/160 [38:01<11:38, 17.92s/it]                                                 {'loss': 0.0135, 'learning_rate': 2.4375e-05, 'epoch': 7.56, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 76%|███████▌  | 121/160 [38:01<11:38, 17.92s/it] 76%|███████▋  | 122/160 [38:19<11:20, 17.90s/it]                                                 {'loss': 0.0119, 'learning_rate': 2.375e-05, 'epoch': 7.62, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 76%|███████▋  | 122/160 [38:19<11:20, 17.90s/it] 77%|███████▋  | 123/160 [38:37<11:03, 17.93s/it]                                                 {'loss': 0.0126, 'learning_rate': 2.3125000000000003e-05, 'epoch': 7.69, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 77%|███████▋  | 123/160 [38:37<11:03, 17.93s/it] 78%|███████▊  | 124/160 [38:54<10:45, 17.92s/it]                                                 {'loss': 0.0124, 'learning_rate': 2.25e-05, 'epoch': 7.75, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 78%|███████▊  | 124/160 [38:54<10:45, 17.92s/it] 78%|███████▊  | 125/160 [39:12<10:23, 17.80s/it]                                                 {'loss': 0.0094, 'learning_rate': 2.1875e-05, 'epoch': 7.81, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 78%|███████▊  | 125/160 [39:12<10:23, 17.80s/it] 79%|███████▉  | 126/160 [39:30<10:05, 17.82s/it]                                                 {'loss': 0.013, 'learning_rate': 2.125e-05, 'epoch': 7.88, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 79%|███████▉  | 126/160 [39:30<10:05, 17.82s/it] 79%|███████▉  | 127/160 [39:48<09:49, 17.85s/it]                                                 {'loss': 0.0133, 'learning_rate': 2.0625e-05, 'epoch': 7.94, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 79%|███████▉  | 127/160 [39:48<09:49, 17.85s/it][2023-08-21 16:26:16,739] [INFO] [logging.py:68:log_dist] [Rank 0] step=128, skipped=0, lr=[2e-05], mom=[[0.9, 0.999]]
[2023-08-21 16:26:16,745] [INFO] [timer.py:241:stop] 0/128, RunningAvgSamplesPerSec=1.7748899381388135, CurrSamplesPerSec=1.6772613804429737, MemAllocated=2.3GB, MaxMemAllocated=79.67GB
 80%|████████  | 128/160 [40:07<09:43, 18.23s/it]                                                 {'loss': 0.0107, 'learning_rate': 2e-05, 'epoch': 8.0, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 80%|████████  | 128/160 [40:07<09:43, 18.23s/it] 81%|████████  | 129/160 [40:24<09:15, 17.92s/it]                                                 {'loss': 0.0101, 'learning_rate': 1.9375e-05, 'epoch': 8.06, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 81%|████████  | 129/160 [40:24<09:15, 17.92s/it] 81%|████████▏ | 130/160 [40:42<08:55, 17.83s/it]                                                 {'loss': 0.0095, 'learning_rate': 1.8750000000000002e-05, 'epoch': 8.12, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 81%|████████▏ | 130/160 [40:42<08:55, 17.83s/it] 82%|████████▏ | 131/160 [41:00<08:45, 18.11s/it]                                                 {'loss': 0.0093, 'learning_rate': 1.8125e-05, 'epoch': 8.19, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 82%|████████▏ | 131/160 [41:00<08:45, 18.11s/it] 82%|████████▎ | 132/160 [41:19<08:30, 18.22s/it]                                                 {'loss': 0.0092, 'learning_rate': 1.75e-05, 'epoch': 8.25, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 82%|████████▎ | 132/160 [41:19<08:30, 18.22s/it] 83%|████████▎ | 133/160 [41:36<08:06, 18.02s/it]                                                 {'loss': 0.0094, 'learning_rate': 1.6875000000000004e-05, 'epoch': 8.31, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.67, 'total_memory_available (GB)': 93.74}
 83%|████████▎ | 133/160 [41:36<08:06, 18.02s/it] 84%|████████▍ | 134/160 [41:54<07:42, 17.80s/it]                                                 {'loss': 0.0097, 'learning_rate': 1.6250000000000002e-05, 'epoch': 8.38, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 84%|████████▍ | 134/160 [41:54<07:42, 17.80s/it] 84%|████████▍ | 135/160 [42:12<07:25, 17.83s/it]                                                 {'loss': 0.0756, 'learning_rate': 1.5625e-05, 'epoch': 8.44, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 84%|████████▍ | 135/160 [42:12<07:25, 17.83s/it] 85%|████████▌ | 136/160 [42:29<07:04, 17.68s/it]                                                 {'loss': 0.0111, 'learning_rate': 1.5e-05, 'epoch': 8.5, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 85%|████████▌ | 136/160 [42:29<07:04, 17.68s/it] 86%|████████▌ | 137/160 [42:48<06:54, 18.03s/it]                                                 {'loss': 0.012, 'learning_rate': 1.4374999999999999e-05, 'epoch': 8.56, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 86%|████████▌ | 137/160 [42:48<06:54, 18.03s/it] 86%|████████▋ | 138/160 [43:07<06:40, 18.22s/it]                                                 {'loss': 0.009, 'learning_rate': 1.3750000000000002e-05, 'epoch': 8.62, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 86%|████████▋ | 138/160 [43:07<06:40, 18.22s/it] 87%|████████▋ | 139/160 [43:24<06:19, 18.05s/it]                                                 {'loss': 0.011, 'learning_rate': 1.3125e-05, 'epoch': 8.69, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 87%|████████▋ | 139/160 [43:24<06:19, 18.05s/it] 88%|████████▊ | 140/160 [43:42<05:58, 17.91s/it]                                                 {'loss': 0.0098, 'learning_rate': 1.25e-05, 'epoch': 8.75, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 88%|████████▊ | 140/160 [43:42<05:58, 17.91s/it] 88%|████████▊ | 141/160 [43:59<05:39, 17.85s/it]                                                 {'loss': 0.0092, 'learning_rate': 1.1875e-05, 'epoch': 8.81, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 88%|████████▊ | 141/160 [43:59<05:39, 17.85s/it] 89%|████████▉ | 142/160 [44:18<05:23, 17.99s/it]                                                 {'loss': 0.0094, 'learning_rate': 1.125e-05, 'epoch': 8.88, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 89%|████████▉ | 142/160 [44:18<05:23, 17.99s/it] 89%|████████▉ | 143/160 [44:35<05:03, 17.85s/it]                                                 {'loss': 0.0107, 'learning_rate': 1.0625e-05, 'epoch': 8.94, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 89%|████████▉ | 143/160 [44:35<05:03, 17.85s/it] 90%|█████████ | 144/160 [44:53<04:45, 17.85s/it]                                                 {'loss': 0.0101, 'learning_rate': 1e-05, 'epoch': 9.0, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 90%|█████████ | 144/160 [44:53<04:45, 17.85s/it] 91%|█████████ | 145/160 [45:11<04:30, 18.00s/it]                                                 {'loss': 0.0094, 'learning_rate': 9.375000000000001e-06, 'epoch': 9.06, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 91%|█████████ | 145/160 [45:12<04:30, 18.00s/it] 91%|█████████▏| 146/160 [45:29<04:11, 17.97s/it]                                                 {'loss': 0.0086, 'learning_rate': 8.75e-06, 'epoch': 9.12, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 91%|█████████▏| 146/160 [45:29<04:11, 17.97s/it] 92%|█████████▏| 147/160 [45:47<03:52, 17.90s/it]                                                 {'loss': 0.0093, 'learning_rate': 8.125000000000001e-06, 'epoch': 9.19, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 92%|█████████▏| 147/160 [45:47<03:52, 17.90s/it] 92%|█████████▎| 148/160 [46:05<03:33, 17.76s/it]                                                 {'loss': 0.0084, 'learning_rate': 7.5e-06, 'epoch': 9.25, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 92%|█████████▎| 148/160 [46:05<03:33, 17.76s/it] 93%|█████████▎| 149/160 [46:22<03:14, 17.70s/it]                                                 {'loss': 0.0094, 'learning_rate': 6.875000000000001e-06, 'epoch': 9.31, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 93%|█████████▎| 149/160 [46:22<03:14, 17.70s/it] 94%|█████████▍| 150/160 [46:39<02:55, 17.58s/it]                                                 {'loss': 0.0099, 'learning_rate': 6.25e-06, 'epoch': 9.38, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 94%|█████████▍| 150/160 [46:39<02:55, 17.58s/it] 94%|█████████▍| 151/160 [46:57<02:39, 17.71s/it]                                                 {'loss': 0.0076, 'learning_rate': 5.625e-06, 'epoch': 9.44, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 94%|█████████▍| 151/160 [46:57<02:39, 17.71s/it] 95%|█████████▌| 152/160 [47:16<02:23, 17.94s/it]                                                 {'loss': 0.0079, 'learning_rate': 5e-06, 'epoch': 9.5, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 95%|█████████▌| 152/160 [47:16<02:23, 17.94s/it] 96%|█████████▌| 153/160 [47:34<02:05, 17.95s/it]                                                 {'loss': 0.0089, 'learning_rate': 4.375e-06, 'epoch': 9.56, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 96%|█████████▌| 153/160 [47:34<02:05, 17.95s/it] 96%|█████████▋| 154/160 [47:51<01:46, 17.74s/it]                                                 {'loss': 0.0087, 'learning_rate': 3.75e-06, 'epoch': 9.62, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 96%|█████████▋| 154/160 [47:51<01:46, 17.74s/it] 97%|█████████▋| 155/160 [48:09<01:28, 17.77s/it]                                                 {'loss': 0.009, 'learning_rate': 3.125e-06, 'epoch': 9.69, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 97%|█████████▋| 155/160 [48:09<01:28, 17.77s/it] 98%|█████████▊| 156/160 [48:26<01:10, 17.69s/it]                                                 {'loss': 0.0092, 'learning_rate': 2.5e-06, 'epoch': 9.75, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 98%|█████████▊| 156/160 [48:26<01:10, 17.69s/it] 98%|█████████▊| 157/160 [48:44<00:53, 17.73s/it]                                                 {'loss': 0.009, 'learning_rate': 1.875e-06, 'epoch': 9.81, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 98%|█████████▊| 157/160 [48:44<00:53, 17.73s/it] 99%|█████████▉| 158/160 [49:02<00:35, 17.73s/it]                                                 {'loss': 0.0087, 'learning_rate': 1.25e-06, 'epoch': 9.88, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 99%|█████████▉| 158/160 [49:02<00:35, 17.73s/it] 99%|█████████▉| 159/160 [49:20<00:17, 17.75s/it]                                                 {'loss': 0.0093, 'learning_rate': 6.25e-07, 'epoch': 9.94, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
 99%|█████████▉| 159/160 [49:20<00:17, 17.75s/it]100%|██████████| 160/160 [49:37<00:00, 17.68s/it]                                                 {'loss': 0.0117, 'learning_rate': 0.0, 'epoch': 10.0, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
100%|██████████| 160/160 [49:37<00:00, 17.68s/it][INFO|trainer.py:1041] 2023-08-21 16:35:47,240 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 2977.8723, 'train_samples_per_second': 1.698, 'train_steps_per_second': 0.056, 'train_loss': 0.22238197326660156, 'epoch': 10.0, 'memory_allocated (GB)': 2.29, 'max_memory_allocated (GB)': 79.73, 'total_memory_available (GB)': 93.74}
100%|██████████| 160/160 [49:37<00:00, 17.68s/it]100%|██████████| 160/160 [49:37<00:00, 18.61s/it]
Traceback (most recent call last):
  File "run_clm_starcoder.py", line 568, in <module>
    main()
  File "run_clm_starcoder.py", line 564, in main
    model.save_pretrained(training_args.output_dir, state_dict=unwrapped_model.state_dict())
NameError: name 'unwrapped_model' is not defined
[2023-08-21 16:35:55,244] [INFO] [launch.py:354:main] Process 3517391 exits successfully.
[2023-08-21 16:35:55,245] [INFO] [launch.py:354:main] Process 3517389 exits successfully.
[2023-08-21 16:35:55,246] [INFO] [launch.py:322:sigkill_handler] Killing subprocess 3517388
[2023-08-21 16:35:55,246] [INFO] [launch.py:322:sigkill_handler] Killing subprocess 3517389
[2023-08-21 16:35:55,246] [INFO] [launch.py:322:sigkill_handler] Killing subprocess 3517390
[2023-08-21 16:35:55,254] [INFO] [launch.py:322:sigkill_handler] Killing subprocess 3517391
[2023-08-21 16:35:55,254] [ERROR] [launch.py:328:sigkill_handler] ['/usr/bin/python3', '-u', 'run_clm_starcoder.py', '--model_name_or_path', 'bigcode/starcoder', '--train_file', '/starcoder/finetune-data/roblox-train.json', '--validation_file', '/starcoder/finetune-data/roblox-validation.json', '--report_to', 'tensorboard', '--bf16', 'True', '--output_dir', './checkpoints/starcoder-full-deepspeed-offload-4-hpu-8-1-adamw', '--num_train_epochs', '10', '--per_device_train_batch_size', '8', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'no', '--learning_rate', '1e-4', '--logging_steps', '1', '--dataset_concatenation', '--do_train', '--use_habana', '--use_lazy_mode', '--throughput_warmup_steps', '3', '--cache_dir', '/starcoder/cache/', '--use_hpu_graphs_for_inference', '--deepspeed', 'deepspeed_config_s3_offload.json'] exits with return code = 1
[ERROR|distributed_runner.py:218] 2023-08-21 16:35:55,782 >> deepspeed --num_nodes 1 --num_gpus 4 --no_local_rank run_clm_starcoder.py --model_name_or_path bigcode/starcoder --train_file /starcoder/finetune-data/roblox-train.json --validation_file /starcoder/finetune-data/roblox-validation.json --report_to tensorboard --bf16 True --output_dir ./checkpoints/starcoder-full-deepspeed-offload-4-hpu-8-1-adamw --num_train_epochs 10 --per_device_train_batch_size 8 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy no --learning_rate 1e-4 --logging_steps 1 --dataset_concatenation --do_train --use_habana --use_lazy_mode --throughput_warmup_steps 3 --cache_dir /starcoder/cache/ --use_hpu_graphs_for_inference --deepspeed deepspeed_config_s3_offload.json  exited with status = 1
