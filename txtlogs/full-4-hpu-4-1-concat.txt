DistributedRunner run(): command = deepspeed --num_nodes 1 --num_gpus 4 --no_local_rank run_clm_starcoder.py --model_name_or_path bigcode/starcoder --train_file /starcoder/finetune-data/roblox-train.json --validation_file /starcoder/finetune-data/roblox-validation.json --report_to tensorboard --bf16 True --output_dir ./checkpoints/starcoder-full-deepspeed-4-hpu-4-1-concat --num_train_epochs 10 --per_device_train_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy no --learning_rate 1e-4 --logging_steps 1 --dataset_concatenation --do_train --use_habana --use_lazy_mode --throughput_warmup_steps 3 --cache_dir /starcoder/cache/ --use_hpu_graphs_for_inference --deepspeed deepspeed_config.json
[2023-08-18 21:31:39,702] [WARNING] [runner.py:185:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Namespace(autotuning='', elastic_training=False, exclude='', force_multi=False, hostfile='/job/hostfile', include='', launcher='pdsh', launcher_args='', master_addr='', master_port=29500, max_elastic_nodes=-1, min_elastic_nodes=-1, module=False, no_local_rank=True, no_python=False, no_ssh_check=False, num_gpus=4, num_nodes=1, save_pid=False, use_hpu=True, user_args=['--model_name_or_path', 'bigcode/starcoder', '--train_file', '/starcoder/finetune-data/roblox-train.json', '--validation_file', '/starcoder/finetune-data/roblox-validation.json', '--report_to', 'tensorboard', '--bf16', 'True', '--output_dir', './checkpoints/starcoder-full-deepspeed-4-hpu-4-1-concat', '--num_train_epochs', '10', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'no', '--learning_rate', '1e-4', '--logging_steps', '1', '--dataset_concatenation', '--do_train', '--use_habana', '--use_lazy_mode', '--throughput_warmup_steps', '3', '--cache_dir', '/starcoder/cache/', '--use_hpu_graphs_for_inference', '--deepspeed', 'deepspeed_config.json'], user_script='run_clm_starcoder.py')
[2023-08-18 21:31:41,459] [INFO] [runner.py:543:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --no_local_rank run_clm_starcoder.py --model_name_or_path bigcode/starcoder --train_file /starcoder/finetune-data/roblox-train.json --validation_file /starcoder/finetune-data/roblox-validation.json --report_to tensorboard --bf16 True --output_dir ./checkpoints/starcoder-full-deepspeed-4-hpu-4-1-concat --num_train_epochs 10 --per_device_train_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy no --learning_rate 1e-4 --logging_steps 1 --dataset_concatenation --do_train --use_habana --use_lazy_mode --throughput_warmup_steps 3 --cache_dir /starcoder/cache/ --use_hpu_graphs_for_inference --deepspeed deepspeed_config.json
[2023-08-18 21:31:43,406] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2023-08-18 21:31:43,406] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2023-08-18 21:31:43,406] [INFO] [launch.py:164:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2023-08-18 21:31:43,406] [INFO] [launch.py:165:main] dist_world_size=4
[2023-08-18 21:31:49,661] [INFO] [comm.py:762:init_distributed] Initializing TorchBackend in DeepSpeed with backend hccl
08/18/2023 21:31:50 - WARNING - __main__ -   Process rank: 0, device: hpu
distributed training: True, 16-bits training: True
08/18/2023 21:31:50 - WARNING - __main__ -   Process rank: 3, device: hpu
distributed training: True, 16-bits training: True
08/18/2023 21:31:50 - INFO - __main__ -   Training/evaluation parameters GaudiTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-06,
adjust_throughput=False,
auto_find_batch_size=False,
bf16=True,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=230,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=deepspeed_config.json,
disable_tqdm=False,
distribution_strategy=ddp,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fsdp_config=None,
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gaudi_config_name=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=hpu_amp,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./checkpoints/starcoder-full-deepspeed-4-hpu-4-1-concat/runs/Aug18_21-31-49_sysid674631,
logging_first_step=False,
logging_nan_inf_filter=False,
logging_steps=1,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
no_cuda=False,
non_blocking_data_copy=False,
num_train_epochs=10.0,
optim=adamw_torch,
optim_args=None,
output_dir=./checkpoints/starcoder-full-deepspeed-4-hpu-4-1-concat,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
pipelining_fwd_bwd=False,
prediction_loss_only=False,
profiling_steps=0,
profiling_warmup_steps=0,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=./checkpoints/starcoder-full-deepspeed-4-hpu-4-1-concat,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=no,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
throughput_warmup_steps=3,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
use_habana=True,
use_hpu_graphs=False,
use_hpu_graphs_for_inference=True,
use_hpu_graphs_for_training=False,
use_ipex=False,
use_lazy_mode=True,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
08/18/2023 21:31:50 - WARNING - __main__ -   Process rank: 1, device: hpu
distributed training: True, 16-bits training: True
08/18/2023 21:31:50 - WARNING - __main__ -   Process rank: 2, device: hpu
distributed training: True, 16-bits training: True
[INFO|configuration_utils.py:668] 2023-08-18 21:31:50,581 >> loading configuration file config.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/config.json
[INFO|configuration_utils.py:720] 2023-08-18 21:31:50,585 >> Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoder",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 6144,
  "n_head": 48,
  "n_inner": 24576,
  "n_layer": 40,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

[INFO|tokenization_utils_base.py:1809] 2023-08-18 21:31:50,706 >> loading file vocab.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/vocab.json
[INFO|tokenization_utils_base.py:1809] 2023-08-18 21:31:50,706 >> loading file merges.txt from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/merges.txt
[INFO|tokenization_utils_base.py:1809] 2023-08-18 21:31:50,706 >> loading file tokenizer.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/tokenizer.json
[INFO|tokenization_utils_base.py:1809] 2023-08-18 21:31:50,706 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2023-08-18 21:31:50,706 >> loading file special_tokens_map.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/special_tokens_map.json
[INFO|tokenization_utils_base.py:1809] 2023-08-18 21:31:50,706 >> loading file tokenizer_config.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/tokenizer_config.json
/usr/local/lib/python3.8/dist-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
[INFO|modeling_utils.py:2534] 2023-08-18 21:31:51,321 >> loading weights file pytorch_model.bin from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/pytorch_model.bin.index.json
[INFO|modeling_utils.py:1176] 2023-08-18 21:31:51,323 >> Instantiating GaudiGPTBigCodeForCausalLM model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2623] 2023-08-18 21:31:51,323 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:575] 2023-08-18 21:31:51,330 >> Generate config GaudiGenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "eos_token_id": 0,
  "ignore_eos": null,
  "static_shapes": null,
  "transformers_version": "4.28.1"
}

=============================HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_HPU_LAZY_EAGER_OPTIM_CACHE = 1
 PT_HPU_ENABLE_COMPILE_THREAD = 0
 PT_HPU_ENABLE_EXECUTION_THREAD = 1
 PT_HPU_ENABLE_LAZY_EAGER_EXECUTION_THREAD = 1
 PT_ENABLE_INTER_HOST_CACHING = 0
 PT_ENABLE_INFERENCE_MODE = 1
 PT_ENABLE_HABANA_CACHING = 1
 PT_HPU_MAX_RECIPE_SUBMISSION_LIMIT = 0
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_MAX_COMPOUND_OP_SIZE_SS = 10
 PT_HPU_ENABLE_STAGE_SUBMISSION = 1
 PT_HPU_STAGE_SUBMISSION_MODE = 2
 PT_HPU_PGM_ENABLE_CACHE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 0
 PT_HCCL_SLICE_SIZE_MB = 16
 PT_HCCL_MEMORY_ALLOWANCE_MB = 0
 PT_HPU_INITIAL_WORKSPACE_SIZE = 0
 PT_HABANA_POOL_SIZE = 24
 PT_HPU_POOL_STRATEGY = 5
 PT_HPU_POOL_LOG_FRAGMENTATION_INFO = 0
 PT_ENABLE_MEMORY_DEFRAGMENTATION = 1
 PT_ENABLE_DEFRAGMENTATION_INFO = 0
 PT_HPU_ENABLE_SYNAPSE_OUTPUT_PERMUTE = 1
 PT_HPU_ENABLE_VALID_DATA_RANGE_CHECK = 1
 PT_HPU_FORCE_USE_DEFAULT_STREAM = 0
 PT_RECIPE_CACHE_PATH = 
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_DYNAMIC_MIN_POLICY_ORDER = 4,5,3,1
 PT_HPU_DYNAMIC_MAX_POLICY_ORDER = 2,4,5,3,1
 PT_HPU_LAZY_ACC_PAR_MODE = 0
 PT_HPU_CLUSTERED_PROGRAM = 0
 PT_HPU_CLUSTERED_PROGRAM_ENFORCE = 0
 PT_HPU_CLUSTERED_PROGRAM_SPLIT_STR = default
 PT_HPU_CLUSTERED_PROGRAM_SCHED_STR = default
=============================SYSTEM CONFIGURATION ========================================= 
Num CPU Cores = 160
CPU RAM = 1056389528 KB 
============================================================================================ 
[2023-08-18 21:31:58,715] [INFO] [partition_parameters.py:481:__exit__] finished initializing model with 15.82B parameters
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
Loading checkpoint shards:  14%|█▍        | 1/7 [00:07<00:42,  7.04s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:07<00:42,  7.06s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:06<00:41,  6.89s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:07<00:42,  7.07s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:10<00:24,  4.90s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:10<00:24,  4.90s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:10<00:24,  4.91s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:10<00:24,  4.85s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:14<00:17,  4.37s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:14<00:17,  4.38s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:14<00:17,  4.39s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:14<00:17,  4.35s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:17<00:11,  3.97s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:17<00:11,  3.97s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:17<00:11,  3.94s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:17<00:11,  3.98s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:21<00:07,  3.80s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:21<00:07,  3.81s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:21<00:07,  3.81s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:20<00:07,  3.80s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:25<00:04,  4.01s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:25<00:04,  4.02s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:25<00:04,  4.02s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:25<00:04,  4.01s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:27<00:00,  3.47s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:27<00:00,  3.98s/it]
[INFO|modeling_utils.py:3190] 2023-08-18 21:32:26,689 >> All model checkpoint weights were used when initializing GaudiGPTBigCodeForCausalLM.

[INFO|modeling_utils.py:3198] 2023-08-18 21:32:26,689 >> All the weights of GaudiGPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoder.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GaudiGPTBigCodeForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 7/7 [00:27<00:00,  3.47s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:27<00:00,  3.98s/it]
Loading checkpoint shards: 100%|██████████| 7/7 [00:27<00:00,  3.46s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:27<00:00,  3.95s/it]
Loading checkpoint shards: 100%|██████████| 7/7 [00:27<00:00,  3.48s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:27<00:00,  3.98s/it]
[INFO|configuration_utils.py:537] 2023-08-18 21:32:26,829 >> loading configuration file generation_config.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/generation_config.json
[INFO|configuration_utils.py:575] 2023-08-18 21:32:26,830 >> Generate config GaudiGenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "eos_token_id": 0,
  "ignore_eos": null,
  "static_shapes": null,
  "transformers_version": "4.28.1"
}

08/18/2023 21:32:27 - INFO - __main__ -   Using data collator of type DataCollatorForSeq2Seq
Number of trainable paraemters: 0
[INFO|trainer.py:621] 2023-08-18 21:32:27,469 >> Using hpu_amp half precision backend
[2023-08-18 21:32:27,534] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.7.7+hpu.synapse.v1.10.0, git-hash=4bc77a6, git-branch=1.10.0
Number of trainable paraemters: 0
Number of trainable paraemters: 0
Number of trainable paraemters: 0
[2023-08-18 21:32:27,744] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-18 21:32:27,745] [INFO] [logging.py:68:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-08-18 21:32:27,745] [INFO] [logging.py:68:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-08-18 21:32:27,800] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdamW
[2023-08-18 21:32:27,800] [INFO] [utils.py:58:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdamW type=<class 'habana_frameworks.torch.hpex.optimizers.FusedAdamW.FusedAdamW'>
[2023-08-18 21:32:27,800] [INFO] [logging.py:68:log_dist] [Rank 0] Creating bf16 ZeRO stage 3 optimizer
[2023-08-18 21:32:27,800] [INFO] [logging.py:68:log_dist] [Rank 0] Creating bf16 ZeRO stage 3 optimizer
[2023-08-18 21:32:27,884] [INFO] [utils.py:930:see_memory_usage] Stage 3 initialize beginning
[2023-08-18 21:32:27,894] [WARNING] [utils.py:49:not_implemented] max_memory_reserved is not implemented on HPU
[2023-08-18 21:32:27,894] [INFO] [utils.py:931:see_memory_usage] MA 37.46 GB         Max_MA 38.59 GB         CA 37.46 GB         Max_CA 0 GB 
[2023-08-18 21:32:27,895] [INFO] [utils.py:939:see_memory_usage] CPU Virtual Memory:  used = 90.83 GB, percent = 9.0%
[2023-08-18 21:32:27,899] [INFO] [stage3.py:116:__init__] Reduce bucket size 500,000,000
[2023-08-18 21:32:27,899] [INFO] [stage3.py:117:__init__] Prefetch bucket size 50,000,000
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py38_cpu/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.844022274017334 seconds
Loading extension module utils...
Time to load utils op: 0.8059895038604736 seconds
Loading extension module utils...
Time to load utils op: 0.9061429500579834 seconds
Loading extension module utils...
Time to load utils op: 0.805790901184082 seconds
[2023-08-18 21:32:30,490] [INFO] [utils.py:930:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2023-08-18 21:32:30,501] [WARNING] [utils.py:49:not_implemented] max_memory_reserved is not implemented on HPU
[2023-08-18 21:32:30,501] [INFO] [utils.py:931:see_memory_usage] MA 37.46 GB         Max_MA 0.0 GB         CA 37.46 GB         Max_CA 0 GB 
[2023-08-18 21:32:30,501] [INFO] [utils.py:939:see_memory_usage] CPU Virtual Memory:  used = 90.86 GB, percent = 9.0%
Parameter Offload: Total persistent parameters: 2725888 in 322 params
[2023-08-18 21:32:30,610] [INFO] [utils.py:930:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2023-08-18 21:32:30,620] [WARNING] [utils.py:49:not_implemented] max_memory_reserved is not implemented on HPU
[2023-08-18 21:32:30,620] [INFO] [utils.py:931:see_memory_usage] MA 37.46 GB         Max_MA 0.0 GB         CA 37.46 GB         Max_CA 0 GB 
[2023-08-18 21:32:30,620] [INFO] [utils.py:939:see_memory_usage] CPU Virtual Memory:  used = 90.86 GB, percent = 9.0%
tcmalloc: large alloc 7758733312 bytes == 0x10ef38000 @  0x7f557b547680 0x7f557b568824 0x7f557b568b8a 0x7f5545fecadc 0x7f5545fc6ef7 0x7f54bcfa32f0 0x7f54bcf9ce38 0x7f54bcf9ceb8 0x7f54bcf9cf38 0x7f54bd7020c3 0x7f54be36f290 0x7f54be36f30f 0x7f54bdfa2e7b 0x7f54be32aa93 0x7f54bdfe7c65 0x7f54c802f33a 0x5f6939 0x5f7506 0x571019 0x5f6ce6 0x570556 0x5697da 0x5f6ec3 0x56b619 0x5697da 0x5f6ec3 0x59c427 0x5f746f 0x571019 0x50b07e 0x570556
tcmalloc: large alloc 7758733312 bytes == 0x10e3d4000 @  0x7f1e88b6b680 0x7f1e88b8c824 0x7f1e88b8cb8a 0x7f1dc96a1adc 0x7f1dc967bef7 0x7f1dca99f2f0 0x7f1dca998e38 0x7f1dca998eb8 0x7f1dca998f38 0x7f1dcb0fe0c3 0x7f1dcbd6b290 0x7f1dcbd6b30f 0x7f1dcb99ee7b 0x7f1dcbd26a93 0x7f1dcb9e3c65 0x7f1dd5a2b33a 0x5f6939 0x5f7506 0x571019 0x5f6ce6 0x570556 0x5697da 0x5f6ec3 0x56b619 0x5697da 0x5f6ec3 0x59c427 0x5f746f 0x571019 0x50b07e 0x570556
tcmalloc: large alloc 7758733312 bytes == 0x10ec18000 @  0x7f582777c680 0x7f582779d824 0x7f582779db8a 0x7f57e8205adc 0x7f57e81dfef7 0x7f57691d82f0 0x7f57691d1e38 0x7f57691d1eb8 0x7f57691d1f38 0x7f57699370c3 0x7f576a5a4290 0x7f576a5a430f 0x7f576a1d7e7b 0x7f576a55fa93 0x7f576a21cc65 0x7f577426433a 0x5f6939 0x5f7506 0x571019 0x5f6ce6 0x570556 0x5697da 0x5f6ec3 0x56b619 0x5697da 0x5f6ec3 0x59c427 0x5f746f 0x571019 0x50b07e 0x570556
tcmalloc: large alloc 7758733312 bytes == 0x10e810000 @  0x7f6b30063680 0x7f6b30084824 0x7f6b30084b8a 0x7f6a70b9cadc 0x7f6a70b76ef7 0x7f6a71e9a2f0 0x7f6a71e93e38 0x7f6a71e93eb8 0x7f6a71e93f38 0x7f6a725f90c3 0x7f6a73266290 0x7f6a7326630f 0x7f6a72e99e7b 0x7f6a73221a93 0x7f6a72edec65 0x7f6a7cf3533a 0x5f6939 0x5f7506 0x571019 0x5f6ce6 0x570556 0x5697da 0x5f6ec3 0x56b619 0x5697da 0x5f6ec3 0x59c427 0x5f746f 0x571019 0x50b07e 0x570556
[2023-08-18 21:32:55,198] [INFO] [stage3.py:378:_setup_for_real_optimizer] optimizer state initialized
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0007169246673583984 seconds
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0008003711700439453 secondsTime to load utils op: 0.0007872581481933594 seconds

You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
[2023-08-18 21:33:06,376] [INFO] [utils.py:930:see_memory_usage] After initializing ZeRO optimizer
[2023-08-18 21:33:06,386] [WARNING] [utils.py:49:not_implemented] max_memory_reserved is not implemented on HPU
[2023-08-18 21:33:06,386] [INFO] [utils.py:931:see_memory_usage] MA 58.58 GB         Max_MA 91.83 GB         CA 58.58 GB         Max_CA 0 GB 
[2023-08-18 21:33:06,386] [INFO] [utils.py:939:see_memory_usage] CPU Virtual Memory:  used = 97.66 GB, percent = 9.7%
[2023-08-18 21:33:06,388] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2023-08-18 21:33:06,388] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-08-18 21:33:06,389] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f1da556cd30>
[2023-08-18 21:33:06,389] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0001], mom=[(0.9, 0.999), (0.9, 0.999)]
[2023-08-18 21:33:06,390] [INFO] [config.py:1028:print] DeepSpeedEngine configuration:
[2023-08-18 21:33:06,390] [INFO] [config.py:1032:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-18 21:33:06,390] [INFO] [config.py:1032:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-18 21:33:06,390] [INFO] [config.py:1032:print]   amp_enabled .................. False
[2023-08-18 21:33:06,390] [INFO] [config.py:1032:print]   amp_params ................... False
[2023-08-18 21:33:06,391] [INFO] [config.py:1032:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-18 21:33:06,391] [INFO] [config.py:1032:print]   bfloat16_enabled ............. True
[2023-08-18 21:33:06,391] [INFO] [config.py:1032:print]   checkpoint_parallel_write_pipeline  False
[2023-08-18 21:33:06,391] [INFO] [config.py:1032:print]   checkpoint_tag_validation_enabled  True
[2023-08-18 21:33:06,391] [INFO] [config.py:1032:print]   checkpoint_tag_validation_fail  False
[2023-08-18 21:33:06,391] [INFO] [config.py:1032:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1d751d71c0>
[2023-08-18 21:33:06,391] [INFO] [config.py:1032:print]   communication_data_type ...... None
[2023-08-18 21:33:06,391] [INFO] [config.py:1032:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-18 21:33:06,391] [INFO] [config.py:1032:print]   curriculum_enabled ........... False
[2023-08-18 21:33:06,391] [INFO] [config.py:1032:print]   curriculum_params ............ False
[2023-08-18 21:33:06,391] [INFO] [config.py:1032:print]   dataloader_drop_last ......... False
[2023-08-18 21:33:06,391] [INFO] [config.py:1032:print]   disable_allgather ............ False
[2023-08-18 21:33:06,391] [INFO] [config.py:1032:print]   dump_state ................... False
[2023-08-18 21:33:06,391] [INFO] [config.py:1032:print]   dynamic_loss_scale_args ...... None
[2023-08-18 21:33:06,391] [INFO] [config.py:1032:print]   eigenvalue_enabled ........... False
[2023-08-18 21:33:06,391] [INFO] [config.py:1032:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-18 21:33:06,391] [INFO] [config.py:1032:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-18 21:33:06,391] [INFO] [config.py:1032:print]   eigenvalue_layer_num ......... 0
[2023-08-18 21:33:06,391] [INFO] [config.py:1032:print]   eigenvalue_max_iter .......... 100
[2023-08-18 21:33:06,391] [INFO] [config.py:1032:print]   eigenvalue_stability ......... 1e-06
[2023-08-18 21:33:06,391] [INFO] [config.py:1032:print]   eigenvalue_tol ............... 0.01
[2023-08-18 21:33:06,391] [INFO] [config.py:1032:print]   eigenvalue_verbose ........... False
[2023-08-18 21:33:06,391] [INFO] [config.py:1032:print]   elasticity_enabled ........... False
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   fp16_auto_cast ............... None
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   fp16_enabled ................. False
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   fp16_master_weights_and_gradients  False
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   global_rank .................. 0
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   grad_accum_dtype ............. None
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   gradient_accumulation_steps .. 1
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   gradient_clipping ............ 1
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   gradient_predivide_factor .... 1.0
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   initial_dynamic_scale ........ 1
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   load_universal_checkpoint .... False
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   loss_scale ................... 1.0
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   memory_breakdown ............. False
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7f1d751d74f0>
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   optimizer_legacy_fusion ...... False
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   optimizer_name ............... None
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   optimizer_params ............. None
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   pld_enabled .................. False
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   pld_params ................... False
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   prescale_gradients ........... False
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   scheduler_name ............... None
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   scheduler_params ............. None
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   sparse_attention ............. None
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   sparse_gradients_enabled ..... False
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   steps_per_print .............. 64
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   train_batch_size ............. 16
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   train_micro_batch_size_per_gpu  4
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   use_node_local_storage ....... False
[2023-08-18 21:33:06,392] [INFO] [config.py:1032:print]   wall_clock_breakdown ......... False
[2023-08-18 21:33:06,393] [INFO] [config.py:1032:print]   world_size ................... 4
[2023-08-18 21:33:06,393] [INFO] [config.py:1032:print]   zero_allow_comm_data_type_fp32  False
[2023-08-18 21:33:06,393] [INFO] [config.py:1032:print]   zero_allow_untested_optimizer  True
[2023-08-18 21:33:06,393] [INFO] [config.py:1032:print]   zero_config .................. stage=3 contiguous_gradients=False reduce_scatter=False reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False max_group_size=4000000000 load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2023-08-18 21:33:06,393] [INFO] [config.py:1032:print]   zero_enabled ................. True
[2023-08-18 21:33:06,393] [INFO] [config.py:1032:print]   zero_optimization_stage ...... 3
[2023-08-18 21:33:06,393] [INFO] [config.py:1017:print_user_config]   json = {
    "steps_per_print": 64, 
    "train_batch_size": 16, 
    "train_micro_batch_size_per_gpu": 4, 
    "gradient_accumulation_steps": 1, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1, 
    "zero_optimization": {
        "stage": 3, 
        "overlap_comm": false, 
        "reduce_scatter": false, 
        "contiguous_gradients": false
    }, 
    "zero_allow_untested_optimizer": true
}
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0004143714904785156 seconds
[INFO|trainer.py:770] 2023-08-18 21:33:06,395 >> ***** Running training *****
[INFO|trainer.py:771] 2023-08-18 21:33:06,395 >>   Num examples = 489
[INFO|trainer.py:772] 2023-08-18 21:33:06,395 >>   Num Epochs = 10
[INFO|trainer.py:773] 2023-08-18 21:33:06,395 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:774] 2023-08-18 21:33:06,395 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:775] 2023-08-18 21:33:06,395 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:776] 2023-08-18 21:33:06,395 >>   Total optimization steps = 310
[INFO|trainer.py:777] 2023-08-18 21:33:06,400 >>   Number of trainable parameters = 15,517,456,384
  0%|          | 0/310 [00:00<?, ?it/s][WARNING|logging.py:280] 2023-08-18 21:33:06,432 >> You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py", line 1451, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py", line 2532, in all_gather_into_tensor
    work = group._allgather_base(output_tensor, input_tensor)
RuntimeError: [Rank:2] FATAL ERROR :: MODULE:PT_DEVMEM Allocation failed for size::100663296 (96)MB

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py", line 1451, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py", line 2567, in _all_gather_base
    return all_gather_into_tensor(output_tensor, input_tensor, group, async_op)
  File "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py", line 1456, in wrapper
    "args": f"{args}, {kwargs}",
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor.py", line 437, in __repr__
    return torch._tensor_str._str(self, tensor_contents=tensor_contents)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 636, in _str
    return _str_intern(self, tensor_contents=tensor_contents)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 567, in _str_intern
    tensor_str = _tensor_str(self, indent)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 327, in _tensor_str
    formatter = _Formatter(get_summarized_data(self) if summarize else self)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 115, in __init__
    nonzero_finite_vals = torch.masked_select(
RuntimeError: Empty tensor optional

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run_clm_starcoder.py", line 568, in <module>
    main()
  File "run_clm_starcoder.py", line 561, in main
    trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/trainer.py", line 572, in train
    return inner_training_loop(
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/trainer.py", line 933, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/trainer.py", line 623, in training_step
    loss = self.compute_loss(model, inputs)
  File "/usr/local/lib/python3.8/dist-packages/transformers/trainer.py", line 2731, in compute_loss
    outputs = model(**inputs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1504, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/engine.py", line 1826, in forward
    loss = self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py", line 423, in forward
    transformer_outputs = self.transformer(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py", line 295, in gaudi_gpt_bigcode_model_forward
    outputs = block(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py", line 142, in gaudi_gpt_bigcode_block_forward
    feed_forward_hidden_states = self.mlp(hidden_states)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py", line 278, in forward
    hidden_states = self.c_proj(hidden_states)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1530, in _call_impl
    result = hook(self, args)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/parameter_offload.py", line 350, in _pre_forward_module_hook
    self.pre_sub_module_forward_function(module)
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/parameter_offload.py", line 473, in pre_sub_module_forward_function
    param_coordinator.fetch_sub_module(sub_module)
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py", line 262, in fetch_sub_module
    self.__all_gather_params(params_to_fetch)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py", line 405, in __all_gather_params
    handle = partitioned_params[0].all_gather_coalesced(partitioned_params)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/partition_parameters.py", line 962, in all_gather_coalesced
    handle = _dist_allgather_fn(partitions[self.rank],
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/partition_parameters.py", line 106, in _dist_allgather_fn
    return instrument_w_nvtx(dist.allgather_fn)(output_tensor,
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/comm/comm.py", line 356, in allgather_fn
    return all_gather_base(output_tensor,
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/comm/comm.py", line 131, in log_wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/comm/comm.py", line 332, in all_gather_base
    return cdb.all_gather_base(output_tensor=output_tensor,
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/comm/torch.py", line 84, in all_gather_base
    return torch.distributed.distributed_c10d._all_gather_base(
  File "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py", line 1456, in wrapper
    "args": f"{args}, {kwargs}",
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor.py", line 437, in __repr__
    return torch._tensor_str._str(self, tensor_contents=tensor_contents)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 636, in _str
    return _str_intern(self, tensor_contents=tensor_contents)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 567, in _str_intern
    tensor_str = _tensor_str(self, indent)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 327, in _tensor_str
    formatter = _Formatter(get_summarized_data(self) if summarize else self)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 115, in __init__
    nonzero_finite_vals = torch.masked_select(
RuntimeError: Empty tensor optional
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py", line 1451, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py", line 2532, in all_gather_into_tensor
    work = group._allgather_base(output_tensor, input_tensor)
RuntimeError: [Rank:0] FATAL ERROR :: MODULE:PT_DEVMEM Allocation failed for size::100663296 (96)MB

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py", line 1451, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py", line 2567, in _all_gather_base
    return all_gather_into_tensor(output_tensor, input_tensor, group, async_op)
  File "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py", line 1456, in wrapper
    "args": f"{args}, {kwargs}",
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor.py", line 437, in __repr__
    return torch._tensor_str._str(self, tensor_contents=tensor_contents)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 636, in _str
    return _str_intern(self, tensor_contents=tensor_contents)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 567, in _str_intern
    tensor_str = _tensor_str(self, indent)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 327, in _tensor_str
    formatter = _Formatter(get_summarized_data(self) if summarize else self)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 115, in __init__
    nonzero_finite_vals = torch.masked_select(
RuntimeError: Empty tensor optional

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run_clm_starcoder.py", line 568, in <module>
    main()
  File "run_clm_starcoder.py", line 561, in main
    trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/trainer.py", line 572, in train
    return inner_training_loop(
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/trainer.py", line 933, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/trainer.py", line 623, in training_step
    loss = self.compute_loss(model, inputs)
  File "/usr/local/lib/python3.8/dist-packages/transformers/trainer.py", line 2731, in compute_loss
    outputs = model(**inputs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1504, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/engine.py", line 1826, in forward
    loss = self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py", line 423, in forward
    transformer_outputs = self.transformer(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py", line 295, in gaudi_gpt_bigcode_model_forward
    outputs = block(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py", line 142, in gaudi_gpt_bigcode_block_forward
    feed_forward_hidden_states = self.mlp(hidden_states)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py", line 278, in forward
    hidden_states = self.c_proj(hidden_states)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1530, in _call_impl
    result = hook(self, args)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/parameter_offload.py", line 350, in _pre_forward_module_hook
    self.pre_sub_module_forward_function(module)
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/parameter_offload.py", line 473, in pre_sub_module_forward_function
    param_coordinator.fetch_sub_module(sub_module)
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py", line 262, in fetch_sub_module
    self.__all_gather_params(params_to_fetch)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py", line 405, in __all_gather_params
    handle = partitioned_params[0].all_gather_coalesced(partitioned_params)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/partition_parameters.py", line 962, in all_gather_coalesced
    handle = _dist_allgather_fn(partitions[self.rank],
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/partition_parameters.py", line 106, in _dist_allgather_fn
    return instrument_w_nvtx(dist.allgather_fn)(output_tensor,
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/comm/comm.py", line 356, in allgather_fn
    return all_gather_base(output_tensor,
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/comm/comm.py", line 131, in log_wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/comm/comm.py", line 332, in all_gather_base
    return cdb.all_gather_base(output_tensor=output_tensor,
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/comm/torch.py", line 84, in all_gather_base
    return torch.distributed.distributed_c10d._all_gather_base(
  File "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py", line 1456, in wrapper
    "args": f"{args}, {kwargs}",
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor.py", line 437, in __repr__
    return torch._tensor_str._str(self, tensor_contents=tensor_contents)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 636, in _str
    return _str_intern(self, tensor_contents=tensor_contents)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 567, in _str_intern
    tensor_str = _tensor_str(self, indent)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 327, in _tensor_str
    formatter = _Formatter(get_summarized_data(self) if summarize else self)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 115, in __init__
    nonzero_finite_vals = torch.masked_select(
RuntimeError: Empty tensor optional
  0%|          | 0/310 [00:24<?, ?it/s]
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py", line 1451, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py", line 2532, in all_gather_into_tensor
    work = group._allgather_base(output_tensor, input_tensor)
RuntimeError: [Rank:1] FATAL ERROR :: MODULE:PT_DEVMEM Allocation failed for size::100663296 (96)MB

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py", line 1451, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py", line 2567, in _all_gather_base
    return all_gather_into_tensor(output_tensor, input_tensor, group, async_op)
  File "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py", line 1456, in wrapper
    "args": f"{args}, {kwargs}",
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor.py", line 437, in __repr__
    return torch._tensor_str._str(self, tensor_contents=tensor_contents)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 636, in _str
    return _str_intern(self, tensor_contents=tensor_contents)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 567, in _str_intern
    tensor_str = _tensor_str(self, indent)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 327, in _tensor_str
    formatter = _Formatter(get_summarized_data(self) if summarize else self)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 115, in __init__
    nonzero_finite_vals = torch.masked_select(
RuntimeError: Empty tensor optional

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run_clm_starcoder.py", line 568, in <module>
    main()
  File "run_clm_starcoder.py", line 561, in main
    trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/trainer.py", line 572, in train
    return inner_training_loop(
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/trainer.py", line 933, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/trainer.py", line 623, in training_step
    loss = self.compute_loss(model, inputs)
  File "/usr/local/lib/python3.8/dist-packages/transformers/trainer.py", line 2731, in compute_loss
    outputs = model(**inputs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1504, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/engine.py", line 1826, in forward
    loss = self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py", line 423, in forward
    transformer_outputs = self.transformer(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py", line 295, in gaudi_gpt_bigcode_model_forward
    outputs = block(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py", line 142, in gaudi_gpt_bigcode_block_forward
    feed_forward_hidden_states = self.mlp(hidden_states)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py", line 278, in forward
    hidden_states = self.c_proj(hidden_states)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1530, in _call_impl
    result = hook(self, args)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/parameter_offload.py", line 350, in _pre_forward_module_hook
    self.pre_sub_module_forward_function(module)
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/parameter_offload.py", line 473, in pre_sub_module_forward_function
    param_coordinator.fetch_sub_module(sub_module)
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py", line 262, in fetch_sub_module
    self.__all_gather_params(params_to_fetch)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py", line 405, in __all_gather_params
    handle = partitioned_params[0].all_gather_coalesced(partitioned_params)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/partition_parameters.py", line 962, in all_gather_coalesced
    handle = _dist_allgather_fn(partitions[self.rank],
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/partition_parameters.py", line 106, in _dist_allgather_fn
    return instrument_w_nvtx(dist.allgather_fn)(output_tensor,
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/comm/comm.py", line 356, in allgather_fn
    return all_gather_base(output_tensor,
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/comm/comm.py", line 131, in log_wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/comm/comm.py", line 332, in all_gather_base
    return cdb.all_gather_base(output_tensor=output_tensor,
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/comm/torch.py", line 84, in all_gather_base
    return torch.distributed.distributed_c10d._all_gather_base(
  File "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py", line 1456, in wrapper
    "args": f"{args}, {kwargs}",
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor.py", line 437, in __repr__
    return torch._tensor_str._str(self, tensor_contents=tensor_contents)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 636, in _str
    return _str_intern(self, tensor_contents=tensor_contents)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 567, in _str_intern
    tensor_str = _tensor_str(self, indent)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 327, in _tensor_str
    formatter = _Formatter(get_summarized_data(self) if summarize else self)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 115, in __init__
    nonzero_finite_vals = torch.masked_select(
RuntimeError: Empty tensor optional
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py", line 1451, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py", line 2532, in all_gather_into_tensor
    work = group._allgather_base(output_tensor, input_tensor)
RuntimeError: [Rank:3] FATAL ERROR :: MODULE:PT_DEVMEM Allocation failed for size::100663296 (96)MB

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py", line 1451, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py", line 2567, in _all_gather_base
    return all_gather_into_tensor(output_tensor, input_tensor, group, async_op)
  File "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py", line 1456, in wrapper
    "args": f"{args}, {kwargs}",
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor.py", line 437, in __repr__
    return torch._tensor_str._str(self, tensor_contents=tensor_contents)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 636, in _str
    return _str_intern(self, tensor_contents=tensor_contents)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 567, in _str_intern
    tensor_str = _tensor_str(self, indent)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 327, in _tensor_str
    formatter = _Formatter(get_summarized_data(self) if summarize else self)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 115, in __init__
    nonzero_finite_vals = torch.masked_select(
RuntimeError: Empty tensor optional

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "run_clm_starcoder.py", line 568, in <module>
    main()
  File "run_clm_starcoder.py", line 561, in main
    trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/trainer.py", line 572, in train
    return inner_training_loop(
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/trainer.py", line 933, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/trainer.py", line 623, in training_step
    loss = self.compute_loss(model, inputs)
  File "/usr/local/lib/python3.8/dist-packages/transformers/trainer.py", line 2731, in compute_loss
    outputs = model(**inputs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1504, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/engine.py", line 1826, in forward
    loss = self.module(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py", line 423, in forward
    transformer_outputs = self.transformer(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py", line 295, in gaudi_gpt_bigcode_model_forward
    outputs = block(
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py", line 142, in gaudi_gpt_bigcode_block_forward
    feed_forward_hidden_states = self.mlp(hidden_states)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py", line 278, in forward
    hidden_states = self.c_proj(hidden_states)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1530, in _call_impl
    result = hook(self, args)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/parameter_offload.py", line 350, in _pre_forward_module_hook
    self.pre_sub_module_forward_function(module)
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/parameter_offload.py", line 473, in pre_sub_module_forward_function
    param_coordinator.fetch_sub_module(sub_module)
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py", line 262, in fetch_sub_module
    self.__all_gather_params(params_to_fetch)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py", line 405, in __all_gather_params
    handle = partitioned_params[0].all_gather_coalesced(partitioned_params)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/partition_parameters.py", line 962, in all_gather_coalesced
    handle = _dist_allgather_fn(partitions[self.rank],
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/partition_parameters.py", line 106, in _dist_allgather_fn
    return instrument_w_nvtx(dist.allgather_fn)(output_tensor,
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/comm/comm.py", line 356, in allgather_fn
    return all_gather_base(output_tensor,
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/comm/comm.py", line 131, in log_wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/comm/comm.py", line 332, in all_gather_base
    return cdb.all_gather_base(output_tensor=output_tensor,
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/comm/torch.py", line 84, in all_gather_base
    return torch.distributed.distributed_c10d._all_gather_base(
  File "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py", line 1456, in wrapper
    "args": f"{args}, {kwargs}",
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor.py", line 437, in __repr__
    return torch._tensor_str._str(self, tensor_contents=tensor_contents)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 636, in _str
    return _str_intern(self, tensor_contents=tensor_contents)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 567, in _str_intern
    tensor_str = _tensor_str(self, indent)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 327, in _tensor_str
    formatter = _Formatter(get_summarized_data(self) if summarize else self)
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor_str.py", line 115, in __init__
    nonzero_finite_vals = torch.masked_select(
RuntimeError: Empty tensor optional
[2023-08-18 21:33:33,594] [INFO] [launch.py:322:sigkill_handler] Killing subprocess 2129677
[2023-08-18 21:33:34,948] [INFO] [launch.py:322:sigkill_handler] Killing subprocess 2129678
[2023-08-18 21:33:35,325] [INFO] [launch.py:322:sigkill_handler] Killing subprocess 2129679
[2023-08-18 21:33:35,325] [INFO] [launch.py:322:sigkill_handler] Killing subprocess 2129680
[2023-08-18 21:33:35,422] [ERROR] [launch.py:328:sigkill_handler] ['/usr/bin/python3', '-u', 'run_clm_starcoder.py', '--model_name_or_path', 'bigcode/starcoder', '--train_file', '/starcoder/finetune-data/roblox-train.json', '--validation_file', '/starcoder/finetune-data/roblox-validation.json', '--report_to', 'tensorboard', '--bf16', 'True', '--output_dir', './checkpoints/starcoder-full-deepspeed-4-hpu-4-1-concat', '--num_train_epochs', '10', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'no', '--learning_rate', '1e-4', '--logging_steps', '1', '--dataset_concatenation', '--do_train', '--use_habana', '--use_lazy_mode', '--throughput_warmup_steps', '3', '--cache_dir', '/starcoder/cache/', '--use_hpu_graphs_for_inference', '--deepspeed', 'deepspeed_config.json'] exits with return code = 1
[ERROR|distributed_runner.py:218] 2023-08-18 21:33:36,169 >> deepspeed --num_nodes 1 --num_gpus 4 --no_local_rank run_clm_starcoder.py --model_name_or_path bigcode/starcoder --train_file /starcoder/finetune-data/roblox-train.json --validation_file /starcoder/finetune-data/roblox-validation.json --report_to tensorboard --bf16 True --output_dir ./checkpoints/starcoder-full-deepspeed-4-hpu-4-1-concat --num_train_epochs 10 --per_device_train_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy no --learning_rate 1e-4 --logging_steps 1 --dataset_concatenation --do_train --use_habana --use_lazy_mode --throughput_warmup_steps 3 --cache_dir /starcoder/cache/ --use_hpu_graphs_for_inference --deepspeed deepspeed_config.json  exited with status = 1
