DistributedRunner run(): command = deepspeed --num_nodes 1 --num_gpus 4 --no_local_rank run_clm_starcoder.py --model_name_or_path bigcode/starcoder --train_file /starcoder/finetune-data/roblox-train.json --validation_file /starcoder/finetune-data/roblox-validation.json --report_to tensorboard --bf16 True --output_dir ./checkpoints/starcoder-full-deepspeed-offload-4-hpu-2-2-concat --num_train_epochs 10 --per_device_train_batch_size 2 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy no --learning_rate 1e-4 --logging_steps 1 --dataset_concatenation --do_train --use_habana --use_lazy_mode --throughput_warmup_steps 3 --cache_dir /starcoder/cache/ --use_hpu_graphs_for_inference --deepspeed deepspeed_config_s3_offload.json
[2023-08-20 10:03:52,160] [WARNING] [runner.py:185:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Namespace(autotuning='', elastic_training=False, exclude='', force_multi=False, hostfile='/job/hostfile', include='', launcher='pdsh', launcher_args='', master_addr='', master_port=29500, max_elastic_nodes=-1, min_elastic_nodes=-1, module=False, no_local_rank=True, no_python=False, no_ssh_check=False, num_gpus=4, num_nodes=1, save_pid=False, use_hpu=True, user_args=['--model_name_or_path', 'bigcode/starcoder', '--train_file', '/starcoder/finetune-data/roblox-train.json', '--validation_file', '/starcoder/finetune-data/roblox-validation.json', '--report_to', 'tensorboard', '--bf16', 'True', '--output_dir', './checkpoints/starcoder-full-deepspeed-offload-4-hpu-2-2-concat', '--num_train_epochs', '10', '--per_device_train_batch_size', '2', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'no', '--learning_rate', '1e-4', '--logging_steps', '1', '--dataset_concatenation', '--do_train', '--use_habana', '--use_lazy_mode', '--throughput_warmup_steps', '3', '--cache_dir', '/starcoder/cache/', '--use_hpu_graphs_for_inference', '--deepspeed', 'deepspeed_config_s3_offload.json'], user_script='run_clm_starcoder.py')
[2023-08-20 10:03:53,877] [INFO] [runner.py:543:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --no_local_rank run_clm_starcoder.py --model_name_or_path bigcode/starcoder --train_file /starcoder/finetune-data/roblox-train.json --validation_file /starcoder/finetune-data/roblox-validation.json --report_to tensorboard --bf16 True --output_dir ./checkpoints/starcoder-full-deepspeed-offload-4-hpu-2-2-concat --num_train_epochs 10 --per_device_train_batch_size 2 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy no --learning_rate 1e-4 --logging_steps 1 --dataset_concatenation --do_train --use_habana --use_lazy_mode --throughput_warmup_steps 3 --cache_dir /starcoder/cache/ --use_hpu_graphs_for_inference --deepspeed deepspeed_config_s3_offload.json
[2023-08-20 10:03:55,752] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2023-08-20 10:03:55,752] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2023-08-20 10:03:55,752] [INFO] [launch.py:164:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2023-08-20 10:03:55,752] [INFO] [launch.py:165:main] dist_world_size=4
[2023-08-20 10:04:01,664] [INFO] [comm.py:762:init_distributed] Initializing TorchBackend in DeepSpeed with backend hccl
08/20/2023 10:04:02 - WARNING - __main__ -   Process rank: 0, device: hpu
distributed training: True, 16-bits training: True
08/20/2023 10:04:02 - WARNING - __main__ -   Process rank: 1, device: hpu
distributed training: True, 16-bits training: True
08/20/2023 10:04:02 - INFO - __main__ -   Training/evaluation parameters GaudiTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-06,
adjust_throughput=False,
auto_find_batch_size=False,
bf16=True,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=230,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=deepspeed_config_s3_offload.json,
disable_tqdm=False,
distribution_strategy=ddp,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fsdp_config=None,
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gaudi_config_name=None,
gradient_accumulation_steps=2,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=hpu_amp,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./checkpoints/starcoder-full-deepspeed-offload-4-hpu-2-2-concat/runs/Aug20_10-04-01_sysid674631,
logging_first_step=False,
logging_nan_inf_filter=False,
logging_steps=1,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
no_cuda=False,
non_blocking_data_copy=False,
num_train_epochs=10.0,
optim=adamw_torch,
optim_args=None,
output_dir=./checkpoints/starcoder-full-deepspeed-offload-4-hpu-2-2-concat,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=2,
pipelining_fwd_bwd=False,
prediction_loss_only=False,
profiling_steps=0,
profiling_warmup_steps=0,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=./checkpoints/starcoder-full-deepspeed-offload-4-hpu-2-2-concat,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=no,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
throughput_warmup_steps=3,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
use_habana=True,
use_hpu_graphs=False,
use_hpu_graphs_for_inference=True,
use_hpu_graphs_for_training=False,
use_ipex=False,
use_lazy_mode=True,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
08/20/2023 10:04:02 - WARNING - __main__ -   Process rank: 2, device: hpu
distributed training: True, 16-bits training: True
08/20/2023 10:04:02 - WARNING - __main__ -   Process rank: 3, device: hpu
distributed training: True, 16-bits training: True
[INFO|configuration_utils.py:668] 2023-08-20 10:04:02,734 >> loading configuration file config.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/config.json
[INFO|configuration_utils.py:720] 2023-08-20 10:04:02,738 >> Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoder",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 6144,
  "n_head": 48,
  "n_inner": 24576,
  "n_layer": 40,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.28.1",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

[INFO|tokenization_utils_base.py:1809] 2023-08-20 10:04:02,849 >> loading file vocab.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/vocab.json
[INFO|tokenization_utils_base.py:1809] 2023-08-20 10:04:02,850 >> loading file merges.txt from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/merges.txt
[INFO|tokenization_utils_base.py:1809] 2023-08-20 10:04:02,850 >> loading file tokenizer.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/tokenizer.json
[INFO|tokenization_utils_base.py:1809] 2023-08-20 10:04:02,850 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2023-08-20 10:04:02,850 >> loading file special_tokens_map.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/special_tokens_map.json
[INFO|tokenization_utils_base.py:1809] 2023-08-20 10:04:02,850 >> loading file tokenizer_config.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/tokenizer_config.json
/usr/local/lib/python3.8/dist-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
[INFO|modeling_utils.py:2534] 2023-08-20 10:04:03,446 >> loading weights file pytorch_model.bin from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/pytorch_model.bin.index.json
[INFO|modeling_utils.py:1176] 2023-08-20 10:04:03,448 >> Instantiating GaudiGPTBigCodeForCausalLM model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2623] 2023-08-20 10:04:03,448 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:575] 2023-08-20 10:04:03,455 >> Generate config GaudiGenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "eos_token_id": 0,
  "ignore_eos": null,
  "static_shapes": null,
  "transformers_version": "4.28.1"
}

=============================HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_HPU_LAZY_EAGER_OPTIM_CACHE = 1
 PT_HPU_ENABLE_COMPILE_THREAD = 0
 PT_HPU_ENABLE_EXECUTION_THREAD = 1
 PT_HPU_ENABLE_LAZY_EAGER_EXECUTION_THREAD = 1
 PT_ENABLE_INTER_HOST_CACHING = 0
 PT_ENABLE_INFERENCE_MODE = 1
 PT_ENABLE_HABANA_CACHING = 1
 PT_HPU_MAX_RECIPE_SUBMISSION_LIMIT = 0
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_MAX_COMPOUND_OP_SIZE_SS = 10
 PT_HPU_ENABLE_STAGE_SUBMISSION = 1
 PT_HPU_STAGE_SUBMISSION_MODE = 2
 PT_HPU_PGM_ENABLE_CACHE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 0
 PT_HCCL_SLICE_SIZE_MB = 16
 PT_HCCL_MEMORY_ALLOWANCE_MB = 0
 PT_HPU_INITIAL_WORKSPACE_SIZE = 0
 PT_HABANA_POOL_SIZE = 24
 PT_HPU_POOL_STRATEGY = 5
 PT_HPU_POOL_LOG_FRAGMENTATION_INFO = 0
 PT_ENABLE_MEMORY_DEFRAGMENTATION = 1
 PT_ENABLE_DEFRAGMENTATION_INFO = 0
 PT_HPU_ENABLE_SYNAPSE_OUTPUT_PERMUTE = 1
 PT_HPU_ENABLE_VALID_DATA_RANGE_CHECK = 1
 PT_HPU_FORCE_USE_DEFAULT_STREAM = 0
 PT_RECIPE_CACHE_PATH = 
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_DYNAMIC_MIN_POLICY_ORDER = 4,5,3,1
 PT_HPU_DYNAMIC_MAX_POLICY_ORDER = 2,4,5,3,1
 PT_HPU_LAZY_ACC_PAR_MODE = 0
 PT_HPU_CLUSTERED_PROGRAM = 0
 PT_HPU_CLUSTERED_PROGRAM_ENFORCE = 0
 PT_HPU_CLUSTERED_PROGRAM_SPLIT_STR = default
 PT_HPU_CLUSTERED_PROGRAM_SCHED_STR = default
=============================SYSTEM CONFIGURATION ========================================= 
Num CPU Cores = 160
CPU RAM = 1056389528 KB 
============================================================================================ 
[2023-08-20 10:04:16,277] [INFO] [partition_parameters.py:481:__exit__] finished initializing model with 15.82B parameters
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
Loading checkpoint shards:  14%|█▍        | 1/7 [00:07<00:43,  7.28s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:07<00:43,  7.29s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:07<00:43,  7.28s/it]Loading checkpoint shards:  14%|█▍        | 1/7 [00:07<00:44,  7.34s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:11<00:27,  5.48s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:11<00:27,  5.48s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:11<00:27,  5.51s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:11<00:27,  5.49s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:15<00:19,  4.84s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:15<00:19,  4.83s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:15<00:19,  4.84s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:15<00:19,  4.84s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:20<00:14,  4.75s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:20<00:14,  4.75s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:20<00:14,  4.75s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:20<00:14,  4.75s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:26<00:10,  5.28s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:26<00:10,  5.29s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:26<00:10,  5.29s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:26<00:10,  5.39s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:33<00:05,  5.73s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:33<00:05,  5.70s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:33<00:05,  5.73s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:33<00:05,  5.73s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:35<00:00,  4.82s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:35<00:00,  5.14s/it]
Loading checkpoint shards: 100%|██████████| 7/7 [00:35<00:00,  4.82s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:35<00:00,  5.14s/it]
Loading checkpoint shards: 100%|██████████| 7/7 [00:36<00:00,  4.84s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:36<00:00,  5.15s/it]
Loading checkpoint shards: 100%|██████████| 7/7 [00:36<00:00,  4.82s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:36<00:00,  5.15s/it]
[INFO|modeling_utils.py:3190] 2023-08-20 10:04:52,452 >> All model checkpoint weights were used when initializing GaudiGPTBigCodeForCausalLM.

[INFO|modeling_utils.py:3198] 2023-08-20 10:04:52,452 >> All the weights of GaudiGPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoder.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GaudiGPTBigCodeForCausalLM for predictions without further training.
[INFO|configuration_utils.py:537] 2023-08-20 10:04:52,576 >> loading configuration file generation_config.json from cache at /starcoder/cache/models--bigcode--starcoder/snapshots/7c6927d25ac2ec0b9e81d98bd54926e36f5c9de1/generation_config.json
[INFO|configuration_utils.py:575] 2023-08-20 10:04:52,577 >> Generate config GaudiGenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "eos_token_id": 0,
  "ignore_eos": null,
  "static_shapes": null,
  "transformers_version": "4.28.1"
}

08/20/2023 10:04:53 - INFO - __main__ -   Using data collator of type DataCollatorForSeq2Seq
Number of trainable paraemters: 0
[INFO|trainer.py:621] 2023-08-20 10:04:53,217 >> Using hpu_amp half precision backend
[INFO|deepspeed.py:289] 2023-08-20 10:04:53,264 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
[2023-08-20 10:04:53,283] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.7.7+hpu.synapse.v1.10.0, git-hash=4bc77a6, git-branch=1.10.0
Number of trainable paraemters: 0
Number of trainable paraemters: 0
Number of trainable paraemters: 0
[2023-08-20 10:04:53,492] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-08-20 10:04:53,493] [INFO] [logging.py:68:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-08-20 10:04:53,493] [INFO] [logging.py:68:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-08-20 10:04:53,548] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdamW
[2023-08-20 10:04:53,548] [INFO] [utils.py:58:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdamW type=<class 'habana_frameworks.torch.hpex.optimizers.FusedAdamW.FusedAdamW'>
[2023-08-20 10:04:53,548] [INFO] [logging.py:68:log_dist] [Rank 0] Creating bf16 ZeRO stage 3 optimizer
[2023-08-20 10:04:53,548] [INFO] [logging.py:68:log_dist] [Rank 0] Creating bf16 ZeRO stage 3 optimizer
[2023-08-20 10:04:53,667] [INFO] [utils.py:930:see_memory_usage] Stage 3 initialize beginning
[2023-08-20 10:04:53,677] [WARNING] [utils.py:49:not_implemented] max_memory_reserved is not implemented on HPU
[2023-08-20 10:04:53,678] [INFO] [utils.py:931:see_memory_usage] MA 0.07 GB         Max_MA 1.75 GB         CA 0.07 GB         Max_CA 0 GB 
[2023-08-20 10:04:53,678] [INFO] [utils.py:939:see_memory_usage] CPU Virtual Memory:  used = 120.91 GB, percent = 12.0%
[2023-08-20 10:04:53,682] [INFO] [stage3.py:116:__init__] Reduce bucket size 500,000,000
[2023-08-20 10:04:53,682] [INFO] [stage3.py:117:__init__] Prefetch bucket size 50,000,000
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py38_cpu as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py38_cpu/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.9677674770355225 seconds
Loading extension module utils...
Time to load utils op: 0.7082045078277588 seconds
Loading extension module utils...
Time to load utils op: 0.7059140205383301 seconds
Loading extension module utils...
Time to load utils op: 0.7066137790679932 seconds
[2023-08-20 10:04:56,615] [INFO] [utils.py:930:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2023-08-20 10:04:56,626] [WARNING] [utils.py:49:not_implemented] max_memory_reserved is not implemented on HPU
[2023-08-20 10:04:56,626] [INFO] [utils.py:931:see_memory_usage] MA 0.07 GB         Max_MA 0.0 GB         CA 0.07 GB         Max_CA 0 GB 
[2023-08-20 10:04:56,626] [INFO] [utils.py:939:see_memory_usage] CPU Virtual Memory:  used = 120.92 GB, percent = 12.0%
Parameter Offload: Total persistent parameters: 2725888 in 322 params
[2023-08-20 10:04:56,738] [INFO] [utils.py:930:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2023-08-20 10:04:56,749] [WARNING] [utils.py:49:not_implemented] max_memory_reserved is not implemented on HPU
[2023-08-20 10:04:56,749] [INFO] [utils.py:931:see_memory_usage] MA 0.07 GB         Max_MA 0.0 GB         CA 0.07 GB         Max_CA 0 GB 
[2023-08-20 10:04:56,749] [INFO] [utils.py:939:see_memory_usage] CPU Virtual Memory:  used = 120.92 GB, percent = 12.0%
tcmalloc: large alloc 7757365248 bytes == 0x28bc52000 @  0x7f5bc54fe680 0x7f5bc551f824 0x7f5bc551fb8a 0x7f5b06034adc 0x7f5b0600eef7 0x7f5b073322f0 0x7f5b0732be38 0x7f5b0732beb8 0x7f5b0732bf38 0x7f5b07a910c3 0x7f5b086fe290 0x7f5b086fe30f 0x7f5b08331e7b 0x7f5b086b9a93 0x7f5b08376c65 0x7f5b123be33a 0x5f6939 0x5f7506 0x571019 0x5f6ce6 0x56b619 0x5697da 0x5f6ec3 0x56b619 0x5697da 0x5f6ec3 0x59c427 0x5f746f 0x571019 0x50b07e 0x570556
tcmalloc: large alloc 7757365248 bytes == 0x28b66a000 @  0x7f11163fb680 0x7f111641c824 0x7f111641cb8a 0x7f1056f34adc 0x7f1056f0eef7 0x7f10582322f0 0x7f105822be38 0x7f105822beb8 0x7f105822bf38 0x7f10589910c3 0x7f10595fe290 0x7f10595fe30f 0x7f1059231e7b 0x7f10595b9a93 0x7f1059276c65 0x7f10632cd33a 0x5f6939 0x5f7506 0x571019 0x5f6ce6 0x56b619 0x5697da 0x5f6ec3 0x56b619 0x5697da 0x5f6ec3 0x59c427 0x5f746f 0x571019 0x50b07e 0x570556
tcmalloc: large alloc 7757365248 bytes == 0x28bfb0000 @  0x7f88fb9b3680 0x7f88fb9d4824 0x7f88fb9d4b8a 0x7f883c4e9adc 0x7f883c4c3ef7 0x7f883d7e72f0 0x7f883d7e0e38 0x7f883d7e0eb8 0x7f883d7e0f38 0x7f883df460c3 0x7f883ebb3290 0x7f883ebb330f 0x7f883e7e6e7b 0x7f883eb6ea93 0x7f883e82bc65 0x7f884887333a 0x5f6939 0x5f7506 0x571019 0x5f6ce6 0x56b619 0x5697da 0x5f6ec3 0x56b619 0x5697da 0x5f6ec3 0x59c427 0x5f746f 0x571019 0x50b07e 0x570556
tcmalloc: large alloc 7757365248 bytes == 0x28b8d2000 @  0x7fd0c5558680 0x7fd0c5579824 0x7fd0c5579b8a 0x7fd085fe1adc 0x7fd085fbbef7 0x7fd006fb42f0 0x7fd006fade38 0x7fd006fadeb8 0x7fd006fadf38 0x7fd0077130c3 0x7fd008380290 0x7fd00838030f 0x7fd007fb3e7b 0x7fd00833ba93 0x7fd007ff8c65 0x7fd01204033a 0x5f6939 0x5f7506 0x571019 0x5f6ce6 0x56b619 0x5697da 0x5f6ec3 0x56b619 0x5697da 0x5f6ec3 0x59c427 0x5f746f 0x571019 0x50b07e 0x570556
Traceback (most recent call last):
  File "run_clm_starcoder.py", line 568, in <module>
    main()
  File "run_clm_starcoder.py", line 561, in main
    trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/trainer.py", line 572, in train
    return inner_training_loop(
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/trainer.py", line 698, in _inner_training_loop
    deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/deepspeed.py", line 158, in deepspeed_init
    deepspeed_engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/__init__.py", line 125, in initialize
    engine = DeepSpeedEngine(args=args,
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/engine.py", line 380, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/engine.py", line 1279, in _configure_optimizer
    self.optimizer = self._configure_zero_optimizer(basic_optimizer)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/engine.py", line 1603, in _configure_zero_optimizer
    optimizer = DeepSpeedZeroOptimizer_Stage3(
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/stage3.py", line 318, in __init__
    self._setup_for_real_optimizer()
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/stage3.py", line 373, in _setup_for_real_optimizer
    self.initialize_optimizer_states()
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/stage3.py", line 948, in initialize_optimizer_states
    self._optimizer_step(i)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/stage3.py", line 867, in _optimizer_step
    self.optimizer.step()
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/hpex/optimizers/FusedAdamW.py", line 58, in wrap_
    result = step_func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/hpex/optimizers/FusedAdamW.py", line 144, in step
    _hpex_C.fused_adamw(
RuntimeError: Got a non-HPU tensor, expecting an HPU tensor
Traceback (most recent call last):
  File "run_clm_starcoder.py", line 568, in <module>
    main()
  File "run_clm_starcoder.py", line 561, in main
    trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/trainer.py", line 572, in train
    return inner_training_loop(
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/trainer.py", line 698, in _inner_training_loop
    deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/deepspeed.py", line 158, in deepspeed_init
    deepspeed_engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/__init__.py", line 125, in initialize
    engine = DeepSpeedEngine(args=args,
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/engine.py", line 380, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/engine.py", line 1279, in _configure_optimizer
    self.optimizer = self._configure_zero_optimizer(basic_optimizer)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/engine.py", line 1603, in _configure_zero_optimizer
    optimizer = DeepSpeedZeroOptimizer_Stage3(
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/stage3.py", line 318, in __init__
    self._setup_for_real_optimizer()
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/stage3.py", line 373, in _setup_for_real_optimizer
    self.initialize_optimizer_states()
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/stage3.py", line 948, in initialize_optimizer_states
    self._optimizer_step(i)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/stage3.py", line 867, in _optimizer_step
    self.optimizer.step()
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/hpex/optimizers/FusedAdamW.py", line 58, in wrap_
    result = step_func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/hpex/optimizers/FusedAdamW.py", line 144, in step
    _hpex_C.fused_adamw(
RuntimeError: Got a non-HPU tensor, expecting an HPU tensor
Traceback (most recent call last):
  File "run_clm_starcoder.py", line 568, in <module>
    main()
  File "run_clm_starcoder.py", line 561, in main
    trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/trainer.py", line 572, in train
    return inner_training_loop(
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/trainer.py", line 698, in _inner_training_loop
    deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/deepspeed.py", line 158, in deepspeed_init
    deepspeed_engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/__init__.py", line 125, in initialize
    engine = DeepSpeedEngine(args=args,
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/engine.py", line 380, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/engine.py", line 1279, in _configure_optimizer
    self.optimizer = self._configure_zero_optimizer(basic_optimizer)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/engine.py", line 1603, in _configure_zero_optimizer
    optimizer = DeepSpeedZeroOptimizer_Stage3(
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/stage3.py", line 318, in __init__
    self._setup_for_real_optimizer()
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/stage3.py", line 373, in _setup_for_real_optimizer
    self.initialize_optimizer_states()
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/stage3.py", line 948, in initialize_optimizer_states
    self._optimizer_step(i)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/stage3.py", line 867, in _optimizer_step
    self.optimizer.step()
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/hpex/optimizers/FusedAdamW.py", line 58, in wrap_
    result = step_func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/hpex/optimizers/FusedAdamW.py", line 144, in step
    _hpex_C.fused_adamw(
RuntimeError: Got a non-HPU tensor, expecting an HPU tensor
Traceback (most recent call last):
  File "run_clm_starcoder.py", line 568, in <module>
    main()
  File "run_clm_starcoder.py", line 561, in main
    trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/trainer.py", line 572, in train
    return inner_training_loop(
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/trainer.py", line 698, in _inner_training_loop
    deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(
  File "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/deepspeed.py", line 158, in deepspeed_init
    deepspeed_engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/__init__.py", line 125, in initialize
    engine = DeepSpeedEngine(args=args,
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/engine.py", line 380, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/engine.py", line 1279, in _configure_optimizer
    self.optimizer = self._configure_zero_optimizer(basic_optimizer)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/engine.py", line 1603, in _configure_zero_optimizer
    optimizer = DeepSpeedZeroOptimizer_Stage3(
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/stage3.py", line 318, in __init__
    self._setup_for_real_optimizer()
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/stage3.py", line 373, in _setup_for_real_optimizer
    self.initialize_optimizer_states()
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/stage3.py", line 948, in initialize_optimizer_states
    self._optimizer_step(i)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/stage3.py", line 867, in _optimizer_step
    self.optimizer.step()
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/hpex/optimizers/FusedAdamW.py", line 58, in wrap_
    result = step_func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/hpex/optimizers/FusedAdamW.py", line 144, in step
    _hpex_C.fused_adamw(
RuntimeError: Got a non-HPU tensor, expecting an HPU tensor
[2023-08-20 10:05:10,891] [INFO] [launch.py:322:sigkill_handler] Killing subprocess 2273116
[2023-08-20 10:05:11,310] [INFO] [launch.py:322:sigkill_handler] Killing subprocess 2273117
[2023-08-20 10:05:11,314] [INFO] [launch.py:322:sigkill_handler] Killing subprocess 2273118
[2023-08-20 10:05:11,317] [INFO] [launch.py:322:sigkill_handler] Killing subprocess 2273119
[2023-08-20 10:05:11,317] [ERROR] [launch.py:328:sigkill_handler] ['/usr/bin/python3', '-u', 'run_clm_starcoder.py', '--model_name_or_path', 'bigcode/starcoder', '--train_file', '/starcoder/finetune-data/roblox-train.json', '--validation_file', '/starcoder/finetune-data/roblox-validation.json', '--report_to', 'tensorboard', '--bf16', 'True', '--output_dir', './checkpoints/starcoder-full-deepspeed-offload-4-hpu-2-2-concat', '--num_train_epochs', '10', '--per_device_train_batch_size', '2', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'no', '--learning_rate', '1e-4', '--logging_steps', '1', '--dataset_concatenation', '--do_train', '--use_habana', '--use_lazy_mode', '--throughput_warmup_steps', '3', '--cache_dir', '/starcoder/cache/', '--use_hpu_graphs_for_inference', '--deepspeed', 'deepspeed_config_s3_offload.json'] exits with return code = 1
[ERROR|distributed_runner.py:218] 2023-08-20 10:05:11,743 >> deepspeed --num_nodes 1 --num_gpus 4 --no_local_rank run_clm_starcoder.py --model_name_or_path bigcode/starcoder --train_file /starcoder/finetune-data/roblox-train.json --validation_file /starcoder/finetune-data/roblox-validation.json --report_to tensorboard --bf16 True --output_dir ./checkpoints/starcoder-full-deepspeed-offload-4-hpu-2-2-concat --num_train_epochs 10 --per_device_train_batch_size 2 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy no --learning_rate 1e-4 --logging_steps 1 --dataset_concatenation --do_train --use_habana --use_lazy_mode --throughput_warmup_steps 3 --cache_dir /starcoder/cache/ --use_hpu_graphs_for_inference --deepspeed deepspeed_config_s3_offload.json  exited with status = 1
